import os
import math
import csv
from collections import Counter
import time
from pathlib import Path
from collections import defaultdict
from itertools import combinations
from tqdm import tqdm
import psutil
import numpy as np
import pandas as pd
import scipy.stats
from scipy.stats import entropy, ttest_ind, fisher_exact
from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list
from sklearn.cluster import KMeans, SpectralBiclustering
from sklearn.preprocessing import normalize
#from sklearn_extra.cluster import KMeansConstrained
import umap
from matplotlib import rcParams
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from matplotlib.ticker import MaxNLocator
from sklearn.preprocessing import StandardScaler, minmax_scale
from sklearn.metrics import (
    roc_curve, auc, precision_recall_curve,
    silhouette_score, adjusted_rand_score,
    normalized_mutual_info_score, confusion_matrix
)
from networkx.algorithms.community import greedy_modularity_communities
from matplotlib import ticker
import torch
import torch.nn as nn
import torch.nn.functional as F
from matplotlib.cm import get_cmap
import dgl
import networkx as nx
from dgl.nn import GNNExplainer
from torch_geometric.nn import GCNConv
from .models import ACGNN, HGDC, EMOGI, MTGCN, GCN, GAT, GraphSAGE, GIN, ChebNet, FocalLoss
from src.utils import (
    choose_model, plot_roc_curve, plot_pr_curve, load_graph_data,
    load_oncokb_genes, plot_and_analyze, save_and_plot_results
)
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.lines import Line2D
from matplotlib.patches import Patch, Rectangle
from matplotlib.gridspec import GridSpec
from matplotlib.colors import Normalize, ListedColormap, to_rgba, to_rgb
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from matplotlib_venn import venn2, venn3, venn2_circles, venn3_circles
from venn import venn
import holoviews as hv
hv.extension('bokeh')
from holoviews import opts
from bokeh.io import output_notebook, export_png
from bokeh.plotting import show
from bokeh.io.export import get_screenshot_as_png
import plotly.graph_objects as go
from gprofiler import GProfiler
import glob
from pathlib import Path   
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.csgraph import laplacian
from scipy.linalg import eigh
import numpy as np  # required for -log10
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from pathlib import Path
from itertools import combinations
from collections import defaultdict
from matplotlib.colors import LinearSegmentedColormap
from sklearn.metrics.pairwise import rbf_kernel
import random
from matplotlib.ticker import FormatStrFormatter
from matplotlib.ticker import ScalarFormatter
from sklearn.metrics import mean_squared_error
from lifelines.statistics import logrank_test, multivariate_logrank_test, pairwise_logrank_test
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.plotting import add_at_risk_counts
import matplotlib.lines as mlines

 
CLUSTER_COLORS = {
    0: '#0077B6',  1: '#0000FF',  2: '#00B4D8',  3: '#48EAC4',
    4: '#F1C0E8',  5: '#B9FBC0',  6: '#32CD32',  7: '#bee1e6',
    8: '#8A2BE2',  9: '#E377C2'
}
'''CLUSTER_COLORS = {
    0: '#0077B6',  1: '#0000FF',  2: '#00B4D8',  3: '#48EAC4',
    4: '#F1C0E8',  5: '#B9FBC0',  6: '#32CD32',  7: '#bee1e6',
    8: '#8A2BE2',  9: '#E377C2', 10: '#8EECF5', 11: '#A3C4F3',
    12: '#FFB347', 13: '#FFD700', 14: '#FF69B4', 15: '#CD5C5C',
    16: '#7FFFD4', 17: '#FF7F50'
}'''
CLUSTER_COLORS = {
    0: '#0077B6',   1: '#0000FF',   2: '#00B4D8',   3: '#48EAC4',
    4: '#F1C0E8',   5: '#B9FBC0',   6: '#32CD32',   7: '#bee1e6',
    8: '#8A2BE2',   9: '#E377C2',  10: '#8EECF5',  11: '#A3C4F3',
    12: '#FFB347', 13: '#FFD700',  14: '#FF69B4',  15: '#CD5C5C',
    16: '#7FFFD4', 17: '#FF7F50',  18: '#C71585',  19: '#20B2AA'
}


CLUSTER_COLORS_OMICS = {
    0: '#D62728',  1: '#1F77B4',  2: '#2CA02C',  3: '#9467BD'
}


def save_confirmed_predictions_to_csv(confirmed_predictions, output_dir, model_type, net_type, num_epochs):
    """
    Save confirmed predicted genes to a CSV file.
    
    Args:
    - confirmed_predictions: List of tuples (gene, score, sources).
    - output_dir: Directory to save the CSV file.
    - model_type, net_type, num_epochs: For naming the output file.
    """
    confirmed_predictions_csv_path = os.path.join(output_dir, f'{model_type}_{net_type}_confirmed_predicted_genes_epo{num_epochs}_2048.csv')
    df_confirmed = pd.DataFrame(confirmed_predictions, columns=["Gene", "Score", "Source"])
    df_confirmed.to_csv(confirmed_predictions_csv_path, index=False)
    print(f"Confirmed predicted genes saved to {confirmed_predictions_csv_path}")

def find_optimal_k(node_features, k_range=(2, 16), plot_path='silhouette_score_plot.png', save_best_k_path='best_k.txt'):
    """
    Find the optimal number of clusters k using silhouette score.
    
    Parameters:
        node_features (numpy.ndarray): The features of the nodes (embeddings).
        k_range (tuple): A range (min, max) for the values of k to evaluate.
        plot_path (str): Path to save the silhouette score plot.
        save_best_k_path (str): Path to save the best k value.
        
    Returns:
        int: The optimal number of clusters based on the highest silhouette score.
    """
    silhouette_scores = []
    K_range = range(k_range[0], k_range[1] + 1)

    # Compute silhouette score for each k in the range
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        row_labels = kmeans.fit_predict(node_features)
        score = silhouette_score(node_features, row_labels)
        silhouette_scores.append(score)
        print(f"Silhouette score for k={k}: {score:.4f}")
    
    # Find the best k (highest silhouette score)
    best_k = K_range[np.argmax(silhouette_scores)]
    print(f"Optimal number of clusters (k) based on Silhouette Score: {best_k}")

    # Plot silhouette scores
    plt.plot(K_range, silhouette_scores, marker='o')
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Score for Different k')

    # Save the plot to the specified path
    plt.savefig(plot_path)
    plt.close()  # Close the plot after saving

    # Save the best k value to a text file
    with open(save_best_k_path, 'w') as f:
        f.write(f"Best k: {best_k}\n")

    return best_k

def analyze_sankey_structure(
    source,
    target,
    value,
    label_to_idx,
    node_names,
    cluster_to_genes,
    gene_to_neighbors,
    row_labels,
    name_to_index,
    relevance_scores
):
    # === Build Directed Graph
    G = nx.DiGraph()
    for s, t, v in zip(source, target, value):
        G.add_edge(s, t, weight=v)

    node_id_to_name = {v: k for k, v in label_to_idx.items()}

    # === 1. Degree Centrality of Genes
    gene_nodes = [label_to_idx[g] for g in node_names if g in label_to_idx]
    degree_centrality = nx.degree_centrality(G)
    gene_centrality_scores = {
        node_id_to_name[n]: degree_centrality[n]
        for n in gene_nodes if n in degree_centrality
    }

    # === 2. Gene-to-Cluster Participation Count
    gene_cluster_participation = defaultdict(set)
    for cluster_label, genes in cluster_to_genes.items():
        for gene in genes:
            gene_cluster_participation[gene].add(cluster_label)
    gene_cluster_counts = {gene: len(clusters) for gene, clusters in gene_cluster_participation.items()}

    # === 3. Entropy of Flow Distributions (per Cluster)
    # cluster_entropy = {}
    # for cluster_label, genes in cluster_to_genes.items():
    #     scores_arr = np.array([relevance_scores[name_to_index[g]] for g in genes])
    #     probs = scores_arr / scores_arr.sum() if scores_arr.sum() > 0 else np.ones_like(scores_arr) / len(scores_arr)
    #     cluster_entropy[cluster_label] = entropy(probs)

    # === 3. Entropy of Flow Distributions (per Cluster)
    cluster_entropy = {}
    for cluster_label, genes in cluster_to_genes.items():
        scores_arr = np.array([np.linalg.norm(relevance_scores[name_to_index[g]]) for g in genes])  # ensure 1D
        if scores_arr.sum() > 0:
            probs = scores_arr / scores_arr.sum()
        else:
            probs = np.ones_like(scores_arr) / len(scores_arr)
        cluster_entropy[cluster_label] = float(entropy(probs))  # ensure scalar

    # === 4. Jaccard Similarity Between Clusters Based on Shared Downstream Clusters
    cluster_to_downstream = defaultdict(set)
    for gene, neighbors in gene_to_neighbors.items():
        if gene not in name_to_index:
            continue
        gene_idx = name_to_index[gene]
        cluster = f"Cluster {row_labels[gene_idx]}"
        for neighbor_idx in neighbors:
            neighbor_cluster = f"Cluster {row_labels[neighbor_idx]}"
            cluster_to_downstream[cluster].add(neighbor_cluster)

    cluster_jaccard = {}
    for c1, c2 in combinations(cluster_to_downstream.keys(), 2):
        s1 = cluster_to_downstream[c1]
        s2 = cluster_to_downstream[c2]
        intersection = len(s1 & s2)
        union = len(s1 | s2)
        if union > 0:
            cluster_jaccard[(c1, c2)] = intersection / union

    return {
        "gene_degree_centrality": gene_centrality_scores,
        "gene_cluster_counts": gene_cluster_counts,
        "cluster_entropy": cluster_entropy,
        "cluster_jaccard": cluster_jaccard,  # <- changed from cluster_jaccard_similarity
        "cluster_participation": gene_cluster_counts
    }

def save_metrics_to_csv(metrics_dict, output_dir='sankey_metrics'):
    os.makedirs(output_dir, exist_ok=True)

    # 1. Degree Centrality
    pd.DataFrame.from_dict(metrics_dict['gene_degree_centrality'], orient='index', columns=['degree_centrality'])\
        .sort_values('degree_centrality', ascending=False)\
        .to_csv(f"{output_dir}/gene_degree_centrality.csv")

    # 2. Cluster Participation Count
    pd.DataFrame.from_dict(metrics_dict['gene_cluster_counts'], orient='index', columns=['cluster_count'])\
        .sort_values('cluster_count', ascending=False)\
        .to_csv(f"{output_dir}/gene_cluster_counts.csv")

    # 3. Entropy per Cluster
    pd.DataFrame.from_dict(metrics_dict['cluster_entropy'], orient='index', columns=['entropy'])\
        .sort_values('entropy', ascending=False)\
        .to_csv(f"{output_dir}/cluster_entropy.csv")

    # 4. Jaccard Similarity between Clusters
    # Jaccard Similarity between Clusters
    jaccard_df = pd.DataFrame([
        {'Cluster 1': k[0], 'Cluster 2': k[1], 'Jaccard Similarity': v}
        for k, v in metrics_dict['cluster_jaccard'].items()
    ])

    jaccard_df.sort_values(by='Jaccard Similarity', ascending=False)\
        .to_csv(f"{output_dir}/cluster_jaccard_similarity.csv", index=False)

def save_predictions_to_csv(predicted_genes, output_dir, model_type, net_type, num_epochs):
    """
    Save the predicted genes with their sources to a CSV file.
    
    Args:
    - predicted_genes: List of tuples (gene, score, sources) to save.
    - output_dir: Directory to save the CSV file.
    - model_type, net_type, num_epochs: For naming the output file.
    """
    os.makedirs(output_dir, exist_ok=True)
    predicted_genes_csv_path = os.path.join(output_dir, f'{model_type}_{net_type}_predicted_driver_genes_epo{num_epochs}_2048.csv')
    df_predictions = pd.DataFrame(predicted_genes, columns=["Gene", "Score", "Confirmed Sources"])
    df_predictions.to_csv(predicted_genes_csv_path, index=False)
    print(f"Predicted driver genes with confirmed sources saved to {predicted_genes_csv_path}")

def save_predicted_known_drivers(predicted_driver_genes, output_dir, model_type, net_type, num_epochs):
    """
    Save predicted known cancer driver genes to a CSV file.
    
    Args:
    - predicted_driver_genes: List of predicted cancer driver genes.
    - output_dir: Directory to save the CSV file.
    - model_type, net_type, num_epochs: For naming the output file.
    """
    predicted_drivers_csv_path = os.path.join(output_dir, f'{model_type}_{net_type}_predicted_known_drivers_epo{num_epochs}_2048.csv')
    df = pd.DataFrame(predicted_driver_genes, columns=["Gene"])
    df.to_csv(predicted_drivers_csv_path, index=False)
    print(f"Predicted known driver genes saved to {predicted_drivers_csv_path}")
        
def visualize_feature_relevance_heatmaps(relevance_df, clusters, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    # Merge relevance with cluster labels
    merged = relevance_df.copy()
    merged['gene'] = merged.index
    merged = pd.merge(merged, clusters, on='gene')

    for cluster_type in clusters.columns[1:]:
        for view in ['cancer', 'omics']:
            cluster_groups = merged.groupby(cluster_type)
            all_cluster_heatmaps = []
            fig, axes = plt.subplots(len(cluster_groups), 1, figsize=(12, 4 * len(cluster_groups)))
            if len(cluster_groups) == 1:
                axes = [axes]

            for ax, (label, group) in zip(axes, cluster_groups):
                data = group.drop(columns=['gene'] + list(clusters.columns[1:]))

                if view == 'cancer':
                    # Collapse omics features per cancer (max-over-omics per cancer)
                    cancer_names = list(set([col.split('_')[0] for col in data.columns]))
                    cancer_view = pd.DataFrame(index=data.index, columns=cancer_names)
                    for ct in cancer_names:
                        cols = [col for col in data.columns if col.startswith(ct + '_')]
                        cancer_view[ct] = data[cols].max(axis=1)
                    plot_data = cancer_view
                    title = f"{cluster_type.capitalize()} cluster {label} ‚Äî Cancer view"
                    fname = f"{cluster_type.capitalize()}_cluster_{label}_cancer_heatmap.png"

                else:
                    # Collapse across cancer types for each omics type (max-over-cancers per omics)
                    omics_types = list(set([col.split('_')[-1] for col in data.columns]))
                    omics_view = pd.DataFrame(index=data.index, columns=omics_types)
                    for om in omics_types:
                        cols = [col for col in data.columns if col.endswith('_' + om)]
                        omics_view[om] = data[cols].max(axis=1)
                    plot_data = omics_view
                    title = f"{cluster_type.capitalize()} cluster {label} ‚Äî Omics view"
                    fname = f"{cluster_type.capitalize()}_cluster_{label}_omics_heatmap.png"

                # Normalize per row for better heatmap contrast
                plot_data = pd.DataFrame(StandardScaler().fit_transform(plot_data.T).T,
                                         index=plot_data.index, columns=plot_data.columns)
                sns.heatmap(plot_data, cmap='viridis', ax=ax, cbar=True)
                ax.set_title(title)
                ax.set_xlabel('Features')
                ax.set_ylabel('Genes')

                fig.tight_layout()
                fig.savefig(os.path.join(output_dir, fname))
                plt.close(fig)

def cluster_and_visualize_predicted_genes(graph, predicted_cancer_genes, node_names, 
                                          output_path_genes_clusters, num_clusters=12):
    """
    Clusters gene embeddings into groups using KMeans, visualizes them with t-SNE, 
    and marks predicted cancer genes with red circles (half the size of non-cancer dots).

    Returns:
        row_labels (np.ndarray): Cluster assignments for each gene.
        total_genes_per_cluster (dict): Total number of genes per cluster.
        pred_counts (dict): Number of predicted cancer genes per cluster.
    """
    # Extract embeddings
    embeddings = graph.ndata['feat'].cpu().numpy()

    # Run KMeans clustering
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=12)
    row_labels = kmeans.fit_predict(embeddings)

    # Store cluster labels in graph
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long, device=graph.device)

    # Calculate the total number of genes per cluster
    total_genes_per_cluster = {i: np.sum(row_labels == i) for i in range(num_clusters)}

    # Calculate the number of predicted cancer genes per cluster
    pred_counts = {i: 0 for i in range(num_clusters)}
    '''for gene_idx in predicted_cancer_genes:
        print(f"gene_idx type: {type(gene_idx)}, value: {gene_idx}")
        print(f"row_labels type: {type(row_labels)}, length: {len(row_labels)}")

        cluster_id = row_labels[gene_idx]
        pred_counts[cluster_id] += 1'''

    # Convert gene names to their corresponding indices
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_cancer_gene_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]

    # Ensure valid indices before using them
    for gene_idx in predicted_cancer_gene_indices:
        if 0 <= gene_idx < len(row_labels):  
            cluster_id = row_labels[gene_idx]
            pred_counts[cluster_id] += 1
        else:
            print(f"Skipping invalid index: {gene_idx}")


    # Reduce dimensions with t-SNE for visualization
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    reduced_embeddings = tsne.fit_transform(embeddings)

    plt.figure(figsize=(12, 10))

    # Plot clusters (Non-cancer genes)
    non_cancer_dot_size = 100  # Default dot size
    red_circle_size = non_cancer_dot_size / 2  # Half the size

    for cluster_id in range(num_clusters):
        idx = np.where(row_labels == cluster_id)[0]  # Get indices of this cluster
        plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], 
                    color=CLUSTER_COLORS[cluster_id], 
                    edgecolor='k', s=non_cancer_dot_size, alpha=0.8)

    # Mark predicted cancer genes with red circles (‚ö™, half the size)
    for gene_idx in predicted_cancer_gene_indices:
        x, y = reduced_embeddings[gene_idx]
        plt.scatter(x, y, facecolors='none', edgecolors='red', s=red_circle_size, linewidths=2)


    # Labels and title
    ##plt.title("Gene clustering with predicted cancer genes")
    plt.xlabel("t-SNE Dimension 1", fontsize=18)
    plt.ylabel("t-SNE Dimension 2", fontsize=18)

    # Save plot
    plt.savefig(output_path_genes_clusters, bbox_inches="tight")  # Ensure proper cropping
    plt.close()

    print(f"Cluster visualization saved to {output_path_genes_clusters}")

    return row_labels, total_genes_per_cluster, pred_counts

def compute_lrp_scores(model, graph, features, node_indices=None):
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features.requires_grad_(True)


    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        # Select the nodes to analyze (e.g., predicted cancer genes)
        if node_indices is None:
            node_indices = torch.nonzero((probs > 0.0)).squeeze()

        relevance_scores = torch.zeros_like(features)

        for idx in node_indices:
            model.zero_grad()
            ##probs[idx].backward(retain_graph=True)
            probs[idx].backward(retain_graph=(idx != node_indices[-1]))

            relevance_scores[idx] = features.grad[idx].detach()

    return relevance_scores

def load_gene_set(file_path):
    """
    Load a gene list from a file and return as a set.
    
    Args:
    - file_path: Path to the file containing genes, one per line.
    
    Returns:
    - Set of gene names.
    """
    with open(file_path, 'r') as f:
        return set(line.strip() for line in f)

def save_row_labels(row_labels, save_path):
    ##row_labels_path = os.path.join(os.path.dirname(save_path), "row_labels.npy")
    np.save(save_path, row_labels)
    print(f"Cluster labels saved to {save_path}")

# Save total genes per cluster
def save_total_genes_per_cluster(total_genes_per_cluster, save_path):
    ##total_genes_path = os.path.join(os.path.dirname(save_path), "total_genes_per_cluster.npy")
    np.save(save_path, total_genes_per_cluster)
    print(f"Total genes per cluster saved to {save_path}")

# Save predicted cancer genes per cluster
def save_predicted_counts(pred_counts, save_path):
    ##pred_counts_path = os.path.join(os.path.dirname(save_path), "predicted_counts.npy")
    np.save(save_path, pred_counts)
    print(f"Predicted cancer genes per cluster saved to {save_path}")

'''# Function to reload the saved graph data
def reload_spectral_biclustering_data(save_path):
    print(f"Loading graph data from {save_path}...")
    
    # Load the saved data
    checkpoint = torch.load(save_path)

    # Extract graph-related data
    edges = checkpoint['edges']
    features = checkpoint['features']
    labels = checkpoint['labels']
    row_labels = checkpoint['cluster']

    print("Graph data loaded successfully.")

    return edges, features, labels, row_labels
'''
# Function to reload cluster labels

def reload_row_labels(save_path):
    ##row_labels_path = os.path.join(os.path.dirname(save_path), "row_labels.npy")
    row_labels = np.load(save_path)
    print(f"Cluster labels loaded from {save_path}")
    return row_labels

# Function to reload total genes per cluster
def reload_total_genes_per_cluster(save_path):
    ##total_genes_path = os.path.join(os.path.dirname(save_path), "total_genes_per_cluster.npy")
    total_genes_per_cluster = np.load(save_path, allow_pickle=True).item()
    print(f"Total genes per cluster loaded from {save_path}")
    return total_genes_per_cluster

# Function to reload predicted cancer genes per cluster
def reload_predicted_counts(save_path):
    ##pred_counts_path = os.path.join(os.path.dirname(save_path), "predicted_counts.npy")
    pred_counts = np.load(save_path, allow_pickle=True).item()
    print(f"Predicted cancer genes per cluster loaded from {save_path}")
    return pred_counts

def load_bioclustered_graph(save_path):
    """
    Loads a previously saved DGL graph that includes features, labels, and cluster assignments.

    Args:
        save_path (str): Path to the saved graph file (.pth)

    Returns:
        dgl.DGLGraph: The reconstructed graph with restored node data.
    """
    data = torch.load(save_path)

    graph = dgl.graph(data['edges'])
    graph.ndata['feat'] = data['features']

    if data.get('label') is not None:
        graph.ndata['label'] = data['label']

    if data.get('cluster') is not None:
        graph.ndata['cluster'] = data['cluster']

    if 'degree' in data:
        graph.ndata['degree'] = data['degree']
    if 'train_mask' in data:
        graph.ndata['train_mask'] = data['train_mask']
    if 'test_mask' in data:
        graph.ndata['test_mask'] = data['test_mask']

    print("‚úÖ Clustered graph loaded successfully.")
    return graph

def save_bioclustered_gene_info_csv(
    row_labels,
    total_genes_per_cluster,
    pred_counts,
    node_names,
    predicted_gene_indices,
    output_csv_path
):
    """
    Saves clustered gene information to a CSV file.

    Args:
        row_labels (np.ndarray): Cluster assignments.
        total_genes_per_cluster (dict): Total genes in each cluster.
        pred_counts (dict): Number of predicted cancer genes per cluster.
        node_names (list): List of all gene names by index.
        predicted_gene_indices (list): Indices of predicted cancer genes.
        output_csv_path (str): Path to save the CSV file.
    """
    gene_data = []
    for idx, name in enumerate(node_names):
        cluster = row_labels[idx]
        is_predicted = 1 if idx in predicted_gene_indices else 0
        gene_data.append({
            "Gene": name,
            "Cluster": cluster,
            "IsPredictedCancerGene": is_predicted,
            "TotalGenesInCluster": total_genes_per_cluster[cluster],
            "PredictedInCluster": pred_counts[cluster]
        })

    df = pd.DataFrame(gene_data)
    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
    df.to_csv(output_csv_path, index=False)
    print(f"Clustered gene information saved to {output_csv_path}")

def save_reduced_feature_relevance_scores(
    relevance_scores,      # Original 2048D matrix, shape [N, 2048]
    gene_names,            # List of gene symbols, length N
    output_csv_path        # File to save the reduced 64D matrix
):
    """
    Reduce 2048D LRP scores to 64D using max-over-16 logic,
    and save to CSV with columns like 'BRCA_mf', ..., 'KIRP_meth'.
    """
    import numpy as np
    import pandas as pd
    import os

    # -- Step 1: Reduce features to 64D
    reduced_scores = extract_summary_features_np_skip(relevance_scores)  # shape [N, 64]

    # -- Step 2: Define feature names in cancer x omics order
    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    omics_types = ['cna', 'ge', 'meth', 'mf']
    column_labels = [f"{cancer}_{omics}" for omics in omics_types for cancer in cancer_names]

    # -- Step 3: Build DataFrame and save
    df = pd.DataFrame(reduced_scores, index=gene_names, columns=column_labels)
    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
    df.to_csv(output_csv_path)
    print(f"‚úÖ Saved reduced feature relevance matrix to {output_csv_path}")

def plot_top_predicted_genes_tsne(graph, node_names, scores, output_path, top_k=1000):
    cluster_ids = graph.ndata['cluster_bio'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    # Get top K predicted genes
    top_indices = np.argsort(scores)[-top_k:]
    top_embeddings = embeddings[top_indices]
    top_clusters = cluster_ids[top_indices]
    top_scores = scores[top_indices]
    top_names = [node_names[i] for i in top_indices]

    # t-SNE projection
    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(top_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    rcParams['pdf.fonttype'] = 42  # prevent font issues in vector graphics

    for c in np.unique(top_clusters):
        mask = top_clusters == c
        coords = tsne_coords[mask]
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            edgecolors='k',
            s=60,
            alpha=0.8
        )

        # Top 1 in this cluster (within top_k)
        cluster_scores = top_scores[mask]
        if cluster_scores.size > 0:
            top_idx_in_cluster = np.argmax(cluster_scores)
            name = np.array(top_names)[mask][top_idx_in_cluster]
            x, y = coords[top_idx_in_cluster]
            # Highlight the top 1 with a yellow circle (half the size of the original dot)
            plt.scatter(x, y, s=60, edgecolors='yellow', alpha=0.6, linewidth=1, marker='o', color='red')
            # Change label text color to red
            plt.text(x, y, name, fontsize=9, fontweight='bold', ha='center', va='center', color='black')

    plt.title("t-SNE of Top 1000 Predicted Genes by Cluster", fontsize=16)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    
    # Remove legend
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Top predicted gene t-SNE plot saved to:\n{output_path}")

def plot_tsne_predicted_genes(graph, node_names, scores, output_path, args):
    cluster_ids = graph.ndata['cluster_bio'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    predicted_mask = scores >= args.score_threshold
    predicted_indices = np.where(predicted_mask)[0]
    predicted_scores = scores[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]
    predicted_embeddings = embeddings[predicted_indices]

    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(predicted_embeddings)

    # Gather top 2 genes per cluster
    top_genes = []
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) == 0:
            continue
        top_indices = cluster_indices[np.argsort(predicted_scores[cluster_indices])[-2:]]  # top 2
        for idx in top_indices:
            top_genes.append((predicted_indices[idx], tsne_coords[idx], c))

    # Plot
    plt.figure(figsize=(10, 7))
    for idx, (node_idx, coord, cluster_id) in enumerate(top_genes):
        color = CLUSTER_COLORS.get(cluster_id, "#333333")
        plt.scatter(coord[0], coord[1], color=color, s=120, edgecolor='k')
        plt.text(coord[0]+1.5, coord[1], node_names[node_idx], fontsize=9, color=color)

    # Legend
    unique_predicted_clusters = np.unique(predicted_clusters)
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=f"Cluster {c}", markersize=10)
        for c, color in CLUSTER_COLORS.items()
        if c in unique_predicted_clusters
    ]
    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.title("t-SNE of Top 2 Predicted Genes per Cluster")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

def plot_feature_importance(
    relevance_vector,
    feature_names=None,
    node_name=None,
    top_k=9,
    gene_names=None,
    output_path="plots/feature_importance.png"):
    # Convert to NumPy array
    if isinstance(relevance_vector, torch.Tensor):
        relevance_vector = relevance_vector.detach().cpu().numpy()
    else:
        relevance_vector = np.array(relevance_vector)

    # --- üîÑ Min-Max Normalization ---
    min_val = relevance_vector.min()
    max_val = relevance_vector.max()
    norm_scores = (relevance_vector - min_val) / (max_val - min_val + 1e-8)

    # --- üîù Top-K selection ---
    top_indices = np.argsort(norm_scores)[-top_k:][::-1]
    top_scores = norm_scores[top_indices]

    if feature_names is None:
        feature_names = [f"Feature {i}" for i in range(len(relevance_vector))]

    # --- üß¨ Labeling ---
    if gene_names is not None:
        top_labels = [gene_names[i].capitalize() if i < len(gene_names) else f"Unknown {i}" for i in top_indices]
    else:
        top_labels = [feature_names[i] for i in top_indices]

    # Construct DataFrame
    df = pd.DataFrame({
        "feature": top_labels,
        "relevance": top_scores
    })

    # --- üìä Plot ---
    plt.figure(figsize=(2.5, 2.5))
    sns.set_style("white")
    sns.barplot(
        data=df,
        x="feature",
        y="relevance",
        palette="Blues_d",
        dodge=False,
        legend=False,
        width=0.6
    )

    plt.ylabel("Relevance score", fontsize=12)
    plt.xlabel("", fontsize=12)
    plt.title(f"{node_name}", fontsize=13)

    plt.xticks(rotation=90, ha='center', fontsize=11)
    plt.yticks(fontsize=11)

    sns.despine()
    plt.tight_layout()

    # --- üíæ Ensure directory exists and save ---
    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"‚úÖ Saved barplot to {output_path}")
    else:
        plt.close()

def _plot_bar(omics_relevance, omics_colors, omics_order, output_path):
    plt.figure(figsize=(1.4, 2))
    sns.barplot(
        x=omics_relevance.index,
        y=omics_relevance.values,
        palette=[omics_colors[o] for o in omics_order],
        width=0.6
    )

    # Capitalize x-axis labels
    plt.xticks(
        ticks=range(len(omics_relevance.index)),
        labels=[label.upper() for label in omics_relevance.index],
        rotation=90,
        fontsize=9
    )
    plt.yticks(fontsize=10)
    plt.xlabel('', fontsize=10)
    plt.ylabel('', fontsize=10)
    plt.tick_params(axis='both', which='both', length=0)
    sns.despine()

    # Use scientific notation on y-axis
    '''ax = plt.gca()
    ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))
    ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))'''
    ax = plt.gca()

    # Set scientific formatter (plain 'e' format)
    formatter = ScalarFormatter(useMathText=False)
    formatter.set_scientific(True)
    formatter.set_powerlimits((0, 0))
    ax.yaxis.set_major_formatter(formatter)

    # Reduce the font size of the offset (e.g., '1e-3' label)
    ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))
    ax.yaxis.offsetText.set_fontsize(6)
    ax.yaxis.offsetText.set_horizontalalignment('left')
        
    plt.tight_layout()

    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"‚úÖ Saved barplot to {output_path}")
    else:
        plt.close()

def extract_summary_features_np_skip(features_np):
    num_nodes = features_np.shape[0]
    total_dim = features_np.shape[1]
    summary_features = []

    for o_idx in range(4):  # omics
        for c_idx in range(16):  # cancer
            base = o_idx * 16 * 16 + c_idx * 16
            if base + 16 > total_dim:
                continue  # skip invalid slice
            group = features_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)

def compute_integrated_gradients(
    model, graph, features, node_indices=None, baseline=None, steps=50):
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    assert baseline.shape == features.shape, "Baseline must match feature shape"

    # Scale inputs and compute gradients
    scaled_inputs = [
        baseline + (float(i) / steps) * (features - baseline)
        for i in range(1, steps + 1)
    ]
    scaled_inputs = torch.stack(scaled_inputs)  # Shape: (steps, num_nodes, num_features)

    # Integrated gradients trainedization
    integrated_grads = torch.zeros_like(features)

    for step_input in scaled_inputs:
        step_input.requires_grad_(True)
        logits = model(graph, step_input)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        grads = torch.autograd.grad(
            outputs=probs[node_indices],
            inputs=step_input,
            grad_outputs=torch.ones_like(probs[node_indices]),
            retain_graph=True,
            create_graph=False,
        )[0]

        integrated_grads += grads.detach()

    # Average the gradients and scale by the input difference
    avg_grads = integrated_grads / steps
    ig_attributions = (features - baseline) * avg_grads

    return ig_attributions

def get_two_hop_neighbors(graph, node_id):
    one_hop = set(graph.neighbors(node_id)) if node_id in graph else set()
    two_hop = set()
    
    for neighbor in one_hop:
        two_hop.update(graph.neighbors(neighbor))
    
    # Remove the original node and one-hop neighbors from two-hop set
    two_hop.difference_update(one_hop)
    two_hop.discard(node_id)
    
    return sorted(two_hop)

def save_cluster_legend(output_path_legend, cluster_colors, num_clusters=12):
    """
    Creates and saves a separate legend image for cluster colors in a single row.

    Args:
        output_path_legend (str): Path to save the legend image.
        cluster_colors (list): List of colors for each cluster.
        num_clusters (int): Number of clusters.
    """
    fig, ax = plt.subplots(figsize=(12, 1.5))  # Wider and shorter for single-row legend

    # Create legend handles labeled from Cluster 0 to Cluster 11
    legend_patches = [mpatches.Patch(color=cluster_colors[i], label=f"Cluster {i}") 
                      for i in range(num_clusters)]

    # Display legend with one row
    ax.legend(handles=legend_patches, loc='center', ncol=num_clusters,
              frameon=False, fontsize=16)

    # Remove axes
    ax.set_xticks([])
    ax.set_yticks([])
    ax.axis("off")

    # Save legend image
    plt.savefig(output_path_legend, bbox_inches="tight", dpi=300)
    plt.close()

    print(f"Legend saved to {output_path_legend}")

def compute_saliency_all_nodes(model, g, features, target_classes=None):
    """
    Compute saliency (relevance scores) for all nodes using gradients.

    Args:
        model: Trained GNN model
        g: DGL graph
        features: Input node features (torch.Tensor or np.ndarray)
        target_classes: Optional tensor/list of target classes per node. If None, use predicted class.

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] with saliency values.
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    
    features = features.clone().detach().requires_grad_(True)
    logits = model(g, features)

    num_nodes = features.shape[0]
    relevance_scores = torch.zeros_like(features)

    if target_classes is None:
        target_classes = torch.argmax(logits, dim=1)

    for node_id in range(num_nodes):
        model.zero_grad()
        if features.grad is not None:
            features.grad.zero_()

        score = logits[node_id, target_classes[node_id]]
        score.backward(retain_graph=True)

        relevance_scores[node_id] = features.grad[node_id].abs().detach()

    return relevance_scores

def plot_gene_feature_contributions_topo_(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_topo(df, barplot_path)

    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=16)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=16)
    if isinstance(score, np.ndarray):
        score = score.item()

    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=16)
    plt.title(f"{gene_name}", fontsize=16)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_bio_(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_bio(df, barplot_path)

    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=16)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=16)
    if isinstance(score, np.ndarray):
        score = score.item()
    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=16)
    plt.title(f"{gene_name}", fontsize=16)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_confirmed_neighbors_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    # scores,
    row_labels,
    total_clusters,
    relevance_scores):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        # gene_score = relevance_scores[node_idx]
        gene_score = relevance_scores[node_idx].sum().item()  # OR .mean().item()

        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
        print(f"{gene} ‚Üí Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        # Filter only those neighbors present in top-k
        neighbor_scores_dict = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    #if rel_score > 0.1:
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/bio_neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )


def plot_confirmed_neighbors_topo(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    # scores,
    row_labels,
    total_clusters,
    relevance_scores):
    # Only top-k names are passed in node_names
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= relevance_scores.shape[0]:
            print(f"‚ö†Ô∏è Skipping {gene}: index {node_idx} out of bounds for relevance_scores with shape {relevance_scores.shape}")
            continue

        gene_score = relevance_scores[node_idx].sum().item()


        # ‚úÖ Get cluster from graph
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()
        print(f"{gene} ‚Üí Node {node_idx} | Topo score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores_dict = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:  # bounds check
                    rel_score = relevance_scores[rel_idx].sum().item()
                    # if rel_score > 0.1:
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/topo_neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/topo_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )


def plot_confirmed_neighbor_relevance(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    scores,
    relevance_scores,
    mode="bio"):  # or "topo"):
    """
    Plots top-10 neighbor relevance scores for confirmed genes.
    Mode can be 'bio' or 'topo'.
    """


    assert mode in ("bio", "topo"), "Mode must be 'bio' or 'topo'"

    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not found in the graph.")
            continue

        node_idx = name_to_index[gene]
        gene_score = scores[node_idx]
        print(f"{gene} ‚Üí Node {node_idx} | {mode.capitalize()} score: {gene_score:.4f}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

        # Get relevance scores for neighbors (filtering by score > 0.1)
        neighbor_scores_dict = {
            i: relevance_scores[i].sum().item()
            for i in neighbor_indices
            if relevance_scores[i].sum().item() > 0.1
        }

        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        # Sort and select top 10
        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_{mode}_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )


def plot_topo_clusterwise_feature_contributions(
    args,
    relevance_scores,           # 2D array (samples x features)
    row_labels,             # 1D array of cluster assignments
    feature_names,              # List of feature names (e.g., TOPO: BRCA, ...)
    per_cluster_feature_contributions_output_dir):  # Output folder
    ##omics_colors                # Dict of omics type colors (e.g., 'topo': '#1F77B4')):
    os.makedirs(per_cluster_feature_contributions_output_dir, exist_ok=True)

    '''def get_omics_color(feature_name):
        prefix = feature_name.split(":")[0].lower()
        return omics_colors.get(prefix, "#AAAAAA")'''

    unique_clusters = np.unique(row_labels)

    for cluster_id in sorted(unique_clusters):
        indices = np.where(row_labels == cluster_id)[0]
        cluster_scores = relevance_scores[indices]
        avg_contribution = np.mean(cluster_scores, axis=0)
        total_score = np.sum(avg_contribution)

        fig, ax = plt.subplots(figsize=(10, 2.5))

        x = np.linspace(0, 1, len(feature_names))
        bar_width = 1 / len(feature_names) * 0.95

        bars = ax.bar(
            x,
            avg_contribution,
            width=bar_width,
            ##color=[get_omics_color(name) for name in feature_names],
            align='center'
        )

        ax.set_title(
            fr"Cluster {cluster_id} $\mathregular{{({len(indices)}\ genes,\ avg = {total_score:.2f})}}$",
            fontsize=16
        )

        clean_labels = [name.split(":")[1].strip() if ":" in name else name for name in feature_names]
        ax.set_xticks(x)
        ax.set_xticklabels(clean_labels, rotation=90)

        '''for label, feature_name in zip(ax.get_xticklabels(), feature_names):
            label.set_color(get_omics_color(feature_name))'''

        ax.tick_params(axis='x', labelsize=9)
        ax.set_xlim(-bar_width, 1 + bar_width)

        plt.tight_layout()
        save_path = os.path.join(
            per_cluster_feature_contributions_output_dir,
            f"{args.model_type}_{args.net_type}_TOPO_cluster_{cluster_id}_feature_contributions_epo{args.num_epochs}.png"
        )
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"‚úÖ Saved TOPO feature contribution barplot for Cluster {cluster_id} to {save_path}")

def plot_feature_importance_topo(
    relevance_vector,
    feature_names,
    node_name=None,
    output_path="plots"):


    if len(relevance_vector) != len(feature_names):
        raise ValueError("Length mismatch between relevance vector and feature names.")

    ##pretty_labels = [f"emb_{i}" for i in range(len(relevance_vector))]
    pretty_labels = [f"{i}" for i in range(len(relevance_vector))]

    df = pd.DataFrame({
        "feature": pretty_labels,
        "relevance": relevance_vector
    })

    plt.figure(figsize=(24, 5))
    sns.set_style("white")
    bars = plt.bar(df["feature"], df["relevance"], color="#607D8B")  # bluish-gray

    # Title (no mean)
    if node_name:
        plt.title(f"{node_name}", fontsize=16)

    # Axis labels and formatting
    plt.xlabel("Topology Embedding Dimension", fontsize=16)
    plt.ylabel("Relevance", fontsize=16)
    plt.xticks(rotation=90, fontsize=12)
    plt.yticks(fontsize=12)
    plt.margins(x=0)

    num_bars = len(df)
    margin = 0.75
    ax = plt.gca()
    ax.set_xlim(-margin, num_bars - 1 + margin)

    ##ax.set_xlim(-bar_width, 1 + bar_width) 

    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved TOPO feature importance plot to {output_path}")

def plot_bio_topo_saliency(bio_scores, topo_scores, title="", save_path=None):
    """
    Plot saliency relevance for bio and topo features with mean values in a color-patched legend.

    Parameters:
        bio_scores (np.ndarray): Relevance scores for bio features (length 1024).
        topo_scores (np.ndarray): Relevance scores for topo features (length 1024).
        title (str): Plot title.
        save_path (str, optional): Path to save the figure.
    """

    # üîπ Normalize
    bio_scores_norm = (bio_scores - bio_scores.min()) / (bio_scores.max() - bio_scores.min() + 1e-8)
    topo_scores_norm = (topo_scores - topo_scores.min()) / (topo_scores.max() - topo_scores.min() + 1e-8)
    #x = np.arange(0, 1024)
    x = np.arange(0, 64)

    # üîπ Means
    mean_bio = bio_scores_norm.mean()
    mean_topo = topo_scores_norm.mean()

    # üîπ Plot
    plt.figure(figsize=(12, 6))
    plt.fill_between(x, bio_scores_norm, alpha=0.4, color="royalblue")
    plt.fill_between(x, topo_scores_norm, alpha=0.4, color="darkorange")

    # üîπ Mean lines
    plt.axhline(mean_bio, color="royalblue", linestyle="--", linewidth=1.5)
    plt.axhline(mean_topo, color="darkorange", linestyle="--", linewidth=1.5)

    # üîπ Custom legend with mean values
    legend_handles = [
        Patch(facecolor='royalblue', label=f'Bio Mean: {mean_bio:.2f}'),
        Patch(facecolor='darkorange', label=f'Topo Mean: {mean_topo:.2f}')
    ]
    plt.legend(handles=legend_handles, loc='upper left', frameon=False, fontsize=10)

    # üîπ Formatting
    #plt.xlim(0, 1023)
    plt.xlim(0, 63)
    plt.ylim(0, 1)
    plt.xlabel("Feature Index (0 - 63)", fontsize=12)
    plt.ylabel("Normalized Relevance Score", fontsize=12)
    plt.title(title, fontsize=16)
    sns.despine()
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"‚úÖ Saved bio-topo saliency plot to {save_path}")
    else:
        plt.close()

def plot_bio_topo_saliency_cuberoot(bio_scores, topo_scores, title="", save_path=None):
    """
    Plot saliency relevance for bio and topo features using cube root transformed normalized values.
    This enhances low scores and compresses high spikes.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Patch
    import seaborn as sns

    # üîπ Normalize to [0, 1]
    bio_scores_norm = (bio_scores - bio_scores.min()) / (bio_scores.max() - bio_scores.min() + 1e-8)
    topo_scores_norm = (topo_scores - topo_scores.min()) / (topo_scores.max() - topo_scores.min() + 1e-8)

    # üîπ Apply cube root transform
    bio_scores_scaled = np.cbrt(bio_scores_norm)
    topo_scores_scaled = np.cbrt(topo_scores_norm)

    x = np.arange(0, len(bio_scores))

    # üîπ Means (after transform)
    mean_bio = bio_scores_scaled.mean()
    mean_topo = topo_scores_scaled.mean()

    # üîπ Plot
    plt.figure(figsize=(12, 6))
    plt.fill_between(x, bio_scores_scaled, alpha=0.4, color="royalblue")
    plt.fill_between(x, topo_scores_scaled, alpha=0.4, color="darkorange")

    # üîπ Mean lines
    plt.axhline(mean_bio, color="royalblue", linestyle="--", linewidth=1.5)
    plt.axhline(mean_topo, color="darkorange", linestyle="--", linewidth=1.5)

    # üîπ Legend
    legend_handles = [
        Patch(facecolor='royalblue', label=f'Bio Mean: {mean_bio:.2f}'),
        Patch(facecolor='darkorange', label=f'Topo Mean: {mean_topo:.2f}')
    ]
    plt.legend(handles=legend_handles, loc='upper left', frameon=False, fontsize=10)

    # üîπ Formatting
    plt.xlim(0, len(bio_scores) - 1)
    plt.ylim(0, 1)
    plt.xlabel("Feature Index (0 ‚Äì 63)", fontsize=12)
    plt.ylabel("Cube Root Transformed Score", fontsize=12)
    plt.title(title, fontsize=16)
    sns.despine()
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"‚úÖ Saved cube root bio-topo saliency plot to {save_path}")
    else:
        plt.close()

def plot_bio_topo_saliency_log(bio_scores, topo_scores, title="", save_path=None):
    """
    Plot saliency relevance for bio and topo features with log-transformed normalized values.
    Enhances small values and compresses high peaks.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Patch
    import seaborn as sns

    # üîπ Normalize
    bio_scores_norm = (bio_scores - bio_scores.min()) / (bio_scores.max() - bio_scores.min() + 1e-8)
    topo_scores_norm = (topo_scores - topo_scores.min()) / (topo_scores.max() - topo_scores.min() + 1e-8)

    # üîπ Apply log transform
    bio_scores_scaled = np.log1p(bio_scores_norm) / np.log1p(1)  # log1p(x)/log1p(1) keeps range in [0,1]
    topo_scores_scaled = np.log1p(topo_scores_norm) / np.log1p(1)

    x = np.arange(0, len(bio_scores))

    # üîπ Means (after transform)
    mean_bio = bio_scores_scaled.mean()
    mean_topo = topo_scores_scaled.mean()

    # üîπ Plot
    plt.figure(figsize=(12, 6))
    plt.fill_between(x, bio_scores_scaled, alpha=0.4, color="royalblue")
    plt.fill_between(x, topo_scores_scaled, alpha=0.4, color="darkorange")

    # üîπ Mean lines
    plt.axhline(mean_bio, color="royalblue", linestyle="--", linewidth=1.5)
    plt.axhline(mean_topo, color="darkorange", linestyle="--", linewidth=1.5)

    # üîπ Legend
    legend_handles = [
        Patch(facecolor='royalblue', label=f'Bio Mean: {mean_bio:.2f}'),
        Patch(facecolor='darkorange', label=f'Topo Mean: {mean_topo:.2f}')
    ]
    plt.legend(handles=legend_handles, loc='upper right', frameon=False, fontsize=10)

    # üîπ Formatting
    plt.xlim(0, len(bio_scores) - 1)
    plt.ylim(0, 1)
    plt.xlabel("Feature Index (0‚Äì1023)", fontsize=12)
    plt.ylabel("Transformed Relevance Score (log1p)", fontsize=12)
    plt.title(title, fontsize=16)
    sns.despine()
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"‚úÖ Saved log-transformed bio-topo saliency plot to {save_path}")
    else:
        plt.close()

def extract_bio_summary_features_np(features_np):
    num_nodes = features_np.shape[0]
    total_dim = features_np.shape[1]
    summary_features = []

    num_omics = 4
    num_cancers = 16
    features_per_pair = 16

    for o_idx in range(num_omics):
        for c_idx in range(num_cancers):
            base = o_idx * num_cancers * features_per_pair + c_idx * features_per_pair
            if base + features_per_pair > total_dim:
                continue
            group = features_np[:, base:base + features_per_pair]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # ‚ûú (N, 64)

def extract_topo_summary_features_np(features_np):
    num_nodes = features_np.shape[0]
    total_dim = features_np.shape[1]
    summary_features = []

    num_topo_blocks = 4
    num_cancers = 16
    features_per_block = 16

    for t_idx in range(num_topo_blocks):
        for c_idx in range(num_cancers):
            base = t_idx * num_cancers * features_per_block + c_idx * features_per_block
            if base + features_per_block > total_dim:
                continue
            group = features_np[:, base:base + features_per_block]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # ‚ûú (N, 64)

def extract_summary_features_np(features_np):
    """
    Extracts summary features by computing the max of 16-dimensional segments across all (omics, cancer) pairs.
    This version only works with the first 1024 biological features.

    Args:
        features_np (np.ndarray): shape [num_nodes, 2048]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = features_np.shape
    summary_features = []

    assert num_features == 2048, f"Expected 2048 features, got {num_features}"

    # First 1024 for biological features (omics and cancer)
    for o_idx in range(4):  # 4 omics types
        for c_idx in range(16):  # 16 cancer types
            base = o_idx * 16 * 16 + c_idx * 16
            group = features_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)  # [num_nodes, 1]
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # shape: [num_nodes, 64]

def plot_neighbor_relevance_by_mode(
    gene,
    relevance_scores,
    mode,
    neighbor_scores,
    neighbors_dict,
    name_to_index,
    node_id_to_name,
    graph,
    row_labels,
    total_clusters,
    args,
    save_dir="results/gene_prediction/neighbor_feature_contributions/"):
    os.makedirs(save_dir, exist_ok=True)

    if gene not in name_to_index:
        print(f"‚ö†Ô∏è Gene {gene} not found in the graph.")
        return

    node_idx = name_to_index[gene]
    gene_score = neighbor_scores[node_idx]

    # ‚úÖ Get gene cluster based on mode
    if mode == "bio":
        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
    elif mode == "topo":
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()
    else:
        gene_cluster = -1  # fallback if cluster type is unknown

    print(f"[{mode}] {gene} ‚Üí Node {node_idx} | Predicted score: {gene_score:.4f} | Cluster: {gene_cluster}")

    neighbors = neighbors_dict.get(gene, [])
    neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

    relevance_vals = [relevance_scores[i].sum().item() for i in neighbor_indices]
    scores_dict = dict(zip(neighbor_indices, relevance_vals))

    output_path = os.path.join(
        save_dir,
        f"{args.model_type}_{args.net_type}_{gene}_{mode}_neighbor_relevance_epo{args.num_epochs}.png"
    )

    plot_neighbor_relevance(
        neighbor_scores=scores_dict,
        gene_name=f"{gene} (Cluster {gene_cluster})",
        node_id_to_name=node_id_to_name,
        output_path=output_path,
        row_labels=row_labels,
        total_clusters=total_clusters,
        add_legend=False
    )

def plot_saliency_for_gene(
    gene,
    relevance_scores,
    node_idx,
    save_dir,
    args,
    bio_feat_names,
    topo_feat_names):
    bio_1024 = relevance_scores[node_idx]["bio"].cpu().numpy().reshape(1, -1)
    topo_1024 = relevance_scores[node_idx]["topo"].cpu().numpy().reshape(1, -1)

    bio_64 = extract_summary_features_np_skip(bio_1024).squeeze()
    topo_64 = extract_summary_features_np_skip(topo_1024).squeeze()

    # BIO plot
    plot_feature_importance_bio(
        relevance_vector=bio_64,
        feature_names=bio_feat_names,
        node_name=gene,
        output_path=os.path.join(
            save_dir,
            f"{args.model_type}_{args.net_type}_{gene}_bio_feature_importance_epo{args.num_epochs}.png"
        )
    )

    # TOPO plot
    plot_feature_importance_topo(
        relevance_vector=topo_64,
        feature_names=topo_feat_names,
        node_name=gene,
        output_path=os.path.join(
            save_dir,
            f"{args.model_type}_{args.net_type}_{gene}_topo_feature_importance_epo{args.num_epochs}.png"
        )
    )

def plot_feature_importance_bio_ori(relevance_vector, feature_names, node_name=None, output_path="plots"):
    """
    Plot biological feature importance in OMICS:CANCER format using alphabetical order of cancer names.

    Parameters:
        relevance_vector (array-like): Relevance scores.
        feature_names (list of str): Feature names in the format Cancer_Omics.
        node_name (str, optional): Name of the node (used for title).
        output_path (str): Path to save the plot.
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import os

    # Define mapping from TCGA codes to cancer names
    cancer_map = {
        'BLCA': 'Bladder', 'BRCA': 'Breast', 'CESC': 'Cervix', 'COAD': 'Colon',
        'ESCA': 'Esophagus', 'HNSC': 'HeadNeck', 'KIRC': 'KidneyCC', 'KIRP': 'KidneyPC',
        'LIHC': 'Liver', 'LUAD': 'LungAD', 'LUSC': 'LungSC', 'PRAD': 'Prostate',
        'READ': 'Rectum', 'STAD': 'Stomach', 'THCA': 'Thyroid', 'UCEC': 'Uterus'
    }

    # Sort cancer codes by alphabetical order of their full names
    sorted_cancer_codes = sorted(cancer_map.keys(), key=lambda k: cancer_map[k])
    omics_order = ['cna', 'ge', 'meth', 'mf']
    column_labels = [f"{omics.upper()}:{cancer}" for omics in omics_order for cancer in sorted_cancer_codes]

    # Validate input
    if len(relevance_vector) != len(feature_names):
        raise ValueError(f"Mismatch: {len(relevance_vector)} values vs {len(feature_names)} names")

    # Build DataFrame
    df = pd.DataFrame({
        "feature": feature_names,
        "relevance": relevance_vector
    })

    df["omics_type"] = df["feature"].apply(lambda x: x.split("_")[1].lower())
    df["cancer_type"] = df["feature"].apply(lambda x: x.split("_")[0])
    df["formatted_label"] = df.apply(lambda row: f"{row['omics_type'].upper()}:{row['cancer_type']}", axis=1)

    # Reorder using column_labels
    df["order"] = df["formatted_label"].apply(lambda x: column_labels.index(x) if x in column_labels else -1)
    df = df[df["order"] != -1].sort_values("order")

    # Color mapping
    omics_color = {
        'cna': '#1F77B4',
        'ge': '#9467BD',
        'meth': '#2CA02C',
        'mf': '#D62728'
    }
    df["bar_color"] = df["omics_type"].map(omics_color)

    # Plotting
    plt.figure(figsize=(24, 5))
    ax = sns.barplot(x="formatted_label", y="relevance", data=df, palette=df["bar_color"].tolist())

    num_bars = len(df)
    margin = 0.75
    ax.set_xlim(-margin, num_bars - 1 + margin)

    ax.set_title(node_name if node_name else "", fontsize=16)
    ax.set_ylabel("Saliency score", fontsize=16)
    ax.set_xlabel("Feature (Omics: Cancer)", fontsize=16)

    # Color tick labels
    for tick, omics in zip(ax.get_xticklabels(), df["omics_type"]):
        tick.set_color(omics_color.get(omics, "black"))

    plt.xticks(rotation=90, fontsize=16)
    plt.yticks(fontsize=16)
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved BIO plot to {output_path}")

def plot_clusterwise_sorted_heatmaps(
    args,
    relevance_scores,          # [N, F] relevance scores for top genes
    row_labels,            # [N] cluster assignments for top genes
    gene_names,                # List[str], top-k gene names
    omics_splits,              # Dict[str, Tuple[int, int]], omics feature ranges
    output_dir,                # str, directory to save the heatmaps
    model_type,                # str, for filename
    net_type,                  # str, for filename
    epoch,                     # int, for filename
    tag="bio"                  # 'bio' or 'topo'
):
    os.makedirs(output_dir, exist_ok=True)
    num_clusters = len(set(row_labels))

    for cluster_id in range(num_clusters):
        # === Subset ===
        cluster_indices = np.where(row_labels == cluster_id)[0]
        if len(cluster_indices) == 0:
            continue

        cluster_scores = relevance_scores[cluster_indices]  # [n_genes_cluster, F]
        cluster_gene_names = [gene_names[i] for i in cluster_indices]

        # === Sort genes (rows) by total relevance ===
        gene_totals = cluster_scores.sum(axis=1)
        sorted_gene_idx = np.argsort(-gene_totals)
        cluster_scores_sorted = cluster_scores[sorted_gene_idx]
        sorted_gene_names = [cluster_gene_names[i] for i in sorted_gene_idx]

        # === Sort columns within each omics group ===
        sorted_columns = []
        for _, (start, end) in omics_splits.items():
            group_scores = cluster_scores_sorted[:, start:end]
            col_totals = group_scores.sum(axis=0)
            sorted_group_columns = np.argsort(-col_totals) + start
            sorted_columns.extend(sorted_group_columns)

        # === Apply column sorting ===
        final_scores = cluster_scores_sorted[:, sorted_columns]

        # === Plot ===
        plt.figure(figsize=(12, max(4, 0.25 * len(sorted_gene_names))))
        sns.heatmap(
            final_scores,
            cmap='YlGnBu',
            yticklabels=sorted_gene_names,
            xticklabels=False  # You can set True and relabel if you have feature names
        )
        plt.title(f"Cluster {cluster_id} ({tag}) - Relevance Heatmap")
        plt.xlabel("Sorted Features")
        plt.ylabel("Genes")

        save_path = os.path.join(
            output_dir,
            f"{model_type}_{net_type}_cluster_{cluster_id}_{tag}_sorted_heatmap_epo{epoch}.png"
        )
        plt.tight_layout()
        plt.savefig(save_path, dpi=300)
        plt.close()

def get_cluster_color(cluster_id, total_clusters=10):
    """
    Get a color from a colormap based on the cluster ID.
    """
    cmap = cm.get_cmap('tab10') if total_clusters <= 10 else cm.get_cmap('tab20')
    return cmap(cluster_id % total_clusters)

def plot_neighbor_relevance_ori(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False):
    """
    Plots the relevance score of neighbors for a confirmed gene, with optional coloring by cluster.
    Filters neighbors with relevance > 0.05, selects top 10, and normalizes scores.
    """
    # Filter and get top-10
    filtered = {k: v for k, v in neighbor_scores.items() if v > 0.05}
    if len(filtered) < 10:
        print(f"‚ö†Ô∏è Less than 10 neighbors > 0.05 for {gene_name}")
        return

    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])
    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"Node {nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores to [0.025, 0.975]
    norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
    norm_scores = norm_scores * 0.95 + 0.025

    # Assign bar colors based on clusters
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None:
            cluster_id = int(row_labels[nid])
            cluster_ids.append(cluster_id)
            colors.append(CLUSTER_COLORS.get(cluster_id % total_clusters, "gray"))
        else:
            colors.append("gray")

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved neighbor relevance plot to {output_path}")

def process_predictions(ranking, args, drivers_file_path, oncokb_file_path, ongene_file_path, ncg_file_path, intogen_file_path, node_names, non_labeled_nodes):
    """
    Process and save the predicted driver genes, confirmed sources, and known drivers.
    
    Args:
    - ranking: List of tuples (gene, score) representing ranked predictions.
    - args: Argument object containing model and network type, and score threshold.
    - drivers_file_path, oncokb_file_path, ongene_file_path, ncg_file_path, intogen_file_path: Paths to the confirmation gene files.
    - node_names, non_labeled_nodes: Information about node names and indices for matching.
    """
    # Load data from the confirmation files
    oncokb_genes = load_gene_set(oncokb_file_path)
    ongene_genes = load_gene_set(ongene_file_path)
    ncg_genes = load_gene_set(ncg_file_path)
    intogen_genes = load_gene_set(intogen_file_path)

    # Threshold for the score
    score_threshold = args.score_threshold

    confirmed_predictions = []
    predicted_genes = []

    for node, score in ranking:
        if score >= score_threshold:
            sources = []  # Accumulate sources confirming the gene
            if node in oncokb_genes:
                sources.append("OncoKB")
            if node in ongene_genes:
                sources.append("OnGene")
            if node in ncg_genes:
                sources.append("NCG")
            if node in intogen_genes:
                sources.append("IntOGen")
            if sources:  # If the gene is confirmed by at least one source
                confirmed_predictions.append((node, score, ", ".join(sources)))
            predicted_genes.append((node, score, ", ".join(sources) if sources else ""))

    # Save predictions to a CSV file
    save_predictions_to_csv(predicted_genes, 'results/gene_prediction/', args.model_type, args.net_type, args.num_epochs)
    save_confirmed_predictions_to_csv(confirmed_predictions, 'results/gene_prediction/', args.model_type, args.net_type, args.num_epochs)

    # Load known cancer driver genes
    with open(drivers_file_path, 'r') as f:
        known_drivers = set(line.strip() for line in f)

    # Collect predicted cancer driver genes that match the known drivers
    predicted_driver_genes = [node_names[i] for i in non_labeled_nodes if node_names[i] in known_drivers]

    # Save the predicted known cancer driver genes to a CSV file
    save_predicted_known_drivers(predicted_driver_genes, 'results/gene_prediction/', args.model_type, args.net_type, args.num_epochs)

def plot_pcg_cancer_genes(
    clusters,
    predicted_cancer_genes_count,
    total_genes_per_cluster,
    node_names,
    row_labels,
    output_path):
    """
    Plots the percentage of predicted cancer genes per cluster.
    """

    # Convert to sorted array
    clusters = np.array(sorted(total_genes_per_cluster.keys()))
    total_genes_array = np.array([total_genes_per_cluster[c] for c in clusters])
    predicted_counts = np.array([predicted_cancer_genes_count.get(c, 0) for c in clusters])

    # Compute percentages
    percent_predicted = np.divide(predicted_counts, total_genes_array, where=total_genes_array > 0)

    # Prepare bar colors
    colors = [CLUSTER_COLORS.get(c, '#333333') for c in clusters]

    # Plot
    fig, ax = plt.subplots(figsize=(8, 5))
    bars = ax.bar(clusters, percent_predicted, color=colors, edgecolor='black')

    # Annotate with raw count
    for bar, cluster_id in zip(bars, clusters):
        height = bar.get_height()
        count = predicted_cancer_genes_count.get(cluster_id, 0)
        ax.text(bar.get_x() + bar.get_width() / 2, height, str(count),
                ha='center', va='bottom', fontsize=16, fontweight='bold')

    # Left margin space
    num_clusters = len(clusters)
    ax.set_xlim(-0.55, num_clusters - 0.65)

    # Labels and formatting
    ax.set_ylabel("Percent of PCGs", fontsize=20)
    plt.xlabel("")  # No xlabel here
    plt.xticks(clusters, fontsize=16)
    plt.yticks(fontsize=16)
    ax.set_ylim(0, max(percent_predicted) + 0.1)

    sns.despine()
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"‚úÖ Plot saved to {output_path}")

def plot_kcg_cancer_genes(clusters, kcg_count, total_genes_per_cluster, node_names, row_labels, output_path):

    cluster_ids = sorted(clusters)
    total = [total_genes_per_cluster[c] for c in cluster_ids]
    kcgs = [kcg_count.get(c, 0) for c in cluster_ids]
    proportions = [k / t if t > 0 else 0 for k, t in zip(kcgs, total)]

    plt.figure(figsize=(8, 5))
    bars = plt.bar(cluster_ids, proportions, 
                   color=[CLUSTER_COLORS.get(c, '#333333') for c in cluster_ids],
                   edgecolor='black')

    # Annotate each bar with the raw KCG count
    for bar, cluster_id in zip(bars, cluster_ids):
        height = bar.get_height()
        count = kcg_count.get(cluster_id, 0)
        plt.text(bar.get_x() + bar.get_width() / 2, height, str(count), 
                 ha='center', va='bottom', fontsize=16, fontweight='bold')

    ax = plt.gca()
    num_clusters = len(cluster_ids)
    ax.set_xlim(-0.55, num_clusters - 0.65)

    # Formatting
    ##plt.xlabel("Cluster ID", fontsize=16)
    plt.ylabel("Percent of KCGs", fontsize=20)
    plt.xlabel("")
    plt.xticks(cluster_ids, fontsize=16)
    plt.yticks(fontsize=16)
    plt.ylim(0, max(proportions) + 0.1)

    sns.despine()  # üîª Remove top/right spines

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

def plot_interactions_with_pcgs(data, output_path):
    """
    Creates a box plot with individual data points showing 
    the number of interactions with predicted cancer genes (PCGs).
    """
    import seaborn as sns
    import matplotlib.pyplot as plt

    plt.figure(figsize=(8, 5))
    sns.set_style("white")

    unique_clusters = sorted(data['Cluster'].unique())
    cluster_color_map = {cluster_id: CLUSTER_COLORS[cluster_id] for cluster_id in unique_clusters}

    ax = sns.boxplot(
        x='Cluster', y='Interactions', data=data, 
        hue='Cluster', palette=cluster_color_map, showfliers=False
    )

    sns.stripplot(
        x='Cluster', y='Interactions', data=data, 
        color='black', alpha=0.2, jitter=True, size=1.5
    )

    if ax.get_legend() is not None:
        ax.get_legend().remove()

    # üëâ Adjust x-axis limits for more space on the left
    num_clusters = len(unique_clusters)
    ax.set_xlim(-0.55, num_clusters - 0.65)  # Or adjust -0.4, -0.5, etc. for more space

    plt.ylabel("Number of interactions with PCGs", fontsize=20)
    plt.xlabel("")
    plt.xticks(rotation=0, ha="right", fontsize=16)
    plt.yticks(fontsize=16)
    plt.ylim(0, 50)

    sns.despine()  # üîª Remove top/right spines

    plt.savefig(output_path, dpi=300, bbox_inches="tight")  
    plt.close()

    print(f"‚úÖ Plot saved to {output_path}")

def plot_interactions_with_kcgs(data, output_path):
    """
    Creates a box plot with individual data points showing 
    the number of interactions with known cancer genes (KCGs).
    """
    import seaborn as sns
    import matplotlib.pyplot as plt

    plt.figure(figsize=(8, 5))
    sns.set_style("white")

    unique_clusters = sorted(data['Cluster'].unique())
    cluster_color_map = {cluster_id: CLUSTER_COLORS[cluster_id] for cluster_id in unique_clusters}

    ax = sns.boxplot(
        x='Cluster', y='Interactions', data=data, 
        hue='Cluster', palette=cluster_color_map, showfliers=False
    )

    sns.stripplot(
        x='Cluster', y='Interactions', data=data, 
        color='black', alpha=0.2, jitter=True, size=1.5
    )

    if ax.get_legend() is not None:
        ax.get_legend().remove()

    # üëâ Adjust x-axis limits for more space on the left
    num_clusters = len(unique_clusters)
    ax.set_xlim(-0.55, num_clusters - 0.65)  # Or adjust -0.4, -0.5, etc. for more space

    plt.ylabel("Number of interactions with KCGs", fontsize=20)
    plt.xlabel("")
    plt.xticks(rotation=0, ha="right", fontsize=16)
    plt.yticks(fontsize=16)
    plt.ylim(0, 50)

    sns.despine()  # üîª Remove top/right spines

    plt.savefig(output_path, dpi=300, bbox_inches="tight")  
    plt.close()

    print(f"‚úÖ Plot saved to {output_path}")



def plot_enriched_term_counts(enrichment_results, output_path, model_type, net_type, num_epochs, bio_color='#1f77b4', topo_color='#ff7f0e'):
    """
    Plot bar chart of the number of enriched terms per cluster for bio and topo clusters,
    with x-axis labels colored according to their type.

    Parameters:
        enrichment_results (dict): Dictionary with enrichment results for 'bio' and 'topo' clusters.
        output_path (str): Path to save the output plot.
        model_type (str): Model type for naming the file.
        net_type (str): Network type for naming the file.
        num_epochs (int): Number of epochs for naming the file.
        bio_color (str): Color for bio bars and labels.
        topo_color (str): Color for topo bars and labels.
        
        plt.plot(bio_scores_rel, label='Bio', color='#1f77b4')
        plt.plot(topo_scores_rel, label='Topo', color='#ff7f0e')
    """
    fig, ax = plt.subplots(figsize=(10, 6))

    bars = []
    xtick_colors = []
    xtick_labels = []

    for cluster_type in ['bio', 'topo']:
        cluster_ids = list(enrichment_results[cluster_type].keys())
        term_counts = [len(res) for res in enrichment_results[cluster_type].values()]
        labels = [f"{cluster_type.capitalize()}_{i}" for i in cluster_ids]

        color = bio_color if cluster_type == 'bio' else topo_color
        bar_container = ax.bar(labels, term_counts, label=cluster_type, color=color)
        bars.extend(bar_container)

        xtick_labels.extend(labels)
        xtick_colors.extend([color] * len(labels))

    # Adjust x-axis limits
    ax.set_xlim(-0.65, len(bars) - 0.5)

    # Labeling
    ax.set_ylabel("Number of enriched terms", fontsize=28)
    ##ax.set_title("Functional Coherence: Enriched Term Counts per Cluster", fontsize=28)

    # Set custom x-tick labels and colors
    ax.set_xticks(range(len(xtick_labels)))
    ax.set_xticklabels(xtick_labels, rotation=90, fontsize=20)
    for tick_label, color in zip(ax.get_xticklabels(), xtick_colors):
        tick_label.set_color(color)

    # Set y-tick font size
    ax.tick_params(axis='y', labelsize=20)

    # Clean up spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    ##ax.legend(frameon=False, fontsize=24)

    plt.tight_layout()
    filename = f"{model_type}_{net_type}_term_counts_barplot_epo{num_epochs}.png"
    plt.savefig(os.path.join(output_path, filename), dpi=300)
    plt.close()

    print(f"Bar plot saved to {os.path.join(output_path, filename)}")

def plot_shared_enriched_pathways_venn(enrichment_results, output_path, model_type, net_type, num_epochs, bio_color='#1f77b4', topo_color='#ff7f0e'):
    """
    Plots a Venn diagram showing the overlap of enriched pathways between bio and topo clusters.

    Parameters:
        enrichment_results (dict): Dictionary with enrichment DataFrames under 'bio' and 'topo'.
        output_path (str): Path to save the output plot.
        model_type (str): Model type for naming the file.
        net_type (str): Network type for naming the file.
        num_epochs (int): Number of epochs for naming the file.
        bio_color (str): Color for the bio set in the Venn diagram.
        topo_color (str): Color for the topo set in the Venn diagram.
    """
    bio_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['bio'].values() if not df.empty], [])
    )
    topo_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['topo'].values() if not df.empty], [])
    )

    plt.figure(figsize=(8, 8))
    venn = venn2(
        [bio_terms, topo_terms],
        set_labels=('Bio', 'Topo'),
        set_colors=(bio_color, topo_color),
        alpha=0.7
    )

    # Set font size for all Venn labels and subset counts
    for text in venn.set_labels:
        if text:
            text.set_fontsize(28)
    for text in venn.subset_labels:
        if text:
            text.set_fontsize(28)

    plt.title("Overlap of enriched pathways", fontsize=16)

    filename = f"{model_type}_{net_type}_shared_pathways_venn_epo{num_epochs}.png"
    venn_path = os.path.join(output_path, filename)
    plt.savefig(venn_path, dpi=300)
    plt.close()

    print(f"Venn diagram saved to {venn_path}")

def plot_contingency_matrix(row_labels_bio_topk, row_labels_topo_topk, ari_score, nmi_score, output_dir, args):
    """
    Plots a contingency matrix comparing bio and topo cluster labels.

    Parameters:
    - row_labels_bio_topk: Cluster labels from bio features.
    - row_labels_topo_topk: Cluster labels from topo features.
    - ari_score: Adjusted Rand Index between the clusterings.
    - nmi_score: Normalized Mutual Information score.
    - output_dir: Directory to save the plot.
    - args: Arguments containing model type, net type, and epoch count.
    """
    from sklearn.metrics import confusion_matrix
    import seaborn as sns
    import matplotlib.pyplot as plt
    import os

    # === Contingency matrix ===
    contingency = confusion_matrix(row_labels_bio_topk, row_labels_topo_topk)

    # Plot heatmap of confusion matrix
    plt.figure(figsize=(8, 8))
    sns.heatmap(contingency, annot=True, fmt='d', cmap='BuPu', cbar=False, annot_kws={"fontsize": 20})

    plt.title(f"Contingency Matrix: Bio vs Topo\n(ARI={ari_score:.2f}, NMI={nmi_score:.2f})", fontsize=30)
    plt.xlabel("Topo clusters", fontsize=28)
    plt.ylabel("Bio clusters", fontsize=28)

    plt.xticks([])
    plt.yticks([])

    # Save plot
    contingency_plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_contingency_matrix_epo{args.num_epochs}.png"
    )
    plt.tight_layout()
    plt.savefig(contingency_plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ Contingency matrix saved to {contingency_plot_path}")

def plot_omics_barplot_bio(df, output_path=None):
    """
    Plot omics relevance for biological features with format like 'MF:BRCA'.
    """
    omics_order = ['cna', 'ge', 'meth', 'mf']
    omics_colors = {
        'cna': '#9370DB',    # purple
        'ge': '#228B22',      # dark green
        'meth': '#00008B',   # dark blue
        'mf': '#b22222',     # dark red
    }

    # Extract 'Omics' and 'Cancer' from features like 'MF:BRCA'
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    omics_relevance = df.groupby('Omics')['Relevance'].sum().reindex(omics_order)

    _plot_bar(omics_relevance, omics_colors, omics_order, output_path)

def plot_omics_barplot_topo(df, output_path=None):
    """
    Plot omics relevance for topological features with format like 'BRCA_mf'.
    """
    omics_order = ['cna', 'ge', 'meth', 'mf']
    omics_colors = {
        'cna': '#9370DB',    # purple
        'ge': '#228B22',      # dark green
        'meth': '#00008B',   # dark blue
        'mf': '#b22222',     # dark red
    }

    # Extract 'Cancer' and 'Omics' from features like 'BRCA_mf'
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    omics_relevance = df.groupby('Omics')['Relevance'].sum().reindex(omics_order)

    _plot_bar(omics_relevance, omics_colors, omics_order, output_path)

def plot_gene_feature_contributions_topo_(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    # Barplot of all 64 topo features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_topo(df, barplot_path)

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def save_and_plot_confirmed_genes_topo_(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="topo",
    confirmed_gene_path="data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their topological feature contributions.
    """

    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{cancer}_{omics}" for cancer in cancer_names for omics in omics_order]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    ##summary_feature_relevance = extract_summary_features_np_topo(summary_feature_relevance)

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        # plot_path = os.path.join(
        #     plot_dir,
        #     f"{args.model_type}_{args.net_type}_{gene_name}_{tag}_confirmed_feature_contributions_epo{args.num_epochs}.png"
        # )
        # cluster_id = row_labels_topk[idx].item()
        # plot_gene_feature_contributions_topo(
        #     #gene_name=gene_name,
        #     gene_name=f"{gene_name} (Cluster {cluster_id})",
        #     relevance_vector=relevance_vector,
        #     feature_names=feature_names,
        #     output_path=plot_path,
        #     score=score
        # )
        output_path = os.path.join(
            "results/gene_prediction/topo_confirmed_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_feature_contributions_epo{args.num_epochs}.png"
        )

        plot_gene_feature_contributions_topo(
            gene_name=gene,
            relevance_vector=importance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=gene_cluster,
            output_path=output_path
        )

def load_ground_truth_cancer_genes(file_path):
    """
    Load ground truth cancer genes from a file.
    
    Args:
        file_path (str): Path to the ground truth cancer gene file.
    
    Returns:
        set: A set containing ground truth cancer gene names.
    """
    with open(file_path, 'r') as f:
        return set(line.strip() for line in f)

def compute_node_saliency(model, graph, features, node_indices=None, use_abs=True, normalize=True):
    """
    Computes a fast gradient-based LRP approximation for selected nodes.

    Args:
        model: Trained GNN model
        graph: DGLGraph
        features: Node features (Tensor or numpy array)
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select (probs > 0.0)
        use_abs: Whether to take absolute value of gradients (recommended)
        normalize: Whether to normalize relevance scores (per node)

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features]
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            probs[idx].backward(retain_graph=(i != len(node_indices) - 1))

            grads = features.grad[idx]
            relevance = grads.abs() if use_abs else grads

            if normalize:
                norm = relevance.norm(p=1)  # L1 norm
                if norm > 0:
                    relevance = relevance / norm

            relevance_scores[idx] = relevance.detach()

    return relevance_scores

def compute_neighbor_saliency(model, graph, features, node_idx):
    features = features.clone().detach().requires_grad_(True)
    output = model(graph, features)
    score = output[node_idx].max()  # pick the score you want
    model.zero_grad()
    score.backward()
    
    # Find neighbors
    neighbors = graph.successors(node_idx)  # if directed, or .neighbors(node_idx) for undirected

    neighbor_saliencies = {}
    for n in neighbors:
        neighbor_saliencies[n] = features.grad[n].abs().sum().item()
    
    return neighbor_saliencies

def plot_neighbor_saliency_heatmap(
    graph,
    confirmed_genes,
    node_names,
    name_to_index,
    relevance_scores,
    omics_splits,
    output_path="neighbor_saliency_heatmap.png"
    ):
    heatmap_data = []
    heatmap_labels = []

    for gene in confirmed_genes:
        if gene not in name_to_index:
            continue
        idx = name_to_index[gene]
        
        # Get relevance for the confirmed gene
        gene_relevance = relevance_scores[idx].cpu().numpy()
        heatmap_data.append(gene_relevance)
        heatmap_labels.append(f"{gene} (self)")
        
        # Get neighbors
        neighbors = graph.successors(idx).tolist()
        for neighbor_idx in neighbors:
            neighbor_name = node_names[neighbor_idx]
            neighbor_relevance = relevance_scores[neighbor_idx].cpu().numpy()
            heatmap_data.append(neighbor_relevance)
            heatmap_labels.append(f"{neighbor_name} (nbr)")
    
    # Stack into matrix
    heatmap_data = np.vstack(heatmap_data)
    
    # Optional: min-max normalization per row
    heatmap_data_norm = (heatmap_data - heatmap_data.min(axis=1, keepdims=True)) / \
                        (heatmap_data.max(axis=1, keepdims=True) - heatmap_data.min(axis=1, keepdims=True) + 1e-8)
    
    # Limit number of neighbors plotted
    MAX_NEIGHBORS = 100  # or whatever you want

    if heatmap_data_norm.shape[0] > MAX_NEIGHBORS:
        neighbor_importance = np.abs(heatmap_data_norm).mean(axis=1)
        top_indices = np.argsort(-neighbor_importance)[:MAX_NEIGHBORS]
        heatmap_data_norm = heatmap_data_norm[top_indices]
        heatmap_labels = [heatmap_labels[i] for i in top_indices]

    # ‚úÖ NOW set figure size based on the reduced number of labels
    plt.figure(figsize=(14, max(8, len(heatmap_labels) * 0.3)))

    # Create heatmap
    sns.heatmap(heatmap_data_norm, cmap="viridis", yticklabels=heatmap_labels, xticklabels=False)
    plt.title("Neighbor Saliency Heatmap")
    plt.xlabel("Omics Feature Dimension")
    plt.ylabel("Confirmed Genes + Neighbors")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"Neighbor saliency heatmap saved at {output_path}")

def build_subgraph(graph, target_idx, neighbors):
    G = nx.Graph()
    G.add_node(target_idx)

    for neighbor_idx in neighbors:
        G.add_node(neighbor_idx)
        G.add_edge(target_idx, neighbor_idx)

    return G

def plot_saliency_graph_multiomics(G, target_idx, bio_saliencies, topo_saliencies, node_names, save_path=None):
    pos = nx.spring_layout(G, seed=42)

    edges = list(G.edges())
    bio_scores = np.array([bio_saliencies.get(v, 0) for u, v in edges])
    topo_scores = np.array([topo_saliencies.get(v, 0) for u, v in edges])
    
    total_scores = bio_scores + topo_scores
    total_scores = np.clip(total_scores, 1e-6, None)  # Avoid zero division

    # Calculate color mixing
    bio_ratio = bio_scores / total_scores
    topo_ratio = topo_scores / total_scores

    edge_colors = [(topo_ratio[i], 0, bio_ratio[i]) for i in range(len(edges))]  # RGB: (red, 0, blue)
    edge_widths = total_scores / total_scores.max() * 5  # Max width = 5

    # Draw nodes
    node_colors = []
    for node in G.nodes():
        if node == target_idx:
            node_colors.append('gold')
        else:
            node_colors.append('lightgray')

    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=600)

    # Draw edges
    for (u, v), color, width in zip(edges, edge_colors, edge_widths):
        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=width, edge_color=[color])

    # Draw labels
    labels = {node: node_names[node] for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels=labels, font_size=10)

    plt.title(f"Neighbor Bio vs Topo Saliency for {node_names[target_idx]}", fontsize=16)
    plt.axis('off')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"‚úÖ Saved multi-omics saliency graph to {save_path}")
    else:
        plt.close()

def plot_dynamic_sankey_topo_clusterlevel_each_gene(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    relevance_scores,
    row_labels,
    total_clusters,
    ):
    # Mapping node IDs and names
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    # Prepare folder to save
    output_dir = os.path.join("results/gene_prediction/topo_dynamic_sankey_clusterlevel")
    os.makedirs(output_dir, exist_ok=True)

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_score = scores[node_idx]

        # ‚úÖ Get cluster label from graph
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()
        print(f"{gene} ‚Üí Node {node_idx} | Topo score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores_dict = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:  # bounds check
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        # Top neighbors (limit to 10)
        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # Sankey nodes
        labels = [f"{gene} (Cluster {gene_cluster})"]
        colors = [f"rgba({(gene_cluster/total_clusters)*255},100,150,0.8)"]

        for idx in top_neighbors.keys():
            neighbor_name = node_id_to_name[idx]
            neighbor_cluster = row_labels[idx]
            labels.append(f"{neighbor_name} (Cluster {neighbor_cluster})")
            colors.append(f"rgba({(neighbor_cluster/total_clusters)*255},180,100,0.8)")

        # Sankey links
        source = [0] * len(top_neighbors)  # source = gene
        target = list(range(1, len(top_neighbors)+1))  # targets = neighbors
        value = list(top_neighbors.values())

        # Build Sankey figure
        fig = go.Figure(data=[go.Sankey(
            node=dict(
                pad=15,
                thickness=20,
                line=dict(color="black", width=0.5),
                label=labels,
                color=colors
            ),
            link=dict(
                source=source,
                target=target,
                value=value,
            ))])

        fig.update_layout(
            title_text=f"Topo Dynamic Sankey for {gene}",
            font_size=10,
            width=800,
            height=600
        )

        # Save HTML
        output_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_neighbor_sankey_epo{args.num_epochs}.html"
        )
        fig.write_html(output_path)
        fig.write_image(output_path.replace('.html', '.png'))

        print(f"‚úÖ Sankey saved: {output_path}")

def plot_dynamic_sankey_bio_clusterlevel_not_fixed_colors(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores
):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    # Mapping cluster-to-cluster scores
    cluster_to_cluster_score = {}

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[node_idx].item()

        neighbors = neighbors_dict.get(gene, [])

        neighbor_scores_dict = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        for rel_idx, rel_score in top_neighbors.items():
            neighbor_cluster = row_labels[rel_idx].item()

            key = (gene_cluster, neighbor_cluster)
            cluster_to_cluster_score[key] = cluster_to_cluster_score.get(key, 0) + rel_score

    if not cluster_to_cluster_score:
        print("‚ö†Ô∏è No cluster-to-cluster links to plot.")
        return

    # Now prepare Sankey inputs
    clusters_involved = set()
    for (src_c, tgt_c) in cluster_to_cluster_score.keys():
        clusters_involved.add(src_c)
        clusters_involved.add(tgt_c)
    clusters_involved = sorted(list(clusters_involved))

    cluster_id_to_label = {c: f"C{c}" for c in clusters_involved}
    label_to_index = {f"C{c}": i for i, c in enumerate(clusters_involved)}

    source = []
    target = []
    value = []
    label = [f"C{c}" for c in clusters_involved]
    color = [f"rgba({(c*37)%255}, {(c*83)%255}, {(c*131)%255}, 0.8)" for c in clusters_involved]

    for (src_c, tgt_c), score in cluster_to_cluster_score.items():
        source.append(label_to_index[f"C{src_c}"])
        target.append(label_to_index[f"C{tgt_c}"])
        value.append(score)

    sankey_fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=label,
            color=color
        ),
        link=dict(
            source=source,
            target=target,
            value=value
        )
    )])

    sankey_fig.update_layout(
        title_text=f"Cluster ‚Üí Neighbor Cluster (Bio) - {args.model_type}_{args.net_type}",
        font_size=10,
        width=1200,
        height=800
    )

    output_dir = "results/gene_prediction/bio_dynamic_sankey_clusterlevel/"
    os.makedirs(output_dir, exist_ok=True)
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_confirmed_dynamic_sankey_clusterlevel_epo{args.num_epochs}.html"
    )
    sankey_fig.write_html(plot_path)
    print(f"‚úÖ Cluster-level Sankey diagram saved to {plot_path}")

def get_neighbors_gene_names(graph, node_names, name_to_index, genes):
    neighbors_dict = {}
    for gene in genes:
        if gene in name_to_index:
            idx = name_to_index[gene]
            neighbors = graph.successors(idx).tolist()
            neighbor_names = [node_names[n] for n in neighbors]
            neighbors_dict[gene] = neighbor_names
    return neighbors_dict

def plot_dynamic_sankey_bio_clusterlevel(
    args,
    graph,
    node_names_topk,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores
):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names_topk, name_to_index, confirmed_genes)

    # Mapping cluster-to-cluster scores
    cluster_to_cluster_score = {}

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[node_idx].item()

        neighbors = neighbors_dict.get(gene, [])

        neighbor_scores_dict = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        for rel_idx, rel_score in top_neighbors.items():
            neighbor_cluster = row_labels[rel_idx].item()

            key = (gene_cluster, neighbor_cluster)
            cluster_to_cluster_score[key] = cluster_to_cluster_score.get(key, 0) + rel_score

    if not cluster_to_cluster_score:
        print("‚ö†Ô∏è No cluster-to-cluster links to plot.")
        return

    # Now prepare Sankey inputs
    clusters_involved = set()
    for (src_c, tgt_c) in cluster_to_cluster_score.keys():
        clusters_involved.add(src_c)
        clusters_involved.add(tgt_c)
    clusters_involved = sorted(list(clusters_involved))

    cluster_id_to_label = {c: f"C{c}" for c in clusters_involved}
    label_to_index = {f"C{c}": i for i, c in enumerate(clusters_involved)}

    source = []
    target = []
    value = []
    label = [f"C{c}" for c in clusters_involved]
    color = [CLUSTER_COLORS.get(c, "#CCCCCC") for c in clusters_involved]  # üõ† fixed color

    for (src_c, tgt_c), score in cluster_to_cluster_score.items():
        source.append(label_to_index[f"C{src_c}"])
        target.append(label_to_index[f"C{tgt_c}"])
        value.append(score)

    sankey_fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=label,
            color=color
        ),
        link=dict(
            source=source,
            target=target,
            value=value
        )
    )])

    sankey_fig.update_layout(
        title_text=f"Cluster ‚Üí Neighbor Cluster (Bio) - {args.model_type}_{args.net_type}",
        font_size=10,
        width=1200,
        height=800
    )

    output_dir = "results/gene_prediction/bio_dynamic_sankey_clusterlevel/"
    os.makedirs(output_dir, exist_ok=True)
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_confirmed_dynamic_sankey_clusterlevel_epo{args.num_epochs}.html"
    )
    sankey_fig.write_html(plot_path)
    print(f"‚úÖ Cluster-level Sankey diagram saved to {plot_path}")

def plot_dynamic_sankey_topo_clusterlevel(
    args,
    graph,
    node_names_topk,
    name_to_index,
    confirmed_genes,
    scores,
    relevance_scores,
    row_labels,
    total_clusters
):
    # Mapping node IDs and names
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names_topk, name_to_index, confirmed_genes)

    # Mapping cluster-to-cluster scores
    cluster_to_cluster_score = {}

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_score = scores[node_idx]

        # ‚úÖ Get topo cluster from graph
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores_dict = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:  # bounds check
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors for {gene}.")
            continue

        # Top neighbors (limit to 10)
        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        for rel_idx, rel_score in top_neighbors.items():
            neighbor_cluster = row_labels[rel_idx].item()

            key = (gene_cluster, neighbor_cluster)
            cluster_to_cluster_score[key] = cluster_to_cluster_score.get(key, 0) + rel_score

    if not cluster_to_cluster_score:
        print("‚ö†Ô∏è No cluster-to-cluster links to plot.")
        return

    # Prepare Sankey inputs
    clusters_involved = set()
    for (src_c, tgt_c) in cluster_to_cluster_score.keys():
        clusters_involved.add(src_c)
        clusters_involved.add(tgt_c)
    clusters_involved = sorted(list(clusters_involved))

    cluster_id_to_label = {c: f"C{c}" for c in clusters_involved}
    label_to_index = {f"C{c}": i for i, c in enumerate(clusters_involved)}

    source = []
    target = []
    value = []
    label = [f"C{c}" for c in clusters_involved]
    color = [CLUSTER_COLORS.get(c, "#CCCCCC") for c in clusters_involved]  # üõ† fixed color

    for (src_c, tgt_c), score in cluster_to_cluster_score.items():
        source.append(label_to_index[f"C{src_c}"])
        target.append(label_to_index[f"C{tgt_c}"])
        value.append(score)

    # Build Sankey figure
    sankey_fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=label,
            color=color
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
        ))])

    sankey_fig.update_layout(
        title_text=f"Cluster ‚Üí Neighbor Cluster (Topo) - {args.model_type}_{args.net_type}",
        font_size=10,
        width=1200,
        height=800
    )

    # Save
    output_dir = "results/gene_prediction/topo_dynamic_sankey_clusterlevel/"
    os.makedirs(output_dir, exist_ok=True)
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_confirmed_dynamic_sankey_clusterlevel_epo{args.num_epochs}.html"
    )
    sankey_fig.write_html(plot_path)
    print(f"‚úÖ Cluster-level Sankey diagram saved to {plot_path}")

def plot_multilevel_sankey_bio_clusterlevel(
    args,
    graph,
    node_names_topk,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names_topk, name_to_index, confirmed_genes)

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        neighbors = neighbors_dict.get(gene, [])
        
        if not neighbors:
            print(f"‚ö†Ô∏è No neighbors found for {gene}.")
            continue

        neighbor_scores = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if not neighbor_scores:
            print(f"‚ö†Ô∏è No valid neighbor relevance for {gene}.")
            continue

        # Limit to top 10 neighbors
        neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:10])

        # Now build node list and mapping
        all_labels = []
        all_colors = []
        label_to_idx = {}

        # 1. Confirmed gene node
        confirmed_label = f"{gene}"
        all_labels.append(confirmed_label)
        all_colors.append("gray")
        label_to_idx[confirmed_label] = 0

        # 2. Neighbor nodes
        for neighbor_idx in neighbor_scores.keys():
            neighbor_name = node_id_to_name[neighbor_idx]
            neighbor_label = f"{neighbor_name}"
            if neighbor_label not in label_to_idx:
                label_to_idx[neighbor_label] = len(all_labels)
                all_labels.append(neighbor_label)
                all_colors.append("lightgray")

        # 3. Cluster nodes
        neighbor_clusters = set()
        for neighbor_idx in neighbor_scores.keys():
            neighbor_cluster = row_labels[neighbor_idx]
            neighbor_clusters.add(neighbor_cluster)

        for cluster_id in sorted(neighbor_clusters):
            cluster_label = f"Cluster {cluster_id}"
            if cluster_label not in label_to_idx:
                label_to_idx[cluster_label] = len(all_labels)
                all_labels.append(cluster_label)
                all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))

        # Build Sankey source-target-value-linkcolor
        source = []
        target = []
        value = []
        link_colors = []

        # Gene -> Neighbor
        for neighbor_idx, score in neighbor_scores.items():
            neighbor_name = node_id_to_name[neighbor_idx]
            source.append(label_to_idx[confirmed_label])
            target.append(label_to_idx[neighbor_name])
            value.append(score)
            link_colors.append("rgba(128,128,128,0.4)")  # gray links

        # Neighbor -> Cluster
        for neighbor_idx, score in neighbor_scores.items():
            neighbor_name = node_id_to_name[neighbor_idx]
            neighbor_cluster = row_labels[neighbor_idx]
            cluster_label = f"nCluster {neighbor_cluster}"

            source.append(label_to_idx[neighbor_name])
            target.append(label_to_idx[cluster_label])
            value.append(score)
            # Link colored by cluster
            cluster_color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
            link_colors.append(cluster_color.replace("#", "rgba(") + ",0.6)")  # lighter

        # Build figure
        fig = go.Figure(data=[go.Sankey(
            arrangement="snap",
            node=dict(
                pad=15,
                thickness=20,
                line=dict(color="black", width=0.5),
                label=all_labels,
                color=all_colors
            ),
            link=dict(
                source=source,
                target=target,
                value=value,
                color=link_colors
            ))])

        fig.update_layout(
            title_text=f"Multi-Level Sankey: {gene} ‚Üí Neighbors ‚Üí Clusters (Bio)",
            font_size=10,
            width=1000,
            height=700
        )

        # Save
        output_dir = "results/gene_prediction/bio_multilevel_sankey/"
        os.makedirs(output_dir, exist_ok=True)

        save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_{gene}_bio_multilevel_sankey_epo{args.num_epochs}.html"
        )
        fig.write_html(save_path)
        print(f"‚úÖ Multi-level Sankey saved: {save_path}")

def compute_combined_relevance_scores(
    model,
    graph,
    features,
    node_indices=None,
    use_abs=True,
    normalize=False,
    prob_threshold=0.5
):
    """
    Computes gradient-based relevance (saliency) scores for selected nodes.

    Args:
        model: Trained GNN model
        graph: DGLGraph
        features: Node features (Tensor or numpy array)
        node_indices: List/Tensor of node indices to compute relevance for. If None, select using prob_threshold
        use_abs: Whether to use absolute value of gradients
        normalize: Whether to normalize relevance scores per node (optional)
        prob_threshold: Probability threshold for auto-selecting nodes

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features]
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > prob_threshold, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            probs[idx].backward(retain_graph=(i != len(node_indices) - 1))

            grads = features.grad[idx]
            relevance = grads.abs() if use_abs else grads

            if normalize:
                norm = relevance.norm(p=1)  # L1 norm
                if norm > 0:
                    relevance = relevance / norm

            relevance_scores[idx] = relevance.detach()

    return relevance_scores

def saliency_to_color(saliency, min_saliency=0.0, max_saliency=1.0):
    saliency = np.clip((saliency - min_saliency) / (max_saliency - min_saliency), 0, 1)

    # Interpolate from blue (low) to red (high)
    r = int(0 + saliency * (255 - 0))    # Red from 0 to 255
    g = int(0)                           # Green stays 0
    b = int(255 - saliency * (255 - 0))  # Blue from 255 to 0

    return f"rgb({r},{g},{b})"

def hex_to_rgba(hex_color, alpha):
    hex_color = hex_color.lstrip('#')
    r, g, b = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
    return f'rgba({r},{g},{b},{alpha})'

def saliency_to_grayscale(saliency, min_saliency=0.0, max_saliency=1.0):
    saliency = np.clip((saliency - min_saliency) / (max_saliency - min_saliency), 0, 1)
    gray_value = int(238 - saliency * (238 - 17))  # 238: #eeeeee (light gray), 17: #111111 (dark gray)
    return f"rgb({gray_value},{gray_value},{gray_value})"

def plot_top_confirmed_gene_neighbors_chord_(
    graph,
    node_names,
    name_to_index,
    scores,
    confirmed_genes,
    output_html="top10_confirmed_gene_neighbors_chord.html",
    output_png="top10_confirmed_gene_neighbors_chord.png",
    top_k_genes=10,
    top_k_neighbors=10,
    min_edge_score=0.05
):
    """
    Plots a chord diagram of top confirmed genes and their neighbors.
    Saves both interactive HTML and static PNG versions.

    Parameters:
        graph: DGLGraph
        node_names: list mapping node indices to gene names
        name_to_index: dict mapping gene names to indices
        scores: numpy array of predicted scores
        confirmed_genes: list of confirmed gene names
        output_html: HTML path to save the interactive chord diagram
        output_png: PNG path to save the static image
        top_k_genes: number of confirmed genes to include
        top_k_neighbors: number of top neighbors per gene
        min_edge_score: minimum score threshold for edges
    """

    # Step 1: Filter top confirmed genes
    confirmed_gene_scores = [
        (gene, scores[name_to_index[gene]])
        for gene in confirmed_genes if gene in name_to_index
    ]
    top_confirmed_genes = sorted(confirmed_gene_scores, key=lambda x: x[1], reverse=True)[:top_k_genes]

    # Step 2: Build edges to top neighbors
    chord_links = []
    seen_edges = set()

    for gene, _ in top_confirmed_genes:
        idx = name_to_index[gene]
        neighbors = graph.successors(idx).tolist()
        neighbor_scores = [
            (node_names[n], scores[n]) for n in neighbors if node_names[n] != gene
        ]
        top_neighbors = sorted(neighbor_scores, key=lambda x: x[1], reverse=True)[:top_k_neighbors]

        for neighbor, score in top_neighbors:
            if score >= min_edge_score:
                edge_key = tuple(sorted((gene, neighbor)))
                if edge_key not in seen_edges:
                    chord_links.append((gene, neighbor, score))
                    seen_edges.add(edge_key)

    if not chord_links:
        print("[Chord Diagram] No valid edges found. Try lowering `min_edge_score`.")
        return

    # Step 3: Create Chord diagram
    chord = hv.Chord(chord_links).select(value=(min_edge_score, None))
    chord.opts(
        opts.Chord(
            cmap='Category20',
            edge_color='source',
            node_color='index',
            labels='name',
            edge_alpha=0.7,
            edge_line_width=hv.dim('value') * 5,
            width=900,
            height=900,
            title="Top Confirmed Genes and Their Neighbors"
        )
    )

    # Step 4: Save HTML
    hv.save(chord, output_html)
    print(f"[‚úî] HTML saved to: {output_html}")

    # Step 5: Save PNG using Bokeh backend
    try:
        from bokeh.io.export import export_png
        from bokeh.io import curdoc
        from holoviews.plotting.bokeh import render

        plot = render(chord)
        export_png(plot, filename=output_png)
        print(f"[‚úî] PNG saved to: {output_png}")
    except Exception as e:
        print(f"[‚ö†] PNG export failed: {e}")
        print("To enable PNG export, make sure you have installed: selenium, pillow, and a compatible web driver like chromedriver.")

def plot_top_confirmed_gene_neighbors_chord_not_gene_name(
    graph,
    node_names,
    name_to_index,
    scores,
    confirmed_genes,
    output_path="top10_confirmed_gene_neighbors_chord.html",
    top_k_genes=10,
    top_k_neighbors=10,
    min_edge_score=0.05  # filter out weak edges for clarity
):
    """
    Plots a chord diagram of top K confirmed genes and their top K neighbors by predicted cancer score.

    Parameters:
        graph: DGL graph
        node_names: list of all node names
        name_to_index: dict mapping names to indices
        scores: numpy array of cancer scores
        confirmed_genes: list of confirmed gene names
        output_path: where to save the HTML file
        top_k_genes: how many confirmed genes to show
        top_k_neighbors: how many neighbors per gene
        min_edge_score: minimum score threshold for showing edges
    """

    # Step 1: Rank confirmed genes by model score
    confirmed_gene_scores = [
        (gene, scores[name_to_index[gene]])
        for gene in confirmed_genes if gene in name_to_index
    ]
    top_confirmed_genes = sorted(confirmed_gene_scores, key=lambda x: x[1], reverse=True)[:top_k_genes]

    # Step 2: For each gene, get top neighbors by score
    chord_links = []
    seen_edges = set()

    for gene, _ in top_confirmed_genes:
        idx = name_to_index[gene]
        neighbors = graph.successors(idx).tolist()
        neighbor_scores = [
            (node_names[n], scores[n]) for n in neighbors if node_names[n] != gene
        ]
        top_neighbors = sorted(neighbor_scores, key=lambda x: x[1], reverse=True)[:top_k_neighbors]

        for neighbor, score in top_neighbors:
            if score >= min_edge_score:
                edge_key = tuple(sorted((gene, neighbor)))
                if edge_key not in seen_edges:
                    chord_links.append((gene, neighbor, score))
                    seen_edges.add(edge_key)

    if not chord_links:
        print("[Chord Diagram] No valid edges found. Try lowering `min_edge_score`.")
        return

    # Step 3: Create Chord Diagram
    chord = hv.Chord(chord_links).select(value=(min_edge_score, None))
    chord.opts(
        opts.Chord(
            cmap='Category20',
            edge_color='source',
            node_color='index',
            labels='name',
            edge_alpha=0.7,
            edge_line_width=hv.dim('value') * 5,
            width=900,
            height=900,
            title="Top Confirmed Genes and Neighbors"
        )
    )

    # Save
    hv.save(chord, output_path)
    print(f"[‚úî] Chord diagram saved to: {output_path}")

def plot_chord_diagram_topo(
    args,
    source_labels,
    target_labels,
    matrix,
    CLUSTER_COLORS
):
    """
    A stylized approximation of a chord diagram using Plotly's Sankey layout.
    """

    all_labels = list(set(source_labels) | set(target_labels))
    label_to_index = {label: idx for idx, label in enumerate(all_labels)}
    
    source = []
    target = []
    value = []
    link_colors = []

    for i, src in enumerate(source_labels):
        for j, tgt in enumerate(target_labels):
            if matrix[i][j] > 0:
                source_idx = label_to_index[src]
                target_idx = label_to_index[tgt]
                source.append(source_idx)
                target.append(target_idx)
                value.append(matrix[i][j])

                # Get color from source if available
                color = CLUSTER_COLORS.get(i, "#999999")
                link_colors.append(hex_to_rgba(color, 0.5))

    node_colors = [CLUSTER_COLORS.get(i, "#888888") for i in range(len(all_labels))]

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=20,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=node_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        title_text="Chord Diagram (Approx) - Topological Gene Interactions",
        font_size=12,
        margin=dict(l=200, r=200, t=100, b=100),
        width=1200,
        height=1000,
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )

    # Save output
    output_dir = "results/gene_prediction/topo_chord_diagram/"
    os.makedirs(output_dir, exist_ok=True)

    html_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_chord_epo{args.num_epochs}.html"
    )
    fig.write_html(html_path)
    print(f"‚úÖ Chord diagram saved as HTML: {html_path}")

    # Save PNG
    try:
        import plotly.io as pio
        png_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_topo_chord_epo{args.num_epochs}.png"
        )
        fig.write_image(png_path, format="png", scale=2, width=1200, height=1000)
        print(f"üñºÔ∏è PNG also saved: {png_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' to enable image export:\n  pip install kaleido")

def plot_top_confirmed_gene_neighbors_chord(
    graph,
    node_names,
    name_to_index,
    scores,
    confirmed_genes,
    row_labels=None,
    cluster_colors=None,
    output_html="results/chord/top10_confirmed_gene_neighbors_chord.html",
    output_png="results/chord/top10_confirmed_gene_neighbors_chord.png",
    top_k_genes=10,
    top_k_neighbors=10,
    min_edge_score=0.05
):
    """
    Plots a chord diagram of top confirmed genes and their neighbors.
    Saves both interactive HTML and static PNG versions.

    Parameters:
        graph: DGLGraph
        node_names: list mapping node indices to gene names
        name_to_index: dict mapping gene names to indices
        scores: numpy array of predicted scores
        confirmed_genes: list of confirmed gene names
        row_labels: list or array of cluster labels for each node (optional)
        cluster_colors: dict mapping cluster label to hex color (optional)
        output_html: path to save interactive HTML
        output_png: path to save static PNG
        top_k_genes: number of confirmed genes to include
        top_k_neighbors: number of top neighbors per gene
        min_edge_score: minimum score threshold for edges
    """
    # Ensure output dirs exist
    os.makedirs(os.path.dirname(output_html), exist_ok=True)
    os.makedirs(os.path.dirname(output_png), exist_ok=True)

    # Step 1: Filter top confirmed genes
    confirmed_gene_scores = [
        (gene, scores[name_to_index[gene]])
        for gene in confirmed_genes if gene in name_to_index
    ]
    top_confirmed_genes = sorted(confirmed_gene_scores, key=lambda x: x[1], reverse=True)[:top_k_genes]

    # Step 2: Build edges to top neighbors
    chord_links = []
    seen_edges = set()

    for gene, _ in top_confirmed_genes:
        idx = name_to_index[gene]
        neighbors = graph.successors(idx).tolist()
        neighbor_scores = [
            (node_names[n], scores[n]) for n in neighbors if node_names[n] != gene
        ]
        top_neighbors = sorted(neighbor_scores, key=lambda x: x[1], reverse=True)[:top_k_neighbors]

        for neighbor, score in top_neighbors:
            if score >= min_edge_score:
                edge_key = tuple(sorted((gene, neighbor)))
                if edge_key not in seen_edges:
                    chord_links.append((gene, neighbor, score))
                    seen_edges.add(edge_key)

    if not chord_links:
        print("[Chord Diagram] No valid edges found. Try lowering `min_edge_score`.")
        return

    # Step 3: Create node color mapping if cluster info is provided
    node_set = set([g for edge in chord_links for g in edge[:2]])
    df_nodes = pd.DataFrame({'name': list(node_set)})

    if row_labels is not None and cluster_colors is not None:
        #df_nodes['cluster'] = df_nodes['name'].map(lambda g: row_labels[name_to_index[g]])
        df_nodes['cluster'] = df_nodes['name'].map(
            lambda g: row_labels[name_to_index[g]] if g in name_to_index else -1
        )

        df_nodes['color'] = df_nodes['cluster'].map(lambda c: cluster_colors.get(c, "#cccccc"))
    else:
        df_nodes['color'] = "#cccccc"

    name_to_color = dict(zip(df_nodes['name'], df_nodes['color']))

    # Step 4: Create Chord diagram
    chord = hv.Chord(chord_links).select(value=(min_edge_score, None))
    chord.opts(
        opts.Chord(
            cmap='Category20',
            edge_color='source',
            node_color=hv.dim('name').categorize(name_to_color, default="#cccccc"),
            labels='name',
            edge_alpha=0.7,
            edge_line_width=hv.dim('value') * 5,
            width=900,
            height=900,
            title="Top Confirmed Genes and Their Neighbors"
        )
    )

    # Step 5: Save HTML
    hv.save(chord, output_html)
    print(f"[‚úî] HTML saved to: {output_html}")

    # Step 6: Save PNG using Bokeh backend
    try:
        from bokeh.io.export import export_png
        from holoviews.plotting.bokeh import render
        plot = render(chord)
        export_png(plot, filename=output_png)
        print(f"[‚úî] PNG saved to: {output_png}")
    except Exception as e:
        print(f"[‚ö†] PNG export failed: {e}")
        print("To enable PNG export, install dependencies: `pip install selenium pillow` and configure a headless browser like `chromedriver`.")

def plot_enriched_pathways_heatmap(
    enrichment_results,
    output_dir,
    model_type,
    net_type,
    num_epochs,
    max_term_len=40,
    max_topo_term_len=60,
    max_rows=50,
    top_n_terms_per_cluster=None,
    return_data=False
):

    heatmap_data = pd.DataFrame()

    for cluster_type in ['bio', 'topo']:
        for cid, df in enrichment_results[cluster_type].items():
            if top_n_terms_per_cluster:
                df = df[df['p_value'] < 0.05].sort_values(by='p_value').head(top_n_terms_per_cluster)
            colname = f"{cluster_type.capitalize()}_{cid}"
            vals = {}
            for _, row in df.iterrows():
                p = row['p_value']
                name = row['name']
                if p < 0.05 and len(name) <= max_term_len:
                    term = f"{name} ({row['source']})"
                    vals[term] = -np.log10(p)
            heatmap_data[colname] = pd.Series(vals)

    heatmap_data = heatmap_data.fillna(0)
    heatmap_data = heatmap_data[heatmap_data.max(axis=1) > 1]

    enrichment_csv_path = os.path.join(
        output_dir,
        f"{model_type}_{net_type}_enrichment_matrix_epo{num_epochs}.csv"
    )
    heatmap_data.to_csv(enrichment_csv_path, index_label='Enriched Pathway')

    # Topo terms export
    topo_terms = []
    for cid, df in enrichment_results['topo'].items():
        if top_n_terms_per_cluster:
            df = df[df['p_value'] < 0.05].sort_values(by='p_value').head(top_n_terms_per_cluster)
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= max_topo_term_len:
                topo_terms.append({
                    "Cluster": f"Topo_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })

    topo_terms_df = pd.DataFrame(topo_terms)
    topo_terms_path = os.path.join(
        output_dir,
        f"{model_type}_{net_type}_topo_cluster_top_terms_epo{num_epochs}.csv"
    )
    topo_terms_df.to_csv(topo_terms_path, index=False)

    if heatmap_data.shape[0] > max_rows:
        step = max(1, heatmap_data.shape[0] // max_rows)
        selected_indices = heatmap_data.index[::step][:max_rows]
        heatmap_data = heatmap_data.loc[selected_indices]

    norm_data = heatmap_data / heatmap_data.max().replace(0, 1)

    colormaps = {
        'bio': get_cmap('Blues'),
        'topo': get_cmap('YlOrRd'),
    }

    colors = np.zeros((heatmap_data.shape[0], heatmap_data.shape[1], 4))
    col_types = []

    for i, col in enumerate(norm_data.columns):
        group = 'bio' if col.lower().startswith("bio") else 'topo'
        col_types.append(group)
        cmap = colormaps[group]
        colors[:, i, :] = cmap(norm_data[col].values)

    fig, ax = plt.subplots(figsize=(0.5 * len(norm_data.columns), 0.2 * len(norm_data)))

    ax.imshow(colors, aspect='auto')

    ax.set_xticks(np.arange(len(norm_data.columns)))
    ax.set_xticklabels(norm_data.columns, rotation=90, fontsize=16)
    ax.set_yticks(np.arange(len(norm_data.index)))
    ax.set_yticklabels(norm_data.index, fontsize=16)

    ax.set_ylabel("Enriched Pathway", fontsize=16, labelpad=20)

    for xtick, col in zip(ax.get_xticklabels(), col_types):
        xtick.set_color('darkblue' if col == 'bio' else 'darkred')

    ax.set_title("Top Enriched Pathways per Cluster (p < 0.05)", fontsize=15, pad=16)
    ax.set_xlabel("Cluster", fontsize=16)

    legend_patches = [
        Patch(color='cornflowerblue', label='Bio'),
        Patch(color='salmon', label='Topo')
    ]
    fig.legend(handles=legend_patches, loc='lower center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))

    sns.despine(ax=ax, trim=True)
    ax.tick_params(axis='both', which='both', length=0)
    plt.tight_layout(rect=[0, 0, 0.95, 0.93])

    enriched_terms_heatmap_path = os.path.join(
        output_dir,
        f"{model_type}_{net_type}_enriched_terms_heatmap_epo{num_epochs}.png"
    )
    plt.savefig(enriched_terms_heatmap_path, dpi=300)
    plt.close()

    if return_data:
        return heatmap_data, topo_terms_df

def plot_enriched_term_counts(enrichment_results, output_path, model_type, net_type, num_epochs):
    """
    Plot bar chart of the number of enriched terms per cluster for bio and topo clusters,
    with x-axis labels colored according to their type.
    """
    fig, ax = plt.subplots(figsize=(10, 6))

    bars = []
    xtick_colors = []
    xtick_labels = []

    colormaps = {
        'bio': get_cmap('Blues')(0.6),     # medium blue
        'topo': get_cmap('YlOrRd')(0.6),   # medium orange-red
    }
        
    for cluster_type in ['bio', 'topo']:
        cluster_ids = list(enrichment_results[cluster_type].keys())
        term_counts = [len(res) for res in enrichment_results[cluster_type].values()]
        labels = [f"{cluster_type.capitalize()}_{i}" for i in cluster_ids]

        color = colormaps[cluster_type]
        bar_container = ax.bar(labels, term_counts, label=cluster_type, color=color)
        bars.extend(bar_container)

        xtick_labels.extend(labels)
        xtick_colors.extend([color] * len(labels))

    ax.set_xlim(-0.65, len(bars) - 0.5)
    ax.set_ylabel("Number of enriched terms", fontsize=28)

    ax.set_xticks(range(len(xtick_labels)))
    ax.set_xticklabels(xtick_labels, rotation=90, fontsize=20)
    for tick_label, color in zip(ax.get_xticklabels(), xtick_colors):
        tick_label.set_color(color)

    ax.tick_params(axis='y', labelsize=20)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    plt.tight_layout()
    filename = f"{model_type}_{net_type}_term_counts_barplot_epo{num_epochs}.png"
    plt.savefig(os.path.join(output_path, filename), dpi=300)
    plt.close()

    print(f"Bar plot saved to {os.path.join(output_path, filename)}")

def plot_shared_enriched_pathways_venn(enrichment_results, output_path, model_type, net_type, num_epochs):
    """
    Plots a Venn diagram showing the overlap of enriched pathways between bio and topo clusters.
    """
    bio_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['bio'].values() if not df.empty], [])
    )
    topo_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['topo'].values() if not df.empty], [])
    )

    colormaps = {
        'bio': get_cmap('Blues')(0.6),     # medium blue
        'topo': get_cmap('YlOrRd')(0.6),   # medium orange-red
    }
    
    plt.figure(figsize=(8, 8))
    venn = venn2(
        [bio_terms, topo_terms],
        set_labels=('Bio', 'Topo'),
        set_colors=(colormaps['bio'], colormaps['topo']),
        alpha=0.7
    )

    for text in venn.set_labels:
        if text:
            text.set_fontsize(28)
    for text in venn.subset_labels:
        if text:
            text.set_fontsize(28)

    plt.title("Overlap of enriched pathways", fontsize=16)

    filename = f"{model_type}_{net_type}_shared_pathways_venn_epo{num_epochs}.png"
    venn_path = os.path.join(output_path, filename)
    plt.savefig(venn_path, dpi=300)
    plt.close()

    print(f"Venn diagram saved to {venn_path}")

def save_and_plot_enriched_pathways_(enrichment_results, args, output_dir):
    # === Prepare Data ===
    heatmap_data = pd.DataFrame()

    for cluster_type in ['bio', 'topo']:
        for cid, df in enrichment_results[cluster_type].items():
            colname = f"{cluster_type.capitalize()}_{cid}"
            vals = {}
            for _, row in df.iterrows():
                p = row['p_value']
                name = row['name']
                if p < 0.05 and len(name) <= 60:
                    term = f"{name} ({row['source']})"
                    vals[term] = -np.log10(p)
            heatmap_data[colname] = pd.Series(vals)

    # Clean and filter
    heatmap_data = heatmap_data.fillna(0)
    heatmap_data = heatmap_data[heatmap_data.max(axis=1) > 1]

    # Save full enrichment data to CSV
    enrichment_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enrichment_matrix_epo{args.num_epochs}.csv"
    )
    heatmap_data.to_csv(enrichment_csv_path, index_label='Enriched Pathway')

    # === Save Topo Cluster ‚Üí Top Enriched Terms to CSV ===
    topo_terms = []
    for cid, df in enrichment_results['topo'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                topo_terms.append({
                    "Cluster": f"Topo_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    topo_terms_df = pd.DataFrame(topo_terms)
    topo_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    topo_terms_df.to_csv(topo_terms_path, index=False)

    # === Save Bio Cluster ‚Üí Top Enriched Terms to CSV ===
    bio_terms = []
    for cid, df in enrichment_results['bio'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                bio_terms.append({
                    "Cluster": f"Bio_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    bio_terms_df = pd.DataFrame(bio_terms)
    bio_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    bio_terms_df.to_csv(bio_terms_path, index=False)

    # === Select 50 evenly spaced rows for plotting ===
    if heatmap_data.shape[0] > 50:
        step = max(1, heatmap_data.shape[0] // 50)
        selected_indices = heatmap_data.index[::step][:50]
        heatmap_data = heatmap_data.loc[selected_indices]

    # === Normalize for color contrast ===
    norm_data = heatmap_data.copy()
    norm_data = norm_data / norm_data.max().replace(0, 1)

    # === Apply group-wise colormaps ===
    colormaps = {
        'bio': get_cmap('Blues'),
        'topo': get_cmap('YlOrRd'),
    }

    colors = np.zeros((heatmap_data.shape[0], heatmap_data.shape[1], 4))  # RGBA
    col_types = []

    for i, col in enumerate(norm_data.columns):
        group = 'bio' if col.lower().startswith("bio") else 'topo'
        col_types.append(group)
        cmap = colormaps[group]
        colors[:, i, :] = cmap(norm_data[col].values)

    # === Plot ===
    fig, ax = plt.subplots(figsize=(0.5 * len(norm_data.columns), 0.2 * len(norm_data)))

    ax.imshow(colors, aspect='auto')
    ax.set_xticks(np.arange(len(norm_data.columns)))
    ax.set_xticklabels(norm_data.columns, rotation=90, fontsize=13)
    ax.set_yticks(np.arange(len(norm_data.index)))
    ax.set_yticklabels(norm_data.index, fontsize=16)
    ax.set_ylabel("Enriched Pathway", fontsize=18, labelpad=20)

    # Color x-axis labels
    for xtick, col in zip(ax.get_xticklabels(), col_types):
        xtick.set_color('darkblue' if col == 'bio' else 'darkred')

    ax.set_title("Top Enriched Pathways per Cluster (p < 0.05)", fontsize=16, pad=16)
    ax.set_xlabel("Cluster", fontsize=16)

    legend_patches = [
        Patch(color='cornflowerblue', label='Bio'),
        Patch(color='salmon', label='Topo')
    ]
    fig.legend(handles=legend_patches, loc='lower center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))

    sns.despine(ax=ax, trim=True)
    ax.tick_params(axis='both', which='both', length=0)
    plt.tight_layout(rect=[0, 0, 0.95, 0.93])

    # Save plot
    enriched_terms_heatmap_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enriched_terms_heatmap_epo{args.num_epochs}.png"
    )
    plt.savefig(enriched_terms_heatmap_path, dpi=300)
    plt.close()

    # === Return DataFrames for downstream analysis ===
    return heatmap_data, topo_terms_df, bio_terms_df

def plot_collapsed_clusterfirst_multilevel_sankey_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    # for g in selected_novel_genes + selected_known_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 20:
            break

    #combined_genes = [g for g in combined_genes if g != "IRF2"]

    confirmed_genes = combined_genes

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def compute_relevance_scores_(model, graph, features, node_indices=None, use_abs=True):
    """
    Computes gradient-based relevance scores (saliency) for selected nodes using sigmoid probabilities.

    Args:
        model: Trained GNN model
        graph: DGL graph
        features: Input node features
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select using probs > 0.0
        use_abs: Whether to use absolute gradients (recommended for visualization)

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] (0s for nodes not analyzed)
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        # Auto-select nodes (e.g., predicted cancer genes)
        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            probs[idx].backward(retain_graph=(i != len(node_indices) - 1))

            grads = features.grad[idx]
            relevance_scores[idx] = grads.abs().detach() if use_abs else grads.detach()

    return relevance_scores

def extract_summary_features_np_bio_(bio_embeddings_np):
    """
    Extracts summary features from just the 1024 biological features (bio only).

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = bio_embeddings_np.shape
    summary_features = []

    assert num_features == 1024, f"Expected 1024 bio features, got {num_features}"

    for o_idx in range(4):  # 4 omics types
        for c_idx in range(16):  # 16 cancer types
            base = o_idx * 16 * 16 + c_idx * 16
            group = bio_embeddings_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)

def extract_summary_features_np_bio(bio_embeddings_np, omics_types, cancer_names, save_path=None):
    import pandas as pd

    num_nodes, num_features = bio_embeddings_np.shape
    summary_features = []

    assert num_features == 1024, f"Expected 1024 bio features, got {num_features}"


    column_names = []

    for o_idx, omics in enumerate(omics_types):
        seen = set()
        for c_idx, cancer in enumerate(cancer_names):
            label = f"{omics}_{cancer}"
            if label in seen:
                raise ValueError(f"Duplicate label: {label}")
            seen.add(label)

            base = o_idx * 16 * 16 + c_idx * 16
            group = bio_embeddings_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)
            column_names.append(label)

    summary_array = np.concatenate(summary_features, axis=1)  # shape [num_nodes, 64]

    # Optional CSV save
    if save_path:
        df = pd.DataFrame(summary_array, columns=column_names)
        df.to_csv(save_path, index=False)
        print(f"‚úÖ Summary bio features saved to: {save_path}")

    return summary_array, column_names

def extract_summary_features_np_topo(topo_features_np):
    """
    Extracts summary features from the topological embedding section (features 1024‚Äì2047)
    by computing the max over each 16-dimensional segment.

    Args:
        features_np (np.ndarray): shape [num_nodes, 2048]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = topo_features_np.shape
    assert num_features == 1024, f"Expected 1024 features, got {num_features}"

    # Select topological features only
    ##topo_features = features_np[:, 1024:]  # shape: [num_nodes, 1024]
    ##topo_features = topo_features_np[:, 1024:2048]
    topo_features = topo_features_np  # already 1024 features


    summary_features = []

    # Pool over 64 chunks of 16 features
    for i in range(64):
        start = i * 16
        end = start + 16
        group = topo_features[:, start:end]  # shape: [num_nodes, 16]
        max_vals = group.max(axis=1, keepdims=True)  # shape: [num_nodes, 1]
        summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # shape: [num_nodes, 64]

def extract_specific_omics_cancer(bio_embeddings_np, omics_target='mf', cancer_target='BRCA'):
    """
    Extracts 16 features for a specific omics-cancer pair from the 1024 bio features.

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]
        omics_target (str): one of ['cna', 'ge', 'meth', 'mf']
        cancer_target (str): one of 16 cancer types like 'BRCA'

    Returns:
        np.ndarray: shape [num_nodes, 16]
    """
    omics_types = ['cna', 'ge', 'meth', 'mf']
    # cancer_names = ['BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP', 'LIHC', 'LUAD',
    #                 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC']
    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    o_idx = omics_types.index(omics_target)
    c_idx = cancer_names.index(cancer_target)

    start = o_idx * 16 * 16 + c_idx * 16
    end = start + 16

    return bio_embeddings_np[:, start:end]

def plot_top_gene_ridge_from_specific_omics_(
    bio_embeddings_np,
    node_names,
    row_labels,
    output_path,
    omics_target='mf',
    cancer_target='BRCA',
    top_n=12
):
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    import os

    # 1. Extract 16-dim features for the selected omics-cancer pair
    features_16 = extract_specific_omics_cancer(bio_embeddings_np, omics_target, cancer_target)

    # 2. Ensure output directory exists
    os.makedirs(output_path, exist_ok=True)

    # 3. Build DataFrame
    df = pd.DataFrame(features_16)
    df["Gene"] = node_names
    df["Cluster"] = row_labels
    df["MeanFeatureValue"] = df.iloc[:, :-2].mean(axis=1)

    clusters = sorted(df["Cluster"].unique())

    for cluster in clusters:
        cluster_df = df[df["Cluster"] == cluster].copy()
        top_genes = cluster_df.nlargest(top_n, "MeanFeatureValue")

        # Prepare melted long-form data for seaborn
        plot_data = []
        for _, row in top_genes.iterrows():
            gene = row["Gene"]
            for i in range(16):
                plot_data.append({
                    "Gene": gene,
                    "FeatureIndex": i,
                    "Value": row[i]
                })
        plot_df = pd.DataFrame(plot_data)

        # Sort gene order
        gene_order = plot_df.groupby("Gene")["Value"].mean().sort_values().index
        plot_df["Gene"] = pd.Categorical(plot_df["Gene"], categories=gene_order, ordered=True)

        # Compute proper x-axis limits with padding
        xmin = plot_df["Value"].min()
        xmax = plot_df["Value"].max()
        x_range = xmax - xmin
        xmin -= x_range * 0.5
        xmax += x_range * 0.5


        # Plot with ridge style
        sns.set(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})
        g = sns.FacetGrid(
            plot_df,
            row="Gene",
            hue="Gene",
            aspect=12,
            height=0.4,
            palette="Spectral",
            sharex=True
        )

        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            fill=True,
            alpha=0.8,
            cut=100,
            clip=(xmin, xmax)
        )
        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            color="black",
            lw=1,
            cut=100,
            clip=(xmin, xmax)
        )


        g.set_titles("")
        g.set(xlim=(xmin, xmax), xlabel="Feature Value", ylabel="", yticks=[])

        # Label genes on the left side
        for ax, gene in zip(g.axes.flat, gene_order):
            ax.set_ylabel(gene, rotation=0, ha='right', va='top', fontsize=18, labelpad=10)

        g.despine(bottom=True, left=True)
        g.fig.subplots_adjust(hspace=-0.3, left=0.3, right=0.95, top=0.93)
        g.fig.suptitle(f"Cluster {cluster}", x=0.6, fontsize=20)

        # After g.set(...) and before plt.savefig(...)
        for ax in g.axes.flat:
            ax.tick_params(axis='x', labelsize=16)  # or 14, 16, etc.

        # Save
        out_path = os.path.join(
            output_path,
            f"{omics_target}_{cancer_target}_cluster{cluster}_top{top_n}_genes_ridge.png"
        )
        plt.savefig(out_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"‚úÖ Saved: {out_path}")

def plot_top_gene_ridge_across_omics_(
    bio_embeddings_np,
    node_names,
    row_labels,
    output_path,
    cancer_target='BRCA',
    top_n=12
):
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    import os

    os.makedirs(output_path, exist_ok=True)

    # 1. Extract all 4 omics (64 features) for the cancer
    features_64 = extract_all_omics_for_cancer(bio_embeddings_np, cancer_target)

    # 2. Prepare DataFrame
    df = pd.DataFrame(features_64)
    df["Gene"] = node_names
    df["Cluster"] = row_labels
    df["MeanFeatureValue"] = df.iloc[:, :-2].mean(axis=1)

    clusters = sorted(df["Cluster"].unique())

    omics_types = ['cna', 'ge', 'meth', 'mf']
    omics_labels = []
    for omics in omics_types:
        omics_labels += [omics] * 16

    for cluster in clusters:
        cluster_df = df[df["Cluster"] == cluster].copy()
        top_genes = cluster_df.nlargest(top_n, "MeanFeatureValue")

        # Melt into long-form with omics and feature index
        plot_data = []
        for _, row in top_genes.iterrows():
            gene = row["Gene"]
            for i in range(64):
                plot_data.append({
                    "Gene": gene,
                    "FeatureIndex": i % 16,
                    "Omics": omics_labels[i],
                    "Value": row[i]
                })
        plot_df = pd.DataFrame(plot_data)

        # Sort genes
        gene_order = plot_df.groupby("Gene")["Value"].mean().sort_values().index
        plot_df["Gene"] = pd.Categorical(plot_df["Gene"], categories=gene_order, ordered=True)

        # X-axis limits
        xmin = plot_df["Value"].min()
        xmax = plot_df["Value"].max()
        x_range = xmax - xmin
        xmin -= x_range * 0.5
        xmax += x_range * 0.5

        # Plot
        sns.set(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})
        g = sns.FacetGrid(
            plot_df,
            row="Gene",
            hue="Omics",  # Color by omics
            aspect=12,
            height=0.4,
            palette="tab10",
            sharex=True
        )

        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            fill=True,
            alpha=0.8,
            cut=100,
            clip=(xmin, xmax)
        )
        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            color="black",
            lw=1,
            cut=100,
            clip=(xmin, xmax)
        )

        g.set_titles("")
        g.set(xlim=(xmin, xmax), xlabel="Feature Value", ylabel="", yticks=[])

        for ax, gene in zip(g.axes.flat, gene_order):
            ax.set_ylabel(gene, rotation=0, ha='right', va='top', fontsize=18, labelpad=10)

        g.despine(bottom=True, left=True)
        g.fig.subplots_adjust(hspace=-0.3, left=0.3, right=0.95, top=0.93)
        g.fig.suptitle(f"Cluster {cluster} ‚Äî {cancer_target} (All Omics)", x=0.6, fontsize=20)

        for ax in g.axes.flat:
            ax.tick_params(axis='x', labelsize=16)

        # Save
        out_path = os.path.join(output_path, f"{cancer_target}_allomics_cluster{cluster}_top{top_n}_genes_ridge.png")
        plt.savefig(out_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"‚úÖ Saved: {out_path}")

def run_gprofiler_enrichment(cluster_dict, cancer_type, tag):
    gp = GProfiler(return_dataframe=True)
    output_files = []
    for cluster, genes in cluster_dict.items():
        try:
            result = gp.profile(
                organism="hsapiens",
                query=genes,
                sources=["REAC", "KEGG", "GO:BP", "HP"],
                user_threshold=0.05,
                significance_threshold_method="fdr"
            )
            if not result.empty:
                for source in ["REAC", "KEGG", "GO:BP", "HP"]:
                    filtered = result[result["source"] == source]
                    path = f"results/gene_prediction/enrichment/{cancer_type}_{tag}_Cluster_{cluster}_{source}_enrichment.csv"
                    dir_path = os.path.dirname(path)
                    os.makedirs(dir_path, exist_ok=True)

                    filtered.to_csv(path, index=False)
                    output_files.append(path)
        except Exception as e:
            print(f"Enrichment failed for {cancer_type} {cluster}: {e}")
    return output_files

def plot_dot_enrichment_per_cluster(cancer_type, tag, source="REAC", top_n=10):
    files = glob.glob(f"results/gene_prediction/{cancer_type}_{tag}_Cluster_*_{source}_enrichment.csv")
    if not files:
        print(f"No enrichment files found for {cancer_type.upper()} [{tag}] and source {source}")
        return

    for f in files:
        try:
            cluster = Path(f).stem.split("_")[2]  # Cluster number
        except IndexError:
            print(f"Filename parsing failed: {f}")
            continue

        df = pd.read_csv(f)
        if df.empty or "intersection_size" not in df or "query_size" not in df:
            print(f"Invalid dataframe for {f}")
            continue

        df["gene_ratio"] = df["intersection_size"] / df["query_size"]
        df["-log10(FDR)"] = -np.log10(df["p_value"].clip(lower=1e-300))
        top_df = df.sort_values("p_value").head(top_n)

        plt.figure(figsize=(10, 6))
        sns.scatterplot(
            data=top_df,
            x="gene_ratio",
            y="name",
            hue="-log10(FDR)",
            size="-log10(FDR)",
            sizes=(40, 200),
            palette="viridis",
            legend="brief"
        )
        plt.title(f"{source} Enrichment for {cancer_type.upper()} [{tag}] - Cluster {cluster}")
        plt.xlabel("Gene Ratio")
        plt.ylabel("Pathway")
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(f"results/gene_prediction/{cancer_type}_{tag}_Cluster_{cluster}_{source}_dotplot.png", dpi=300)
        plt.close()

def plot_dot_enrichment_from_results(enrichment_results, top_n=10, source_filter=["REAC", "KEGG", "GO:BP", "HP"]):
    """
    Plot dot plots for enrichment results stored in memory.

    Args:
        enrichment_results (dict): Nested dictionary like {'bio': {0: df, 1: df, ...}, 'topo': {0: df, ...}}.
        top_n (int): Number of top enriched terms to display.
        source_filter (list): List of enrichment sources to include.
    """
    for cluster_type, cluster_data in enrichment_results.items():
        for cluster_id, df in cluster_data.items():
            if df.empty or "p_value" not in df or "intersection_size" not in df or "query_size" not in df:
                print(f"Skipping {cluster_type} cluster {cluster_id}: invalid or empty data")
                continue

            # Optional: filter by source
            if source_filter:
                df = df[df["source"].isin(source_filter)]

            # Compute additional metrics
            df["gene_ratio"] = df["intersection_size"] / df["query_size"]
            df["-log10(FDR)"] = -np.log10(df["p_value"].clip(lower=1e-300))
            df_plot = df.sort_values("p_value").head(top_n)

            if df_plot.empty:
                print(f"No significant enrichment to plot for {cluster_type} cluster {cluster_id}")
                continue

            # Plot
            plt.figure(figsize=(10, 6))
            sns.scatterplot(
                data=df_plot,
                x="gene_ratio",
                y="name",
                hue="-log10(FDR)",
                size="-log10(FDR)",
                sizes=(40, 200),
                palette="viridis",
                legend="brief"
            )
            plt.title(f"{cluster_type.capitalize()} Cluster {cluster_id} Enrichment")
            plt.xlabel("Gene Ratio")
            plt.ylabel("Pathway")
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()

            # Save plot
            out_path = f"results/enrichment_dotplots/{cluster_type}_cluster_{cluster_id}_dotplot.png"
            Path(out_path).parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(out_path, dpi=300)
            plt.close()
            print(f"Saved: {out_path}")

def plot_dot_enrichment_per_cluster_all(
    cancer_names,
    tag="bio",
    source="REAC",
    top_n=10,
    n_clusters=10,
    base_path="results/gene_prediction/enrichment"
):
    base_path = Path(base_path)

    for cancer_type in cancer_names:
        print(f"\nüîç Processing {cancer_type.upper()} [{tag}] enrichment dotplots...")
        
        for cluster_id in range(n_clusters):
            file_path = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_enrichment.csv"

            if not file_path.exists():
                print(f"  ‚úó Missing: {file_path.name}")
                continue

            try:
                df = pd.read_csv(file_path)
            except Exception as e:
                print(f"  ‚úó Error reading {file_path.name}: {e}")
                continue

            if df.empty or "intersection_size" not in df or "query_size" not in df:
                print(f"  ‚úó Invalid content: {file_path.name}")
                continue

            # Calculate gene ratio and transformed FDR
            df["gene_ratio"] = df["intersection_size"] / df["query_size"]
            df["-log10(FDR)"] = -np.log10(df["p_value"].clip(lower=1e-300))
            top_df = df.sort_values("p_value").head(top_n)

            # Plot
            plt.figure(figsize=(10, 6))
            sns.scatterplot(
                data=top_df,
                x="gene_ratio",
                y="name",
                hue="-log10(FDR)",
                size="-log10(FDR)",
                sizes=(40, 200),
                palette="viridis",
                legend="brief"
            )
            plt.title(f"{source} Enrichment: {cancer_type.upper()} [{tag}] - Cluster {cluster_id}")
            plt.xlabel("Gene Ratio")
            plt.ylabel("Pathway")
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()

            # Save plot
            out_path = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_dotplot.png"
            plt.savefig(out_path, dpi=300)
            plt.close()
            print(f"  ‚úì Saved: {out_path.name}")

def apply_full_spectral_biclustering_cancer(cancer_feature, n_clusters):
    from sklearn.cluster import SpectralBiclustering
    import numpy as np

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")

    assert cancer_feature.shape[1] == 64, f"Expected 64 summary features, got {cancer_feature.shape[1]}"

    # Perform spectral biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(cancer_feature)

    row_labels = bicluster.row_labels_
    print("‚úÖ Spectral Biclustering complete.")

    return row_labels

def plot_all_cancer_ridges_all_omics(
    bio_embeddings_np,
    node_names,
    #row_labels,
    best_k,
    output_base_path,
    top_n=12
):
    cancer_list = [
        'BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP',
        'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC'
    ]
    #                 'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC']
    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]  
    for cancer in cancer_list:
        print(f"\nüìä Plotting ridge for ALL_OMICS ‚Äî {cancer}...")
        output_path = os.path.join(output_base_path, f"ALL_OMICS_{cancer}")
        plot_top_gene_ridge_from_all_omics(
            bio_embeddings_np=bio_embeddings_np,
            node_names=node_names,
            #row_labels=row_labels,
            best_k=best_k,
            output_path=output_path,
            cancer_target=cancer,
            top_n=top_n
        )

def plot_top_gene_ridge_from_all_omics(
    bio_embeddings_np,
    node_names,
    #row_labels,
    best_k,
    output_path,
    cancer_target='BRCA',
    top_n=12
):
    # 1. Extract 64-dim features
    #features_64 = extract_all_omics_for_cancer(bio_embeddings_np, cancer_target)
    cancer_feature = extract_all_omics_for_cancer(bio_embeddings_np, cancer_target)
    row_labels = apply_full_spectral_biclustering_cancer(cancer_feature, n_clusters=best_k)
    
    # 2. Ensure output directory exists
    os.makedirs(output_path, exist_ok=True)

    # 3. Build DataFrame
    df = pd.DataFrame(cancer_feature)
    df["Gene"] = node_names
    df["Cluster"] = row_labels
    df["MeanFeatureValue"] = df.iloc[:, :-2].mean(axis=1)

    clusters = sorted(df["Cluster"].unique())
    cluster_dict = defaultdict(list)  # Save top genes for each cluster

    for cluster in clusters:
        cluster_df = df[df["Cluster"] == cluster].copy()
        top_genes = cluster_df.nlargest(top_n, "MeanFeatureValue")
        cluster_dict[cluster] = top_genes["Gene"].tolist()

        # Prepare long-form data for seaborn
        plot_data = []
        for _, row in top_genes.iterrows():
            gene = row["Gene"]
            for i in range(64):
                plot_data.append({
                    "Gene": gene,
                    "FeatureIndex": i,
                    "Value": row[i]
                })
        plot_df = pd.DataFrame(plot_data)

        gene_order = plot_df.groupby("Gene")["Value"].mean().sort_values().index
        plot_df["Gene"] = pd.Categorical(plot_df["Gene"], categories=gene_order, ordered=True)

        xmin = plot_df["Value"].min()
        xmax = plot_df["Value"].max()
        x_range = xmax - xmin
        xmin -= x_range * 0.5
        xmax += x_range * 0.5

        sns.set(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})
        g = sns.FacetGrid(
            plot_df,
            row="Gene",
            hue="Gene",
            aspect=12,
            height=0.4,
            palette="Spectral",
            sharex=True
        )

        g.map(sns.kdeplot, "Value", bw_adjust=0.5, fill=True, alpha=0.8, cut=100, clip=(xmin, xmax))
        g.map(sns.kdeplot, "Value", bw_adjust=0.5, color="black", lw=1, cut=100, clip=(xmin, xmax))

        g.set_titles("")
        g.set(xlim=(xmin, xmax), xlabel="Feature Value", ylabel="", yticks=[])

        for ax, gene in zip(g.axes.flat, gene_order):
            ax.set_ylabel(gene, rotation=0, ha='right', va='top', fontsize=18, labelpad=10)

        g.despine(bottom=True, left=True)
        g.fig.subplots_adjust(hspace=-0.3, left=0.3, right=0.95, top=0.93)
        g.fig.suptitle(f"Cluster {cluster}", x=0.6, fontsize=20)

        for ax in g.axes.flat:
            ax.tick_params(axis='x', labelsize=16)

        out_path = os.path.join(output_path, f"ALL_OMICS_{cancer_target}_cluster{cluster}_top{top_n}_genes_ridge.png")
        plt.savefig(out_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"‚úÖ Saved: {out_path}")
        # === NEW: Line plot for top genes in this cluster === #
        fig, ax = plt.subplots(figsize=(14, 6))
        for gene in top_genes["Gene"]:
            gene_values = df[df["Gene"] == gene].iloc[:, :64].values.flatten()
            ax.plot(range(64), gene_values, label=gene, linewidth=2, alpha=0.8)

        ax.set_title(f"Feature Profile Curves ‚Äî Cluster {cluster} ({cancer_target})", fontsize=18)
        ax.set_xlabel("Feature Index", fontsize=16)
        ax.set_ylabel("Feature Value", fontsize=16)
        ax.grid(True, linestyle="--", alpha=0.3)
        ax.legend(loc='upper right', fontsize=16)

        curve_path = os.path.join(output_path, f"ALL_OMICS_{cancer_target}_cluster{cluster}_top{top_n}_genes_curves.png")
        plt.tight_layout()
        plt.savefig(curve_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved curve plot: {curve_path}")

    # Save cluster_dict as JSON
    '''cluster_json_path = os.path.join(output_path, f"ALL_OMICS_{cancer_target}_cluster_genes.json")

    def make_json_safe(obj):
        if isinstance(obj, dict):
            return {str(k): make_json_safe(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [make_json_safe(v) for v in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()  # Convert NumPy scalars to native Python types
        else:
            return obj

    cluster_dict_safe = make_json_safe(cluster_dict)

    with open(cluster_json_path, "w") as f:
        json.dump(cluster_dict_safe, f, indent=2)

    print(f"üíæ Saved cluster gene dictionary: {cluster_json_path}")'''

def extract_all_omics_for_cancer(bio_embeddings_np, cancer_target='BRCA'):
    """
    Extracts 64 features (4 omics √ó 16 features) for a specific cancer type from the 1024 bio features.

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]
        cancer_target (str): one of 16 cancer types like 'BRCA'

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    omics_types = ['cna', 'ge', 'meth', 'mf']
    cancer_names = ['BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP',
                    'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC']
    # cancer_names = [
    #     'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
    #     'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    # ]    
    c_idx = cancer_names.index(cancer_target)

    feature_blocks = []
    for o_idx in range(len(omics_types)):
        start = o_idx * 16 * 16 + c_idx * 16
        end = start + 16
        feature_blocks.append(bio_embeddings_np[:, start:end])

    return np.concatenate(feature_blocks, axis=1)  # shape: [num_nodes, 64]

def plot_survival_by_cluster(
    survival_df,  # Must have columns: ['time', 'event', 'cluster']
    output_path="km_cluster_survival.png",
    title="Survival by Cluster"
):
    kmf = KaplanMeierFitter()
    plt.figure(figsize=(6, 6))

    cluster_labels = sorted(survival_df['cluster'].unique())
    color_palette = plt.get_cmap("tab10")

    for i, clus in enumerate(cluster_labels):
        group = survival_df[survival_df['cluster'] == clus]
        kmf.fit(group['time'], group['event'], label=f"Clus{clus+1}")
        kmf.plot_survival_function(ci_show=False, color=color_palette(i), lw=2)

    # Log-rank test
    results = multivariate_logrank_test(
        survival_df['time'],
        survival_df['cluster'],
        survival_df['event']
    )
    pval = results.p_value

    # Annotate p-value
    plt.text(
        x=0.05,
        y=0.2,
        s=f"p < {pval:.4f}" if pval < 0.0001 else f"p = {pval:.4f}",
        fontsize=16,
        transform=plt.gca().transAxes,
        weight='bold'
    )

    plt.title(title, fontsize=16)
    plt.xlabel("Time (Days)", fontsize=16)
    plt.ylabel("Survival probability", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(False)
    plt.legend(title="", loc="best", fontsize=12)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Kaplan‚ÄìMeier survival plot saved to: {output_path}")

def make_cluster_dict(row_labels, node_names):
    cluster_dict = defaultdict(list)
    for idx, label in enumerate(row_labels):
        cluster_dict[label].append(node_names[idx])
    return cluster_dict

def collect_top_enrichments(cancer_type, tag="bio", source="REAC", top_n=10):
    base_path = Path("results/gene_prediction/enrichment")
    base_path.mkdir(parents=True, exist_ok=True)
    terms = []

    for cluster_id in range(10):  # Clusters 0‚Äì9
        file = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_enrichment.csv"
        if not file.exists():
            continue

        df = pd.read_csv(file).sort_values("p_value").head(top_n)
        for _, row in df.iterrows():
            term = f"{row['name']} (C{cluster_id})"
            score = -np.log10(row["p_value"] + 1e-10)
            terms.append((term, score, cluster_id))

    return sorted(terms, key=lambda x: x[1], reverse=True)

def draw_horizontal_bar_plot(terms, cancer_type, source):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    # Sort and select top 20 terms by score
    terms_sorted = sorted(terms, key=lambda x: x[1], reverse=True)[:20]

    labels = [t[0] for t in terms_sorted]
    scores = [t[1] for t in terms_sorted]
    clusters = [t[2] for t in terms_sorted]
    colors = [CLUSTER_COLORS[c] for c in clusters]

    fig, ax = plt.subplots(figsize=(12, 0.5 * len(terms_sorted)))
    y_pos = np.arange(len(terms_sorted))

    ax.barh(y_pos, scores, color=colors, edgecolor='black')
    ax.set_yticks(y_pos)
    ax.set_yticklabels(labels, fontsize=18)  # Larger font size
    ax.invert_yaxis()  # Highest scores on top
    ax.set_xlabel("-log10(p-value)", fontsize=18)
    ax.set_title(f"Top Enriched Pathways ‚Äî {cancer_type.upper()} ({source})", fontsize=18)
    ax.tick_params(axis='x', labelsize=16) 
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.9, bottom=0.1) 

    out_path = Path(f"results/gene_prediction/{cancer_type}_{source}_bar_plot.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved bar plot: {out_path}")

def collect_enrichment_with_ratios(cancer_type, tag="bio", source="REAC", top_n=10):


    base_path = Path("results/gene_prediction/enrichment")
    base_path.mkdir(parents=True, exist_ok=True)
    terms = []

    for cluster_id in range(10):  # Clusters 0‚Äì9
        file = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_enrichment.csv"
        if not file.exists():
            continue

        df = pd.read_csv(file).sort_values("p_value")#.head(top_n)
        df = df[df['name'].str.len() < 60].head(top_n)
        for _, row in df.iterrows():
            raw_name = row['name']
            # Always define `term`, truncate if needed
            term = raw_name# if len(raw_name) <= 60 else raw_name[:57] + "..."
            pval = row['p_value']
            intersection = row['intersection_size']
            input_size = row.get('effective_domain_size', 1)  # Prevent zero division
            gene_ratio = intersection / input_size if input_size > 0 else 0
            terms.append((term, gene_ratio, -np.log10(pval + 1e-10), cluster_id))


    return sorted(terms, key=lambda x: x[2], reverse=True)

def draw_dot_plot_with_ratio_ori(terms, cancer_type, source, top_n=20):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    import seaborn as sns
    import matplotlib.pyplot as plt
    import pandas as pd

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    # üî¢ Select top N by LogP
    df = df.sort_values("LogP", ascending=False).head(top_n)
    
    plt.figure(figsize=(10, 0.4 * len(df)))
    scatter = sns.scatterplot(
        data=df,
        x="GeneRatio", y="Term",
        size="LogP", hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black",
        linewidth=0.5
    )

    # Remove legend
    if scatter.legend_:
        scatter.legend_.remove()
        
    #plt.xlabel("Gene Ratio", fontsize=16)
    plt.ylabel("Enriched Pathway", fontsize=18)
    plt.title(f"Dot Plot (Ratio) ‚Äî {cancer_type.upper()} ({source})", fontsize=18)
    plt.xscale("log")
    plt.xticks(fontsize=18)
    plt.xlabel("Gene Ratio (log scale)", fontsize=18)

    plt.yticks(fontsize=18)
    plt.grid(axis='x', linestyle='--', alpha=0.5)
    plt.tight_layout()

    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved dot plot with gene ratio: {out_path}")

def draw_dot_plot_with_ratio_no_bar(terms, cancer_type, source, top_n=20):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    import seaborn as sns
    import matplotlib.pyplot as plt
    import pandas as pd
    import numpy as np
    from pathlib import Path
    from matplotlib.cm import ScalarMappable
    from matplotlib.colors import Normalize

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])

    # üî¢ Select top N by LogP
    df = df.sort_values("LogP", ascending=False).head(top_n).reset_index(drop=True)

    # Normalize LogP for coloring
    norm = Normalize(vmin=df["LogP"].min(), vmax=df["LogP"].max())
    cmap = plt.get_cmap("Reds")

    fig, ax = plt.subplots(figsize=(10, 0.5 * len(df)))

    # Scatter plot on `ax`
    scatter = ax.scatter(
        df["GeneRatio"], np.arange(len(df)),
        s=df["LogP"] * 20,  # Size of dots
        c=df["LogP"], cmap=cmap,
        edgecolors="black", linewidths=0.5
    )

    # Annotate each dot with its LogP value
    for i, (x, logp) in enumerate(zip(df["GeneRatio"], df["LogP"])):
        ax.text(x, i, f"{logp:.1f}", ha='center', va='center', fontsize=9, color='black')

    # Y-axis labels
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df["Term"], fontsize=13)
    ax.set_xticks(ax.get_xticks())
    ax.set_xlabel("Gene Ratio (log scale)", fontsize=16)
    ax.set_ylabel("Enriched Pathway", fontsize=16)
    ax.set_title(f"Dot Plot ‚Äî {cancer_type.upper()} ({source})", fontsize=16)
    ax.set_xscale("log")
    ax.grid(axis='x', linestyle='--', alpha=0.3)

    # Add color bar for LogP, explicitly using the current Axes
    sm = ScalarMappable(norm=norm, cmap=cmap)
    sm.set_array([])
    cbar = fig.colorbar(sm, ax=ax, aspect=40, pad=0.02)
    cbar.set_label("LogP", fontsize=13)
    cbar.ax.tick_params(labelsize=12)

    plt.tight_layout()

    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved dot plot with gene ratio and LogP: {out_path}")

def draw_dot_plot_with_ratio_(terms, cancer_type, source, top_n=20):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    import seaborn as sns
    import matplotlib.pyplot as plt
    import pandas as pd
    from pathlib import Path

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    # üî¢ Select top N by LogP
    df = df.sort_values("LogP", ascending=False).head(top_n)
    df = df[::-1]  # Flip so most significant at the top

    plt.figure(figsize=(12, 0.5 * len(df)))

    # === Bars behind dots ===
    for i, (term, ratio, color) in enumerate(zip(df["Term"], df["GeneRatio"], df["Color"])):
        plt.barh(
            y=i, width=ratio, height=0.5, color=color, alpha=0.3,
            edgecolor='black', linewidth=0.5
        )
        plt.text(ratio + 0.01, i, f"{ratio:.3f}", va='center', fontsize=12)

    # === Dot scatter ===
    scatter = sns.scatterplot(
        data=df,
        x="GeneRatio", y=[i for i in range(len(df))],
        size="LogP", hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black", linewidth=0.5, legend=False
    )

    # === Aesthetics ===
    scatter.set_yticks(range(len(df)))
    scatter.set_yticklabels(df["Term"], fontsize=16)
    scatter.set_xlabel("Gene Ratio (log scale)", fontsize=18)
    scatter.set_title(f"Dot Plot (Ratio) ‚Äî {cancer_type.upper()} ({source})", fontsize=18)
    scatter.set_xscale("log")
    scatter.set_xlim(left=0.001)  # avoid log(0)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.grid(axis='x', linestyle='--', alpha=0.4)

    plt.tight_layout()
    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved dot plot with gene ratio: {out_path}")

def draw_dot_plot_with_ratio_(terms, cancer_type, source, top_n=20):
    import seaborn as sns
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    import pandas as pd
    from pathlib import Path

    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    # === Prepare Data ===
    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    # Sort by LogP descending, keep top N
    df = df.sort_values("LogP", ascending=False).head(top_n).iloc[::-1]  # Flip to make most significant at top
    y_positions = range(len(df))

    # === Plot Layout ===
    fig, ax = plt.subplots(figsize=(14, 0.5 * len(df)))

    # === Plot Gene Ratio Bar (Background) ===
    for i, (ratio, color) in enumerate(zip(df["GeneRatio"], df["Color"])):
        ax.barh(i, ratio, color=color, alpha=0.25, edgecolor='black', height=0.5, linewidth=0.3)

    # === Dot Plot (Gene Ratio with LogP as size) ===
    sns.scatterplot(
        data=df,
        x="GeneRatio",
        y=[i for i in range(len(df))],
        size="LogP",
        hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black",
        linewidth=0.5,
        legend=False,
        ax=ax
    )

    # Add Gene Ratio text inside bars
    for i, ratio in enumerate(df["GeneRatio"]):
        ax.text(ratio + 0.01, i, f"{ratio:.3f}", va='center', fontsize=10)

    # === Twin Axis for LogP Bar on Right ===
    ax_logp = ax.twiny()
    ax_logp.set_xlim(0, df["LogP"].max() * 1.2)
    ax_logp.set_xticks([])  # Optional: hide tick marks

    for i, logp in enumerate(df["LogP"]):
        ax_logp.barh(i, logp, left=0, color=df.iloc[i]["Color"], height=0.3, alpha=0.6, zorder=-1)
        ax_logp.text(logp + 0.2, i, f"{logp:.2f}", va="center", fontsize=10, color='black')

    # === Aesthetics ===
    ax.set_yticks(y_positions)
    ax.set_yticklabels(df["Term"], fontsize=16)
    ax.set_xlabel("Gene Ratio (log scale)", fontsize=16)
    ax.set_title(f"{cancer_type.upper()} ‚Äî {source}", fontsize=16, pad=12)
    ax.set_xscale("log")
    ax.set_xlim(left=0.001)
    ax.grid(axis='x', linestyle='--', alpha=0.4)
    ax.tick_params(axis='x', labelsize=12)

    ax_logp.set_ylabel("")  # Hide duplicated label
    ax_logp.tick_params(axis='x', labelsize=12)
    ax_logp.set_xlabel("-log10(p-value)", fontsize=16, labelpad=10)

    # === Legend for Clusters ===
    handles = [mpatches.Patch(color=CLUSTER_COLORS[c], label=f"Cluster {c}") for c in sorted(df["Cluster"].unique())]
    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc="upper left", title="Cluster", fontsize=16, title_fontsize=18)

    # === Save Figure ===
    plt.tight_layout()
    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_bar_combined.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved: {out_path}")

def draw_dot_plot_with_ratio_(terms, cancer_type, source, top_n=20):
    import seaborn as sns
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    import pandas as pd
    from pathlib import Path
    from matplotlib.gridspec import GridSpec

    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    # üî¢ Select top N by LogP and reverse so top term appears at top
    df = df.sort_values("LogP", ascending=False).head(top_n).iloc[::-1]
    y_positions = range(len(df))

    # === Layout with GridSpec ===
    fig = plt.figure(figsize=(14, 0.5 * len(df)))
    gs = GridSpec(nrows=1, ncols=2, width_ratios=[4, 1], wspace=0.2)

    # === Dot Plot (Left) ===
    ax1 = fig.add_subplot(gs[0])
    sns.scatterplot(
        data=df,
        x="GeneRatio",
        y=[i for i in range(len(df))],
        size="LogP",
        hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black",
        linewidth=0.5,
        legend=False,
        ax=ax1
    )

    ax1.set_yticks(y_positions)
    ax1.set_yticklabels(df["Term"], fontsize=16)
    ax1.set_xscale("log")
    ax1.set_xlim(left=0.001)
    ax1.set_xlabel("Gene Ratio (log scale)", fontsize=16)
    ax1.set_ylabel("Enriched Pathway", fontsize=16)
    ax1.set_title(f"Dot Plot (Ratio) ‚Äî {cancer_type.upper()} ({source})", fontsize=16)
    ax1.grid(axis='x', linestyle='--', alpha=0.4)
    ax1.tick_params(axis='x', labelsize=12)

    # === Bar Plot for LogP (Right) ===
    ax2 = fig.add_subplot(gs[1], sharey=ax1)
    ax2.barh(y=y_positions, width=df["LogP"], color=df["Color"], alpha=0.6, edgecolor="black")
    for i, logp in enumerate(df["LogP"]):
        ax2.text(logp + 0.2, i, f"{logp:.2f}", va='center', fontsize=10)

    ax2.set_xlim(0, df["LogP"].max() * 1.2)
    ax2.set_xlabel("-log10(p-value)", fontsize=16)
    ax2.tick_params(axis='x', labelsize=12)
    ax2.set_yticks([])  # Hide y-axis labels on right plot
    ax2.set_ylabel("")

    # === Cluster Legend (Optional) ===
    handles = [mpatches.Patch(color=CLUSTER_COLORS[c], label=f"Cluster {c}") for c in sorted(df["Cluster"].unique())]
    ax2.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc="upper left", title="Cluster", fontsize=10, title_fontsize=12)

    # === Save ===
    plt.tight_layout()
    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved dot plot with right-side bar: {out_path}")

def draw_dot_plot_with_ratio_(terms, cancer_type, source, top_n=20):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    df = df.sort_values("LogP", ascending=False).head(top_n)

    fig = plt.figure(figsize=(12, 0.45 * len(df)))
    gs = GridSpec(1, 2, width_ratios=[2, 1], wspace=0.05)

    # Dot plot on the left
    ax1 = fig.add_subplot(gs[0])
    scatter = sns.scatterplot(
        data=df,
        ax=ax1,
        x="GeneRatio", y="Term",
        size="LogP", hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black",
        linewidth=0.5
    )

    if scatter.legend_:
        scatter.legend_.remove()

    ax1.set_ylabel("Enriched Pathway", fontsize=18)
    ax1.set_xlabel("Gene Ratio (log scale)", fontsize=18)
    ax1.set_xscale("log")
    ax1.tick_params(axis='x', labelsize=14)
    ax1.tick_params(axis='y', labelsize=14)
    ax1.set_title(f"Dot Plot ‚Äî {cancer_type.upper()} ({source})", fontsize=18)
    ax1.grid(axis='x', linestyle='--', alpha=0.5)

    # Bar plot for LogP on the right
    ax2 = fig.add_subplot(gs[1], sharey=ax1)
    y_positions = range(len(df))
    ax2.barh(y=y_positions, width=df["LogP"], color=df["Color"], alpha=0.6, edgecolor="black")
    for i, logp in enumerate(df["LogP"]):
        ax2.text(logp + 0.2, i, f"{logp:.2f}", va='center', fontsize=16)

    ax2.set_xlim(0, df["LogP"].max() * 1.2)
    ax2.set_xlabel("-log10(p-value)", fontsize=18)
    ax2.tick_params(axis='x', labelsize=12)
    ax2.set_yticks([])  # Hide y-axis labels
    ax2.set_ylabel("")

    plt.tight_layout()
    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved dot plot with right-side bar: {out_path}")

def draw_dot_plot_with_ratio_(terms, cancer_type, source, top_n=20):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    # üî¢ Select top N by LogP
    df = df.sort_values("LogP", ascending=False).head(top_n)
    df = df[::-1]  # reverse to make highest logP at top in horizontal bar

    y_positions = range(len(df))

    # Create subplots with GridSpec
    fig = plt.figure(figsize=(12, 0.5 * len(df)))
    gs = GridSpec(1, 2, width_ratios=[3, 2], wspace=0.05)

    # === Left: Dot Plot ===
    ax1 = fig.add_subplot(gs[0])
    scatter = sns.scatterplot(
        data=df,
        x="GeneRatio", y="Term",
        size="LogP", hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black",
        linewidth=0.5,
        ax=ax1
    )

    if scatter.legend_:
        scatter.legend_.remove()

    ax1.set_xlabel("Gene Ratio (log scale)", fontsize=16)
    ax1.set_ylabel("Enriched Pathway", fontsize=16)
    ax1.set_xscale("log")
    ax1.tick_params(axis='both', labelsize=12)
    ax1.grid(axis='x', linestyle='--', alpha=0.5)

    # === Right: Bar Plot of LogP ===
    ax2 = fig.add_subplot(gs[1], sharey=ax1)
    ax2.barh(y=y_positions, width=df["LogP"], color=df["Color"], alpha=0.6, edgecolor="black")
    for i, logp in enumerate(df["LogP"]):
        ax2.text(logp + 0.2, i, f"{logp:.2f}", va='center', fontsize=12)

    ax2.set_xlim(0, df["LogP"].max() * 1.2)
    ax2.set_xlabel("-log10(p-value)", fontsize=16)
    ax2.tick_params(axis='x', labelsize=12)
    ax2.set_yticks([])  # Hide y-tick marks on right
    ax2.set_ylabel("")

    # === Save ===
    plt.tight_layout(rect=[0.05, 0, 1, 1])  # leave room on left
    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved dot plot with right-side bar: {out_path}")

def eigengap_analysis_ori(feature_matrix, max_clusters=25, normalize=True, plot_path=None):
    # Step 1: Normalize features (optional)
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Print eigenvalues
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({"Eigenvalue Index": np.arange(len(eigenvals)), "Eigenvalue": eigenvals}).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Optional plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]
        
        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        gaps = np.diff(eigenvals[1:max_clusters + 1])
        best_k = np.argmax(gaps) + 1  # +1 because diff shifts index

        plt.axvline(
            x=best_k,
            color='pink',
            linestyle='--',
            label=f'Eigengap ‚Üí k={best_k}'
        )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=16)

        # Set integer x-axis ticks
        plt.xticks(range(1, max_clusters + 1))

        # Remove grid
        plt.grid(False)

        # Remove legend frame
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()
    else:
        # If no plot is saved, still compute best_k
        gaps = np.diff(eigenvals[1:max_clusters + 1])
        best_k = np.argmax(gaps) + 1

    return best_k, eigenvals

def eigengap_analysis_(feature_matrix, max_clusters=25, normalize=True, plot_path=None):
    # Step 1: Normalize features (optional)
    '''if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)'''

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Print eigenvalues
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({"Eigenvalue Index": np.arange(len(eigenvals)), "Eigenvalue": eigenvals}).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Compute eigengaps and best_k, ensuring k > 1
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    best_k = np.argmax(gaps) + 5  # +1 for diff shift, +1 to ensure k > 1

    # Clamp to max_clusters
    best_k = max(5, min(best_k, max_clusters))

    # Optional plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]
        
        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        plt.axvline(
            x=best_k,
            color='pink',
            linestyle='--',
            label=f'Eigengap ‚Üí k={best_k}'
        )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=16)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)

        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return best_k, eigenvals

def plot_bio_clusterwise_feature_contributions(
    args,
    relevance_scores,           # 2D array (samples x features)
    row_labels,             # 1D array of cluster assignments
    feature_names,              # List of feature names (e.g., MF: BRCA, ...)
    per_cluster_feature_contributions_output_dir, 
    omics_colors):             # Dict of omics type colors (e.g., 'mf': '#D62728')
    
    import os
    import numpy as np
    import matplotlib.pyplot as plt
    from collections import defaultdict

    os.makedirs(per_cluster_feature_contributions_output_dir, exist_ok=True)

    def get_omics_color(feature_name):
        prefix = feature_name.split(":")[0].lower()
        return omics_colors.get(prefix, "#AAAAAA")

    def get_omics_prefix(feature_name):
        return feature_name.split(":")[0].lower()

    # Group features by omics type and preserve their indices
    omics_groups = defaultdict(list)
    for idx, fname in enumerate(feature_names):
        omics_groups[get_omics_prefix(fname)].append((idx, fname))

    # Follow omics_colors ordering if possible
    ordered_features = []
    for omics in omics_colors:
        ordered_features.extend(omics_groups.get(omics, []))
    for omics in omics_groups:
        if omics not in omics_colors:
            ordered_features.extend(omics_groups[omics])

    ordered_indices = [idx for idx, _ in ordered_features]
    ordered_feature_names = [name for _, name in ordered_features]

    unique_clusters = np.unique(row_labels)

    for cluster_id in sorted(unique_clusters):
        indices = np.where(row_labels == cluster_id)[0]
        cluster_scores = relevance_scores[indices]
        avg_contribution = np.mean(cluster_scores, axis=0)
        total_score = np.sum(avg_contribution)

        fig, ax = plt.subplots(figsize=(10, 2.5))

        x = np.linspace(0, 1, len(ordered_feature_names))
        bar_width = 1 / len(ordered_feature_names) * 0.95

        bars = ax.bar(
            x,
            avg_contribution[ordered_indices],
            width=bar_width,
            color=[get_omics_color(name) for name in ordered_feature_names],
            align='center'
        )

        ax.set_title(
            fr"Cluster {cluster_id} $\mathregular{{({len(indices)}\ genes,\ avg = {total_score:.2f})}}$",
            fontsize=16
        )

        clean_labels = [name.split(":")[1].strip() if ":" in name else name for name in ordered_feature_names]
        ax.set_xticks(x)
        ax.set_xticklabels(clean_labels, rotation=90)

        for label, feature_name in zip(ax.get_xticklabels(), ordered_feature_names):
            label.set_color(get_omics_color(feature_name))

        ax.tick_params(axis='x', labelsize=9)
        ax.set_xlim(-bar_width, 1 + bar_width)

        plt.tight_layout()
        save_path = os.path.join(
            per_cluster_feature_contributions_output_dir,
            f"{args.model_type}_{args.net_type}_BIO_cluster_{cluster_id}_feature_contributions_epo{args.num_epochs}.png"
        )
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"‚úÖ Saved BIO feature contribution barplot for Cluster {cluster_id} to {save_path}")

def save_graph_with_clusters(graph, save_path):
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster_bio_summary': graph.ndata['cluster_bio_summary']
    }, save_path)

def compute_total_genes_per_cluster(row_labels, n_clusters):
    return {i: np.sum(row_labels == i) for i in range(n_clusters)}

def compute_relevance_scores_norm(
    model,
    graph,
    features,
    node_indices,
    normalize=True,
    feature_groups=None):  # e.g., {"bio": (0, 1024), "topo": (1024, 2048)}):
    """
    Compute saliency-based relevance scores with optional normalization and feature group selection.

    Args:
        model (torch.nn.Module): Your trained GNN model.
        graph (DGLGraph): The graph structure.
        features (torch.Tensor): Node feature matrix (N x F).
        node_indices (list[int]): Node indices to compute relevance for.
        normalize (bool): Whether to normalize saliency per node.
        feature_groups (dict): Dict of feature group name ‚Üí (start, end) slice indices.

    Returns:
        dict: node_idx ‚Üí dict of {group_name: relevance_tensor} or "all": full saliency
    """
    model.eval()
    features = features.clone().detach().requires_grad_(True)

    output = model(graph, features)
    relevance_dict = {}

    for node_idx in node_indices:
        model.zero_grad()
        node_score = output[node_idx].squeeze()
        node_score.backward(retain_graph=True)

        saliency = features.grad[node_idx].detach().abs()  # (F,)
        
        if normalize:
            saliency = saliency / (saliency.sum() + 1e-9)

        if feature_groups:
            group_scores = {}
            for group_name, (start, end) in feature_groups.items():
                group_scores[group_name] = saliency[start:end]
            relevance_dict[node_idx] = group_scores
        else:
            relevance_dict[node_idx] = {"all": saliency}

        features.grad.zero_()

    return relevance_dict

def count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters):
    pred_counts = {i: 0 for i in range(n_clusters)}
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]
    for idx in predicted_indices:
        if 0 <= idx < len(row_labels):
            pred_counts[row_labels[idx]] += 1
    return pred_counts, predicted_indices

def plot_bio_heatmap_unsort_no_legend_patches(summary_bio_features, row_labels, col_labels, predicted_indices, output_path):
    from matplotlib.colors import LinearSegmentedColormap
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names_64 = [f"{omics}_{cancer}" for omics in ['cna', 'ge', 'meth', 'mf'] for cancer in cancer_names]
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_bio_features[predicted_indices]
    row_labels_topk = row_labels[predicted_indices]
    sorted_indices = np.argsort(row_labels_topk)
    sorted_matrix = summary_topk[sorted_indices, :]
    sorted_labels = row_labels_topk[sorted_indices]

    # === Heatmap layout with narrower cluster bar and no gap
    fig = plt.figure(figsize=(17, 17))
    gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=[0.25, 13.7], wspace=0)

    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])

    # === Cluster color bar
    cluster_colors = [to_rgb(CLUSTER_COLORS[label]) for label in sorted_labels]
    cluster_colors_array = np.array(cluster_colors).reshape(-1, 1, 3)
    ax_cluster.imshow(cluster_colors_array, aspect='auto')
    ax_cluster.axis("off")

    cluster_counts = Counter(sorted_labels)
    start_idx = 0
    for cluster_id in sorted(cluster_counts):
        count = cluster_counts[cluster_id]
        mid_idx = start_idx + count // 2
        ax_cluster.text(-0.75, mid_idx, f'{count}', va='center', ha='right',
                        fontsize=16, fontweight='bold', color='black')  # larger cluster number
        start_idx += count

        # === Heatmap
        bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
        sns.heatmap(sorted_matrix, cmap=bluish_gray_gradient, center=0, vmin=-2, vmax=2,
                    ax=ax_heatmap, cbar=False)
        ax_heatmap.set_title("Heatmap of Summary Bio Features Sorted by Spectral Biclusters", fontsize=18)
        ax_heatmap.set_yticks([])

        xticks = np.arange(len(bio_feat_names_64)) + 0.5
        ax_heatmap.set_xticks(xticks)

        # Set only cancer names, big and bold
        cancer_labels_only = [name.split('_')[1] for name in bio_feat_names_64]
        ax_heatmap.set_xticklabels(cancer_labels_only, rotation=90, fontsize=18)#)

        # Color each tick label based on omics type
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }
        for tick_label, name in zip(ax_heatmap.get_xticklabels(), bio_feat_names_64):
            omics = name.split('_')[0]
            tick_label.set_color(omics_colors.get(omics, 'black'))

        ax_heatmap.set_xlabel("Summary Bio Features (Grouped by Omics)", fontsize=18)
        fig.tight_layout()
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        fig.savefig(output_path, dpi=300)
        plt.close(fig)
        print(f"‚úÖ Summary bio heatmap saved to {output_path}")

def plot_tsne(features, row_labels, predicted_indices, n_clusters, output_path):
    tsne = TSNE(n_components=2, perplexity=min(30, len(features) - 1), random_state=42)
    reduced_embeddings = tsne.fit_transform(features)
    plt.figure(figsize=(12, 10))
    for cluster_id in range(n_clusters):
        idx = np.where(row_labels == cluster_id)[0]
        plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                    color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                    edgecolor='k', s=100, alpha=0.8)
    for idx in predicted_indices:
        x, y = reduced_embeddings[idx]
        plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)
    plt.xlabel("t-SNE Dimension 1", fontsize=18)
    plt.ylabel("t-SNE Dimension 2", fontsize=18)
    plt.title("t-SNE of Spectral Biclustering (Summary Bio Features)")
    plt.savefig(output_path, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ t-SNE visualization saved to {output_path}")

def plot_bio_heatmap_raw(summary_bio_features, predicted_indices, save_path):
    """
    Plot raw (unclustered) heatmap of summary bio features for predicted genes,
    with omics bar and consistent styling.

    Parameters:
    - summary_bio_features: np.ndarray, shape (num_nodes, 64)
    - predicted_indices: list of indices for predicted cancer genes
    - save_path: path to save the heatmap PNG
    """
    print("üìä Plotting raw bio heatmap (unclustered)...")
    assert summary_bio_features.shape[1] == 64, "Expected 64 summary bio features."

    # === Select only predicted cancer genes
    data = summary_bio_features[predicted_indices]

    # === Omics group boundaries for 64-dim summary bio features
    omics_group_sizes = {
        "expression": 16,
        "methylation": 16,
        "mutation": 16,
        "copy_number": 16
    }
    omics_colors = {
        "expression": "#76D7C4",
        "methylation": "#F7DC6F",
        "mutation": "#F5B7B1",
        "copy_number": "#C39BD3"
    }

    # === Feature names
    feature_names = [f"{omics}_{i}" for omics, size in omics_group_sizes.items() for i in range(size)]
    omics_color_bar = []
    for group, size in omics_group_sizes.items():
        omics_color_bar.extend([omics_colors[group]] * size)

    # === Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    # === Plotting
    fig = plt.figure(figsize=(14, 10))
    grid_spec = fig.add_gridspec(nrows=2, ncols=1, height_ratios=[0.15, 0.85])
    ax_contrib = fig.add_subplot(grid_spec[0])
    ax_heatmap = fig.add_subplot(grid_spec[1])

    # === Column contribution bar
    contrib = data.sum(axis=0)
    ax_contrib.bar(np.arange(data.shape[1]), contrib, color="#85929e", width=1.0)
    ax_contrib.axis("off")

    # === Heatmap
    sns.heatmap(
        data,
        cmap=bluish_gray_gradient,
        ax=ax_heatmap,
        cbar_kws={"label": "Feature Relevance"},
        xticklabels=feature_names,
        yticklabels=[f"Gene{i}" for i in range(data.shape[0])],
        linewidths=0.2,
        linecolor='gray'
    )
    ax_heatmap.set_xlabel("Biological Features")
    ax_heatmap.set_ylabel("Predicted Cancer Genes")
    ax_heatmap.tick_params(axis='x', rotation=90)

    # === Omics group bar
    for x, color in enumerate(omics_color_bar):
        ax_heatmap.add_patch(plt.Rectangle((x, -1), 1, 0.5, color=color, transform=ax_heatmap.transData, clip_on=False))

    # === Omics legend
    handles = [Patch(facecolor=color, label=label) for label, color in omics_colors.items()]
    ax_heatmap.legend(
        handles=handles,
        title="Omics Group",
        loc='upper right',
        bbox_to_anchor=(1.15, 1.0),
        frameon=True
    )

    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Raw bio heatmap saved to: {save_path}")

def plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path):
    """
    Plot unclustered raw heatmap of 64-dim summary bio features for top predicted genes.
    Features are grouped into 4 omics types x 16 cancers and are color-coded on x-axis.

    Parameters:
    - summary_bio_features: np.ndarray of shape (num_nodes, 64)
    - predicted_indices: list or array of top predicted gene indices
    - output_path: str, path to save the heatmap image
    """
    print("üìä Plotting raw bio heatmap without clustering...")

    # === Top-K predicted genes to show
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_bio_features[predicted_indices]

    # === Bio feature labels (4 omics √ó 16 cancers)
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names_64 = [f"{omics}_{cancer}" for omics in ['cna', 'ge', 'meth', 'mf'] for cancer in cancer_names]

    # === Figure layout
    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)

    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    # === Fake cluster panel (empty) for visual consistency
    ax_cluster.axis("off")

    # === Colormap and heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(summary_topk, cmap=bluish_gray_gradient, center=0, vmin=-2, vmax=2,
                ax=ax_heatmap, cbar=False)

    ax_heatmap.set_title("Raw Heatmap of Summary Bio Features (Top Predicted Genes)", fontsize=18)
    ax_heatmap.set_yticks([])

    # === X-axis feature names (with omics coloring)
    xticks = np.arange(len(bio_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    cancer_labels_only = [name.split('_')[1] for name in bio_feat_names_64]
    ax_heatmap.set_xticklabels(cancer_labels_only, rotation=90, fontsize=18)

    # === Omics coloring for x-tick labels
    omics_colors = {'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222'}
    for tick_label, name in zip(ax_heatmap.get_xticklabels(), bio_feat_names_64):
        omics = name.split('_')[0]
        tick_label.set_color(omics_colors.get(omics, 'black'))

    # === Legend bar for omics types
    omics_patches = [Patch(color=color, label=omics.upper()) for omics, color in omics_colors.items()]
    ax_legend.legend(handles=omics_patches, loc='center', ncol=len(omics_patches),
                     fontsize=18, frameon=False)

    # === Final adjustments and save
    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.03, hspace=0.2, wspace=0.01)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)

    print(f"‚úÖ Raw bio heatmap saved to {output_path}")

def plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path):
    """
    Plot unclustered raw heatmap of 64-dim topological summary features for top predicted genes.

    Parameters:
    - summary_topo_features: np.ndarray of shape (num_nodes, 64)
    - predicted_indices: list or array of top predicted gene indices
    - output_path: str, path to save the heatmap image
    """
    print("üìä Plotting raw topo heatmap without clustering...")

    # === Top-K predicted genes to show
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_topo_features[predicted_indices]

    # === Topo feature labels: "00" to "63"
    topo_feat_names_64 = [f"{i:02d}" for i in range(64)]

    # === Layout
    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)
    
    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    # === Empty cluster column for layout consistency
    ax_cluster.axis("off")

    # === Heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(
        summary_topk,
        cmap=bluish_gray_gradient,
        center=0,
        vmin=-2, vmax=2,
        ax=ax_heatmap,
        cbar=False
    )
    ax_heatmap.set_title("Raw Topological Feature Heatmap (Top Predicted Genes)", fontsize=18, pad=12)
    ax_heatmap.set_yticks([])

    xticks = np.arange(len(topo_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    ax_heatmap.set_xticklabels(topo_feat_names_64, rotation=90, fontsize=12)

    # === Save
    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.05)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)

    print(f"‚úÖ Raw topo heatmap saved to {output_path}")

def apply_full_spectral_biclustering_bio_(graph, summary_bio_features, node_names,
                                         predicted_cancer_genes, n_clusters,
                                         save_path, save_row_labels_path,
                                         save_total_genes_per_cluster_path, save_predicted_counts_path,
                                         output_path_genes_clusters, output_path_heatmap):

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Normalize features
    summary_bio_features = StandardScaler().fit_transform(summary_bio_features)

    # === Spectral biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_bio_features)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    graph.ndata['cluster_bio_summary'] = torch.tensor(row_labels, dtype=torch.long)
    
    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === Save original (unclustered) heatmap before biclustering
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    #plot_bio_heatmap_raw(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)    
    plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_topo_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    """
    Plots a spectral biclustering heatmap for topological embeddings (1024‚Äì2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # üîπ Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # üîπ Create topo feature names (01‚Äì64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # üîπ Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # üîπ Sort by cluster ‚Üí then by gene-wise relevance within cluster
    sorted_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        sorted_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[sorted_indices]
    sorted_clusters = row_labels[sorted_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in sorted_indices]

    # üîπ Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # üîπ Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # üîπ Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # üîπ Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # üîπ Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # üîπ Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # üîπ Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # üîπ Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved spectral clustering heatmap to {output_path}")

    # üîπ Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path):

    topo_feat_names_64 = [f"{i:02d}" for i in range(64)]
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_topo_features[predicted_indices]
    row_labels_topk = row_labels[predicted_indices]
    sorted_indices = np.argsort(row_labels_topk)
    sorted_matrix = summary_topk[sorted_indices, :]
    sorted_labels = row_labels_topk[sorted_indices]

    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)
    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    cluster_colors = [to_rgb(CLUSTER_COLORS[label]) for label in sorted_labels]
    cluster_colors_array = np.array(cluster_colors).reshape(-1, 1, 3)
    ax_cluster.imshow(cluster_colors_array, aspect='auto')
    ax_cluster.axis("off")

    # Add cluster size labels
    cluster_counts = Counter(sorted_labels)
    start_idx = 0
    for cluster_id in sorted(cluster_counts):
        count = cluster_counts[cluster_id]
        mid_idx = start_idx + count // 2
        ax_cluster.text(-0.75, mid_idx, f'{count}', va='center', ha='right',
                        fontsize=16, fontweight='bold', color='black')
        start_idx += count

    # Heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(
        sorted_matrix,
        cmap=bluish_gray_gradient,
        center=0,
        vmin=-2, vmax=2,
        ax=ax_heatmap,
        cbar=False
    )
    ax_heatmap.set_title("Topological Feature Heatmap (Sorted by Cluster)", fontsize=18, pad=12)
    ax_heatmap.set_yticks([])

    xticks = np.arange(len(topo_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    ax_heatmap.set_xticklabels(topo_feat_names_64, rotation=90, fontsize=12)

    # Save
    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.05)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"‚úÖ Summary topo heatmap saved to {output_path}")

def plot_bio_heatmap_unsort(
    summary_bio_features, 
    row_labels, 
    col_labels, 
    predicted_indices, 
    output_path
    ):

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names_64 = [f"{omics}_{cancer}" for omics in ['cna', 'ge', 'meth', 'mf'] for cancer in cancer_names]
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_bio_features[predicted_indices]
    row_labels_topk = row_labels[predicted_indices]
    sorted_indices = np.argsort(row_labels_topk)
    sorted_matrix = summary_topk[sorted_indices, :]
    sorted_labels = row_labels_topk[sorted_indices]

    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)
    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    cluster_colors = [to_rgb(CLUSTER_COLORS[label]) for label in sorted_labels]
    cluster_colors_array = np.array(cluster_colors).reshape(-1, 1, 3)
    ax_cluster.imshow(cluster_colors_array, aspect='auto')
    ax_cluster.axis("off")

    cluster_counts = Counter(sorted_labels)
    start_idx = 0
    for cluster_id in sorted(cluster_counts):
        count = cluster_counts[cluster_id]
        mid_idx = start_idx + count // 2
        ax_cluster.text(-0.75, mid_idx, f'{count}', va='center', ha='right',
                        fontsize=16, fontweight='bold', color='black')
        start_idx += count

    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(sorted_matrix, cmap=bluish_gray_gradient, center=0, vmin=-2, vmax=2,
                ax=ax_heatmap, cbar=False)
    ax_heatmap.set_title("Heatmap of Summary Bio Features Sorted by Spectral Biclusters", fontsize=18)
    ax_heatmap.set_yticks([])

    xticks = np.arange(len(bio_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    cancer_labels_only = [name.split('_')[1] for name in bio_feat_names_64]
    ax_heatmap.set_xticklabels(cancer_labels_only, rotation=90, fontsize=18)

    omics_colors = {'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222'}
    for tick_label, name in zip(ax_heatmap.get_xticklabels(), bio_feat_names_64):
        omics = name.split('_')[0]
        tick_label.set_color(omics_colors.get(omics, 'black'))

    omics_patches = [Patch(color=color, label=omics.upper()) for omics, color in omics_colors.items()]
    ax_legend.legend(handles=omics_patches, loc='center', ncol=len(omics_patches),
                     fontsize=18, frameon=False)

    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.03, hspace=0.2, wspace=0.01)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"‚úÖ Summary bio heatmap saved to {output_path}")

def plot_bio_biclustering_heatmap_unsort_random_clusterbar(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    
    # üîπ Extract and normalize relevance scores
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # Build feature color bar
    feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    # Normalize per-feature means
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripe
    for i, cluster in enumerate(row_labels):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(row_labels, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # LRP curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_clusters_unsort_(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Sort by cluster only (no intra-cluster sorting)
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    # Keep original feature (column) order
    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_colors = []

    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    feature_names = [feature_names[i] for i in original_col_indices]

    # Colormap setup
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # Top bar per feature
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(x=i + 0.5, height=val, width=1.0, color=color, edgecolor='black', linewidth=0.5, alpha=0.3 + 0.7 * val)

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Feature tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve (LRP sum)
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(np.arange(len(saliency_sums)), 0, saliency_sums, color='#a9cce3', alpha=0.8)#, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bar
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            #alpha=0.3 + 0.7 * norm_mean
            alpha = min(1.0, 0.3 + 0.7 * norm_mean)

        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_topo_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):

    # üîπ Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    #relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    
    # üîπ Create topo feature names (01‚Äì64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # üîπ Sort by cluster ‚Üí then by gene-wise relevance within cluster
    sorted_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        sorted_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[sorted_indices]
    sorted_clusters = row_labels[sorted_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in sorted_indices]

    # üîπ Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # üîπ Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    ax_legend = fig.add_subplot(gs[14, 2:45])

    # üîπ Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # üîπ Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # üîπ Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # üîπ Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # üîπ Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Omics + LRP Legend
    ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )

    # üîπ Saliency Sum curve
    lrp_sums = sorted_scores.sum(axis=1)
    lrp_sums = (lrp_sums - lrp_sums.min()) / (lrp_sums.max() - lrp_sums.min())
    y = np.arange(len(lrp_sums))

    ax_curve.fill_betweenx(
        y, 0, lrp_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(lrp_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved spectral clustering heatmap to {output_path}")

    # üîπ Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_topo_heatmap_unsorted(
    args,
    relevance_scores,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Plots a topological heatmap (unsorted) using the same visual formatting
    as the biclustering version, but with rows and columns in original order.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.
        col_labels (list of str, optional): Optional column label annotations.
    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # üîπ Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # üîπ Feature names
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    if col_labels is not None:
        col_labels = np.array(col_labels)

    # üîπ Apply log1p for better contrast
    scores_log = np.log1p(relevance_scores)

    # üîπ Set up figure
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # üîπ Colorbar range
    vmin, vmax = 0, np.percentile(scores_log, 99)

    # üîπ Feature contribution bar (column saliency)
    feature_means = scores_log.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5,
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )
    ax_bar.set_xlim(0, len(feature_means))
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_xticks([])
    ax_bar.set_yticks([])
    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)

    # üîπ Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # üîπ Heatmap
    sns.heatmap(
        scores_log,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Tick labels (X)
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    # üîπ Omics/Saliency Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # üîπ Saliency sum curve (row-wise sum)
    saliency_sums = scores_log.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ['right', 'left', 'bottom', 'top']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Final layout
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved unsorted topological heatmap to {output_path}")

    return pd.DataFrame(scores_log, index=gene_names, columns=feature_names)

def plot_topo_biclustering_heatmap_unsorted_(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Unsorted version of topo spectral biclustering heatmap ‚Äî preserves original feature and gene order.
    """
    # üîπ Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # üîπ Create topo feature names (01‚Äì64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    if col_labels is not None:
        col_labels = np.array(col_labels)

    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    # üîπ No sorting of rows or columns
    scores = relevance_scores
    clusters = row_labels


    if gene_names is not None:
        gene_names = list(gene_names)

    # üîπ Compute cluster boundaries and centers
    _, counts = np.unique(clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]


    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # üîπ Apply log transformation
    scores = np.log1p(scores)

    # üîπ Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )


    # üîπ Feature-wise average bar (same order)
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0,
               color="#B0BEC5", linewidth=0, alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # üîπ Heatmap intensity scaling
    vmin, vmax = 0, np.percentile(scores, 99)

    sns.heatmap(
        scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster color stripes
    for i, cluster in enumerate(clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Cluster size labels
    for cluster_id, center_y, count in zip(np.unique(clusters), cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # üîπ Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # üîπ Saliency Sum Curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(y, 0, saliency_sums,
                           color='#a9cce3', alpha=0.8, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.05, xmin=0, xmax=1,
                    color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Save figure
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved unsorted topo clustering heatmap to {output_path}")

    # üîπ Optional: Per-cluster contribution plot
    '''plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # original, not sorted
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )'''

    return pd.DataFrame(scores, index=gene_names, columns=feature_names)

def plot_topo_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())# * 10

    # if topo_colors is None:
    #     topo_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'] * 16  # Adjust to match 64 features

    # üîπ Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    row_labels = np.array(row_labels)
    
    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    #feature_colors = topo_colors[:len(feature_names)]

    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    
    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # üîπ Log-transform scores
    scores = np.log1p(relevance_scores)

    # üîπ Color map
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # üîπ Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # üîπ Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1, 
            linewidth=0, 
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), 
            clip_on=False
        ))

    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size
    
    # üîπ X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # üîπ Saliency Sum Curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 
        
        0, 
        saliency_sums,
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # üîπ Save figure
    plt.subplots_adjust(wspace=0, hspace=0)
    
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved unsorted topo clustering heatmap to {output_path}")

def plot_bio_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Grid layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])       # top bar
    ax        = fig.add_subplot(gs[1:13, 2:45])     # main heatmap
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)  # saliency curve
    ax_cbar   = fig.add_subplot(gs[5:9, 49])       # colorbar

    # Top feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)


    # Saliency curve 
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars (right below feature bar, above heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    
    # üîπ Extract and normalize relevance scores
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # üîπ Feature color mapping
    '''feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color'''

    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    # Normalize per-feature means
    #feature_means = relevance_scores.mean(axis=0)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Sort by cluster only (no intra-cluster sorting)
    '''cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]'''

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1, 
            linewidth=0, 
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), 
            clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # üîπ Add cluster size labels
    # for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
    #     ax.text(
    #         -2.0, center_y, f"{count}",
    #         va='center', ha='right', fontsize=18, fontweight='bold'
    #     )
        
    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    '''mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )'''

    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )
        
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_topo_biclustering_heatmap_unsorted(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Unsorted topo heatmap with connected cluster bar and gene count labels.
    """
    from matplotlib.patches import Rectangle

    # üîπ Normalize summary topo features (0-1)
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # üîπ Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    row_labels = np.array(row_labels)

    sorted_indices = np.argsort(row_labels)
    row_labels = row_labels[sorted_indices]
    
    # üîπ Compute cluster stats
    _, counts = np.unique(row_labels, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # üîπ Log-transform scores
    scores = np.log1p(relevance_scores)

    # üîπ Color map
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # üîπ Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # üîπ Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        scores, cmap=bluish_gray_gradient, vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster bar
    for i, cluster in enumerate(row_labels):
        ax.add_patch(Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Add cluster counts
    for cluster_id, center_y, count in zip(np.unique(row_labels), cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # üîπ Saliency sum curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.05, xmin=0, xmax=1,
                    color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Save plot
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0.02, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved unsorted topo clustering heatmap to {output_path}")

    return pd.DataFrame(scores, index=gene_names, columns=feature_names)

def best_balanced_biclustering_(data, n_clusters, n_trials=20):
    best_model = None
    best_balance_score = float('inf')

    for seed in range(n_trials):
        model = SpectralBiclustering(
            n_clusters=n_clusters,
            method='log',
            svd_method='arpack',
            n_best=3,
            random_state=seed
        )
        model.fit(data)
        _, counts = np.unique(model.row_labels_, return_counts=True)
        balance_score = counts.max() - counts.min()
        if balance_score < best_balance_score:
            best_balance_score = balance_score
            best_model = model
    return best_model

def best_balanced_biclustering_(data, n_clusters, n_trials=20):
    from sklearn.cluster import SpectralBiclustering
    import numpy as np

    n_rows, n_cols = data.shape
    max_clusters = min(n_rows, n_cols) - 1
    if n_clusters >= max_clusters:
        print(f"‚ö†Ô∏è Reducing n_clusters from {n_clusters} to {max_clusters} to satisfy SVD requirements.")
        n_clusters = max_clusters

    best_model = None
    best_balance_score = float('inf')

    for seed in range(n_trials):
        try:
            model = SpectralBiclustering(
                n_clusters=n_clusters,
                method='log',
                svd_method='arpack',
                n_best=3,
                random_state=seed
            )
            model.fit(data)
            _, counts = np.unique(model.row_labels_, return_counts=True)
            balance_score = counts.max() - counts.min()
            if balance_score < best_balance_score:
                best_balance_score = balance_score
                best_model = model
        except ValueError as e:
            print(f"Trial {seed} failed with error: {e}")
            continue

    if best_model is None:
        raise RuntimeError("‚ùå All Spectral Biclustering trials failed. Check input shape or cluster size.")

    return best_model

def best_balanced_biclustering(data, n_clusters, n_trials=20, verbose=False):
    from sklearn.cluster import SpectralBiclustering
    import numpy as np

    n_rows, n_cols = data.shape
    max_clusters = min(n_rows, n_cols) - 1
    if n_clusters >= max_clusters:
        print(f"‚ö†Ô∏è Reducing n_clusters from {n_clusters} to {max_clusters} to satisfy SVD requirements.")
        n_clusters = max_clusters

    best_model = None
    best_balance_score = float('inf')
    best_seed = None

    for seed in range(n_trials):
        try:
            model = SpectralBiclustering(
                n_clusters=n_clusters,
                method='log',
                svd_method='arpack',
                n_best=3,
                random_state=seed
            )
            model.fit(data)
            _, row_counts = np.unique(model.row_labels_, return_counts=True)
            balance_score = row_counts.max() - row_counts.min()

            if verbose:
                print(f"Trial {seed}: cluster sizes = {row_counts}, balance score = {balance_score}")

            if balance_score < best_balance_score:
                best_balance_score = balance_score
                best_model = model
                best_seed = seed

        except ValueError as e:
            if verbose:
                print(f"‚ö†Ô∏è Trial {seed} failed: {e}")
            continue

    if best_model is None:
        raise RuntimeError("‚ùå All Spectral Biclustering trials failed. Check input shape or cluster size.")

    if verbose:
        print(f"‚úÖ Best model found with seed {best_seed}, balance score = {best_balance_score}")

    return best_model

def plot_bio_biclustering_heatmap_clusters_unsort_descending(
    args,
    relevance_scores,
    row_labels,  # not used now
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize to [0, 10]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = relevance_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])       # top bar
    ax        = fig.add_subplot(gs[1:13, 2:45])     # main heatmap
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)  # saliency curve
    ax_cbar   = fig.add_subplot(gs[5:9, 49])       # colorbar

    # Top feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # X-axis labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve 
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_topo_biclustering_heatmap_clusters_unsort_descending(
    args,
    relevance_scores,
    output_path,
    gene_names=None,
    col_labels=None
):
    # üîπ Normalize relevance scores to [0, 1]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # üîπ Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # üîπ Log-transform scores for smoother heatmap
    scores = np.log1p(relevance_scores)

    # üîπ Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # üîπ Prepare layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # üîπ Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # üîπ Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # üîπ Saliency Sum Curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y,
        0,
        saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5,
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # üîπ Save figure
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved topo heatmap (unsorted, no clusters) to {output_path}")

def plot_topo_biclustering_heatmap_unsorted_descending(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Unsorted topo heatmap with connected cluster bar and gene count labels.
    """
    from matplotlib.patches import Rectangle

    # üîπ Normalize summary topo features (0-1)
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # üîπ Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    row_labels = np.array(row_labels)

    sorted_indices = np.argsort(row_labels)
    row_labels = row_labels[sorted_indices]
    
    # üîπ Compute cluster stats
    _, counts = np.unique(row_labels, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # üîπ Log-transform scores
    scores = np.log1p(relevance_scores)

    # üîπ Color map
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # üîπ Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # üîπ Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        scores, cmap=bluish_gray_gradient, vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster bar
    for i, cluster in enumerate(row_labels):
        ax.add_patch(Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Add cluster counts
    for cluster_id, center_y, count in zip(np.unique(row_labels), cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # üîπ Saliency sum curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.05, xmin=0, xmax=1,
                    color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Save plot
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0.02, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved unsorted topo clustering heatmap to {output_path}")

    return pd.DataFrame(scores, index=gene_names, columns=feature_names)

def compute_relevance_scores_integrated_gradients_no_progress_bar(
    model, graph, features, node_indices=None,
    baseline=None, steps=50, use_abs=True
):
    """
    Computes relevance scores using Integrated Gradients for selected nodes.

    Args:
        model: Trained GNN model.
        graph: DGL graph.
        features: [N, D] tensor of input features.
        node_indices: Node indices to compute relevance for. If None, auto-select based on sigmoid output > 0.5.
        baseline: Baseline input tensor [N, D] to start integration from. If None, uses zeros.
        steps: Number of steps for the IG path.
        use_abs: Whether to return absolute attributions (recommended).

    Returns:
        relevance_scores: [N, D] tensor with non-zero rows for selected nodes.
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    if node_indices is None:
        with torch.no_grad():
            probs = torch.sigmoid(model(graph, features)).squeeze()
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

    relevance_scores = torch.zeros_like(features)

    for idx in node_indices:
        total_grad = torch.zeros_like(features[idx])
        for alpha in torch.linspace(0, 1, steps):
            interpolated = baseline[idx] + alpha * (features[idx] - baseline[idx])
            input_vec = features.clone().detach()
            input_vec[idx] = interpolated
            input_vec.requires_grad_(True)

            logits = model(graph, input_vec)
            prob = torch.sigmoid(logits.squeeze())[idx]

            model.zero_grad()
            if input_vec.grad is not None:
                input_vec.grad.zero_()
            prob.backward(retain_graph=True)

            grad = input_vec.grad[idx]
            if grad is not None:
                total_grad += grad
                # print('idx========\n', idx)
                # print('total_grad---------------------------\n', total_grad)
            else:
                print(f"‚ö†Ô∏è Gradient was None at alpha={alpha.item():.2f}")

        avg_grad = total_grad / steps
        ig = (features[idx] - baseline[idx]) * avg_grad
        relevance_scores[idx] = ig.abs() if use_abs else ig

    return relevance_scores

def plot_cluster_sizes_and_enrichment_(graph, cluster_key, enrichment_dict=None, title_suffix=""):
    import matplotlib.pyplot as plt

    cluster_counts = pd.Series(nx.get_node_attributes(graph, cluster_key)).value_counts().sort_index()
    fig, ax = plt.subplots(figsize=(10, 5))
    cluster_counts.plot(kind="bar", ax=ax)
    ax.set_xlabel("Cluster ID")
    ax.set_ylabel("Number of Genes")
    ax.set_title(f"Cluster Sizes {title_suffix}")
    plt.tight_layout()
    plt.close()

    if enrichment_dict:
        enriched_counts = pd.Series({k: len(v) for k, v in enrichment_dict.items()})
        fig, ax = plt.subplots(figsize=(10, 5))
        enriched_counts.plot(kind="bar", ax=ax, color='orange')
        ax.set_xlabel("Cluster ID")
        ax.set_ylabel("Enriched Terms Count")
        ax.set_title(f"Enrichment Counts per Cluster {title_suffix}")
        plt.tight_layout()
        plt.close()

def plot_cluster_sizes_and_enrichment(graph, cluster_key, enrichment_dict=None, title_suffix=""):
    import matplotlib.pyplot as plt
    import pandas as pd
    import networkx as nx

    cluster_attrs = nx.get_node_attributes(graph, cluster_key)
    missing = set(graph.nodes()) - set(cluster_attrs.keys())
    if missing:
        print(f"‚ö†Ô∏è Warning: {len(missing)} nodes missing '{cluster_key}' attribute.")

    cluster_counts = pd.Series(list(cluster_attrs.values())).value_counts().sort_index()
    fig, ax = plt.subplots(figsize=(10, 5))
    cluster_counts.plot(kind="bar", ax=ax)
    ax.set_xlabel("Cluster ID")
    ax.set_ylabel("Number of Genes")
    ax.set_title(f"Cluster Sizes {title_suffix}")
    plt.tight_layout()
    plt.close()

    if enrichment_dict:
        enriched_counts = pd.Series({k: len(v) for k, v in enrichment_dict.items()})
        fig, ax = plt.subplots(figsize=(10, 5))
        enriched_counts.plot(kind="bar", ax=ax, color='orange')
        ax.set_xlabel("Cluster ID")
        ax.set_ylabel("Enriched Terms Count")
        ax.set_title(f"Enrichment Counts per Cluster {title_suffix}")
        plt.tight_layout()
        plt.close()

def plot_relevance_boxplots(relevance_scores, graph, cluster_key, title_suffix=""):
    df = pd.DataFrame(relevance_scores, index=list(graph.nodes))
    df['cluster'] = pd.Series(nx.get_node_attributes(graph, cluster_key))
    df['relevance'] = df.drop(columns='cluster').max(axis=1)
    
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, x='cluster', y='relevance')
    plt.title(f"Relevance Score Distribution per Cluster {title_suffix}")
    plt.xlabel("Cluster")
    plt.ylabel("Max Relevance Score")
    plt.tight_layout()
    plt.close()

def plot_top_genes_heatmap_(relevance_scores, graph, cluster_key, top_k=10, title_suffix=""):
    df = pd.DataFrame(relevance_scores, index=list(graph.nodes))
    df['cluster'] = pd.Series(nx.get_node_attributes(graph, cluster_key))

    top_genes = []
    for clust_id, group in df.groupby("cluster"):
        top = group.drop(columns='cluster').mean(axis=1).nlargest(top_k).index.tolist()
        top_genes.extend(top)
    
    df_top = df.loc[top_genes].drop(columns='cluster')
    sns.clustermap(df_top, cmap="viridis", figsize=(12, 10))
    plt.suptitle(f"Top {top_k} Genes per Cluster {title_suffix}", y=1.02)
    plt.close()

def plot_relevance_tsne_umap(relevance_scores, graph, cluster_key, method='tsne', title_suffix=""):

    X = np.array(relevance_scores)
    labels = pd.Series(nx.get_node_attributes(graph, cluster_key)).reindex(graph.nodes()).values

    reducer = TSNE(n_components=2, random_state=42) if method == 'tsne' else umap.UMAP(n_components=2, random_state=42)
    X_reduced = reducer.fit_transform(X)

    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=labels, palette='tab10', s=30)
    plt.title(f"{method.upper()} of Relevance Scores {title_suffix}")
    plt.xlabel("Dim 1")
    plt.ylabel("Dim 2")
    plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.close()

def plot_relevance_tsne_umap_grey(relevance_scores, graph, cluster_key, method='tsne', title_suffix=""):
    X = np.array(relevance_scores)
    labels = pd.Series(nx.get_node_attributes(graph, cluster_key)).reindex(graph.nodes()).values

    reducer = TSNE(n_components=2, random_state=42) if method == 'tsne' else umap.UMAP(n_components=2, random_state=42)
    X_reduced = reducer.fit_transform(X)

    # Map labels to colors using CLUSTER_COLORS
    palette = [CLUSTER_COLORS.get(label, "#777777") for label in sorted(set(labels))]

    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=labels, palette=palette, s=30)
    plt.title(f"{method.upper()} of Relevance Scores {title_suffix}")
    plt.xlabel("Dim 1")
    plt.ylabel("Dim 2")
    plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.close()

def run_enrichment_per_cluster(graph, cluster_key, gene_name_map=None, background_genes=None):
    from gseapy import enrichr

    cluster_genes = defaultdict(list)
    for node, clust in nx.get_node_attributes(graph, cluster_key).items():
        gene = gene_name_map[node] if gene_name_map else node
        cluster_genes[clust].append(gene)

    enrichment_results = {}
    for clust_id, genes in cluster_genes.items():
        enr = enrichr(gene_list=genes,
                      gene_sets='GO_Biological_Process_2021',
                      background=background_genes,
                      outdir=None, verbose=False)
        enrichment_results[clust_id] = enr.results
    return enrichment_results

def compute_relevance_scores_no_progress_bar(model, graph, features, node_indices=None, method="saliency", use_abs=True, baseline=None, steps=50):
    """
    Computes relevance scores for selected nodes using either saliency (gradients) or integrated gradients (IG).

    Args:
        model: Trained GNN model
        graph: DGL graph
        features: Input node features (torch.Tensor or np.ndarray)
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select using probs > 0.0
        method: "saliency" or "integrated_gradients"
        use_abs: Whether to use absolute values of gradients
        baseline: Baseline input for IG (default: zero vector)
        steps: Number of steps for IG approximation

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] (0s for nodes not analyzed)
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            if method == "saliency":
                probs[idx].backward(retain_graph=(i != len(node_indices) - 1))
                grads = features.grad[idx]
                relevance_scores[idx] = grads.abs().detach() if use_abs else grads.detach()

            elif method == "integrated_gradients":
                # Define baseline
                if baseline is None:
                    baseline_input = torch.zeros_like(features)
                else:
                    baseline_input = baseline.clone().detach()

                # Generate scaled inputs
                scaled_inputs = [baseline_input + (float(alpha) / steps) * (features - baseline_input) for alpha in range(1, steps + 1)]
                total_grad = torch.zeros_like(features)

                for input_step in scaled_inputs:
                    input_step.requires_grad_()
                    out = model(graph, input_step)
                    prob = torch.sigmoid(out.squeeze())[idx]

                    model.zero_grad()
                    if input_step.grad is not None:
                        input_step.grad.zero_()

                    prob.backward(retain_graph=True)
                    grad = input_step.grad#.detach()
                    total_grad += grad

                avg_grad = total_grad / steps
                ig = (features - baseline_input) * avg_grad
                relevance_scores[idx] = ig[idx].abs() if use_abs else ig[idx]

            else:
                raise ValueError(f"Unknown method: {method}. Use 'saliency' or 'integrated_gradients'.")

    return relevance_scores

def compute_relevance_scores(model, graph, features, node_indices=None, method="saliency", use_abs=True, baseline=None, steps=50):
    """
    Computes relevance scores for selected nodes using either saliency (gradients) or integrated gradients (IG).

    Args:
        model: Trained GNN model
        graph: DGL graph
        features: Input node features (torch.Tensor or np.ndarray)
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select using probs > 0.0
        method: "saliency" or "integrated_gradients"
        use_abs: Whether to use absolute values of gradients
        baseline: Baseline input for IG (default: zero vector)
        steps: Number of steps for IG approximation

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] (0s for nodes not analyzed)
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(tqdm(node_indices, desc=f"Computing relevance ({method})", leave=True)):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            if method == "saliency":
                probs[idx].backward(retain_graph=(i != len(node_indices) - 1))
                grads = features.grad[idx]
                relevance_scores[idx] = grads.abs().detach() if use_abs else grads.detach()

            elif method == "integrated_gradients":
                # Define baseline
                if baseline is None:
                    baseline_input = torch.zeros_like(features)
                else:
                    baseline_input = baseline.clone().detach()

                # Generate scaled inputs
                total_grad = torch.zeros_like(features)
                for alpha in range(1, steps + 1):
                    scaled_input = baseline_input + (alpha / steps) * (features - baseline_input)
                    scaled_input.requires_grad_()

                    out = model(graph, scaled_input)
                    prob = torch.sigmoid(out.squeeze())[idx]

                    model.zero_grad()
                    if scaled_input.grad is not None:
                        scaled_input.grad.zero_()

                    prob.backward(retain_graph=True)
                    grad = scaled_input.grad
                    total_grad += grad

                avg_grad = total_grad / steps
                ig = (features - baseline_input) * avg_grad
                relevance_scores[idx] = ig[idx].abs() if use_abs else ig[idx]

            else:
                raise ValueError(f"Unknown method: {method}. Use 'saliency' or 'integrated_gradients'.")

    return relevance_scores

def compute_relevance_scores_integrated_gradients_no_captum(
    model, graph, features, node_indices=None,
    baseline=None, steps=50, use_abs=True
):
    """
    Computes relevance scores using Integrated Gradients for selected nodes.

    Args:
        model: Trained GNN model.
        graph: DGL graph.
        features: [N, D] tensor of input features.
        node_indices: Node indices to compute relevance for. If None, auto-select based on sigmoid output > 0.5.
        baseline: Baseline input tensor [N, D] to start integration from. If None, uses zeros.
        steps: Number of steps for the IG path.
        use_abs: Whether to return absolute attributions (recommended).

    Returns:
        relevance_scores: [N, D] tensor with non-zero rows for selected nodes.
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    if node_indices is None:
        with torch.no_grad():
            probs = torch.sigmoid(model(graph, features)).squeeze()
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

    relevance_scores = torch.zeros_like(features)

    for idx in tqdm(node_indices, desc="Computing IG relevance"):
        total_grad = torch.zeros_like(features[idx])

        for alpha in torch.linspace(0, 1, steps):
            interpolated = baseline[idx] + alpha * (features[idx] - baseline[idx])
            input_vec = features.clone().detach()
            input_vec[idx] = interpolated
            input_vec.requires_grad_(True)

            logits = model(graph, input_vec)
            prob = torch.sigmoid(logits.squeeze())[idx]

            model.zero_grad()
            if input_vec.grad is not None:
                input_vec.grad.zero_()
            prob.backward(retain_graph=True)

            grad = input_vec.grad[idx]
            if grad is not None:
                total_grad += grad
            else:
                print(f"‚ö†Ô∏è Gradient was None at alpha={alpha.item():.2f}")

        avg_grad = total_grad / steps
        ig = (features[idx] - baseline[idx]) * avg_grad
        relevance_scores[idx] = ig.abs() if use_abs else ig

    return relevance_scores

def compute_relevance_scores_integrated_gradients(
    model, graph, features, node_indices=None,
    baseline=None, steps=50, use_abs=True
):
    """
    Computes relevance scores using Integrated Gradients via Captum for selected nodes.

    Args:
        model: Trained GNN model.
        graph: DGL graph.
        features: [N, D] input tensor.
        node_indices: Nodes to compute relevance for (auto-select if None).
        baseline: [N, D] baseline tensor. Default = zero.
        steps: Number of IG steps.
        use_abs: Return abs(relevance) if True.

    Returns:
        relevance_scores: [N, D] tensor, relevance per node; 0s for unselected nodes.
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    if node_indices is None:
        with torch.no_grad():
            probs = torch.sigmoid(model(graph, features)).squeeze()
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

    relevance_scores = torch.zeros_like(features)

    for idx in tqdm(node_indices.tolist(), desc="Computing IG relevance (Captum)"):

        # Only input for IG is [N, D] features
        def model_forward(input_feat):
            out = model(graph, input_feat)
            return torch.sigmoid(out).squeeze()[idx]  # scalar output for node idx

        ig = IntegratedGradients(model_forward)

        input_feat = features.clone().detach().requires_grad_(True)
        baseline_feat = baseline.clone().detach()

        attr = ig.attribute(
            inputs=input_feat,
            baselines=baseline_feat,
            n_steps=steps
        )

        relevance_scores[idx] = attr[idx].abs() if use_abs else attr[idx]

    return relevance_scores

def compute_entropy(labels):
    counts = np.array(list(Counter(labels).values()))
    probs = counts / counts.sum()
    return entropy(probs, base=2)

def compute_gini(labels):
    counts = np.array(list(Counter(labels).values()))
    sorted_counts = np.sort(counts)
    n = len(counts)
    cum_counts = np.cumsum(sorted_counts)
    gini = (n + 1 - 2 * np.sum(cum_counts) / cum_counts[-1]) / n
    return gini

def compute_silhouette(relevance_matrix, row_labels):
    try:
        return silhouette_score(relevance_matrix, row_labels)
    except:
        return float("nan")

#def evaluate_model_biclustering(model_type, graph, features, predicted_cancer_genes, top_gene_indices, node_names, best_k, device):
def evaluate_model_biclustering_(model, graph, relevance_matrix_bio, predicted_cancer_genes, top_gene_indices, node_names, best_k, device):
    
    # Load model
    #model = choose_model(model_type, in_feats=features.shape[1], hidden_feats=64, out_feats=1).to(device)
    #model.load_state_dict(torch.load(f'models/{model_type}.pt'))  # load trained model checkpoint
    model.eval()

    # Compute relevance scores
    '''relevance_scores, _ = compute_relevance_scores_integrated_gradients(
        model=model,
        graph=graph,
        features=features,
        node_indices=top_gene_indices,
        use_abs=True
    )'''
    '''relevance_scores = compute_relevance_scores(
        model=model,
        graph=graph,
        features=features,
        node_indices=top_gene_indices,
        use_abs=True
    )'''
    
    relevance_scores_bio = relevance_scores[:, :1024]
    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    relevance_scores_topk_bio = relevance_scores_bio[topk_node_indices_tensor]
    summary_bio_topk = extract_summary_features_np_bio(relevance_scores_topk_bio.detach().cpu().numpy())

    print(summary_bio_topk.shape)
    relevance_matrix_bio = summary_bio_topk
    
    # Run spectral biclustering
    graph_bio, row_labels_bio, col_labels_bio, _, _ = apply_full_spectral_biclustering_bio(
        graph=graph,
        summary_bio_features=relevance_matrix_bio, 
        node_names=node_names,
        predicted_cancer_genes=predicted_cancer_genes,
        n_clusters=best_k,
        save_path=None,
        save_row_labels_path=None,
        save_total_genes_per_cluster_path=None,
        save_predicted_counts_path=None,
        output_path_genes_clusters=None,
        output_path_heatmap=None,
        topk_node_indices=top_gene_indices
    )

    # Assign cluster labels
    full_row_labels_bio = torch.full((graph_bio.num_nodes(),), -1, dtype=torch.long)
    full_row_labels_bio[top_gene_indices] = torch.tensor(row_labels_bio, dtype=torch.long)
    graph_bio.ndata['cluster_bio'] = full_row_labels_bio

    # Compute metrics on clustered top genes
    valid_mask = full_row_labels_bio[top_gene_indices] != -1
    valid_clusters = full_row_labels_bio[top_gene_indices][valid_mask].cpu().numpy()
    valid_features = relevance_matrix_bio[top_gene_indices][valid_mask]

    entropy_score = compute_entropy(valid_clusters)
    gini_score = compute_gini(valid_clusters)
    silhouette = compute_silhouette(valid_features.cpu().numpy(), valid_clusters)

    return {
        "Model": model_type,
        "Entropy": entropy_score,
        "Gini": gini_score,
        "Silhouette": silhouette
    }

def apply_full_spectral_biclustering_bio_no_tsne(
        graph, summary_bio_features, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=20  # New param to control how many seeds to try
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]
    ##filtered_features = summary_bio_features[high_saliency_indices]

    #relevance_matrix = filtered_features

    # === Use balanced biclustering
    bicluster = best_balanced_biclustering(summary_bio_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional: Heatmaps and t-SNE
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo_no_tsne(
        graph, summary_topo_features, node_names,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on topo features with {n_clusters} clusters...")
    assert summary_topo_features.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features.shape[1]}"

    # === Normalize topo features
    summary_topo_features = StandardScaler().fit_transform(summary_topo_features)

    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)
    
    # === Save original (unclustered) heatmap before biclustering
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_topo_biclustering_heatmap_no_csv(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    
    """
    Plots a spectral biclustering heatmap for topological embeddings (1024‚Äì2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # üîπ Extract 64D summary of topological features
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # üîπ Create topo feature names (01‚Äì64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # üîπ Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # üîπ Sort by cluster ‚Üí then by gene-wise relevance within cluster
    sorted_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        sorted_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[sorted_indices]
    sorted_clusters = row_labels[sorted_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in sorted_indices]

    # üîπ Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # üîπ Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # üîπ Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # üîπ Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # üîπ Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # üîπ Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # üîπ Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # üîπ Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved spectral clustering heatmap to {output_path}")

    # üîπ Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_bio_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    cancer_names,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
    ):  
    
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    # üîπ Normalize relevance scores row-wise (per gene)
    '''
    relevance_scores = (relevance_scores - relevance_scores.min(axis=1, keepdims=True)) / \
                    (relevance_scores.max(axis=1, keepdims=True) - relevance_scores.min(axis=1, keepdims=True) + 1e-6)'''
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    ##relevance_scores = StandardScaler().fit_transform(relevance_scores)*20
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())*20
    
    # üîπ Set default omics colors
    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',      # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    # üîπ Cancer types and omics order
    
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]

    omics_order = ['cna', 'ge', 'meth', 'mf']

    # üîπ Build feature names internally
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    sorted_indices = np.argsort(row_labels)
    relevance_scores = relevance_scores[sorted_indices]
    row_labels = row_labels[sorted_indices]

    # üîπ Then sort within each cluster (optional: by LRP sum descending)
    new_order = []
    unique_clusters = np.unique(row_labels)
    for cluster in unique_clusters:
        cluster_indices = np.where(row_labels == cluster)[0]
        
        # Sort within this cluster, for example by total relevance score (descending)
        cluster_relevance_sums = relevance_scores[cluster_indices].sum(axis=1)
        sorted_within = cluster_indices[np.argsort(-cluster_relevance_sums)]  # Descending
        new_order.extend(sorted_within)

    # üîπ Apply new sorted order
    sorted_scores = relevance_scores[new_order]
    sorted_clusters = row_labels[new_order]


    # üîπ Reorder columns using col_labels (if provided)
    if col_labels is not None:
        sorted_order = np.argsort(col_labels)
        sorted_scores = sorted_scores[:, sorted_order]
        feature_names = [feature_names[i] for i in sorted_order]

            
    # üîπ Feature color mapping
    feature_colors = []
    for i, name in enumerate(feature_names):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")

    # üîπ Setup figure
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])
    ax_bar.axis("off")  # Hide regular axis stuff
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
        
        
    # üîπ Compute cluster boundaries
    unique_clusters, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]


    # üîπ Compute mean relevance per omics group
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = sorted_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    # üîπ Sort omics groups by descending mean relevance
    sorted_omics = sorted(omics_means.keys(), key=lambda x: omics_means[x], reverse=True)

    # üîπ Reorder columns: first by omics group, then within-group by mean column relevance
    sorted_col_indices = []
    sorted_feature_names = []
    feature_colors = []

    for omics in sorted_omics:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))

        # Get mean relevance per feature in this omics group
        col_means = sorted_scores[:, group_indices].mean(axis=0)
        group_sorted_indices = [group_indices[i] for i in np.argsort(-col_means)]  # descending

        sorted_col_indices.extend(group_sorted_indices)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted_indices])
        feature_colors.extend([omics_colors[omics]] * len(group_sorted_indices))

    # üîπ Apply sorted column order
    sorted_scores = sorted_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names

    # üîπ Compute per-feature mean relevance (column-wise)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    # üîπ Plot one bar per feature (aligned to heatmap)
    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,  # center over column
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val  # dimmer for low values
        )


    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )
    
    # üîπ Use dynamic vmax for contrast
    vmin = 0#np.percentile(sorted_scores, 1)
    vmax = np.percentile(sorted_scores, 99)

    # üîπ Plot heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )
    
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Add cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ Color x-tick labels

    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    #ax.tick_params(axis='x', bottom=True, labelbottom=True)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Omics + LRP curve legend
    '''ax_legend.axis("off")

    # Create omics patches
    omics_patches = [
        Patch(color=color, label=omics.upper())
        for omics, color in omics_colors.items()
    ]

    # Create LRP patch
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')

    # Combine and render
    ax_legend.legend(
        handles=omics_patches + [lrp_patch],
        loc="center",
        ncol=len(omics_patches) + 1,
        frameon=False,
        fontsize=18,
        handleheight=1.5,
        handlelength=3
    )'''


    # üîπ LRP curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Compute mean relevance per omics group (column-wise)
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = sorted_scores[:, start:end+1]  # inclusive end
        omics_means[omics] = group_scores.mean()

    # Map omics to x-center positions
    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    # Normalize mean values for alpha mapping
    mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)  # prevent zero division

    # Plot bars with darkness mapped to mean relevance
    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,  # small bar height
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]  # range alpha from 0.3 (light) to 1.0 (dark)
        )


    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Create legend patches
    '''lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')

    # Optional: Also create cluster color legend (if not already somewhere else)
    cluster_patches = [
        Patch(facecolor=color, label=f'Cluster {cid}')
        for cid, color in CLUSTER_COLORS.items()
    ]

    # üîπ Place legend near the X-axis label area (or wherever appropriate)
    legend_ax = fig.add_subplot(gs[11, 2:48])
    legend_ax.axis('off')  # Hide the axis

    legend_ax.legend(
        handles=[lrp_patch],  # add cluster_patches + [lrp_patch] if you want both
        loc='center',
        ncol=1,
        frameon=False,
        fontsize=12
    )'''

    # üîπ Layout and save
    #plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout()
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved spectral clustering heatmap to {output_path}")

    # üîπ Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

    return pd.DataFrame(relevance_scores, index=gene_names, columns=feature_names)

def plot_bio_biclustering_heatmap__(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Step 1: Sort rows by cluster labels
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    # Step 2: Column sorting
    feature_avgs = sorted_scores.mean(axis=0)

    # Sort omics groups by group average
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean

    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    # Sort features within each omics group
    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    # Apply column sort
    sorted_scores = sorted_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Grid layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars (new positions from sorted order)
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_not_clustermap(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_clusters_sorted(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def build_contribution_graph(gene_names, contribution_matrix, threshold=0.14):
    """
    Build a directed graph where an edge from A to B exists if A contributes to B
    with a score above the threshold.

    Parameters:
        gene_names: list of gene names corresponding to matrix indices.
        contribution_matrix: numpy array [num_genes x num_genes] of importance scores.
        threshold: float, minimum contribution score to keep an edge.

    Returns:
        G: networkx.DiGraph representing the reduced contribution graph.
    """
    G = nx.DiGraph()
    num_genes = len(gene_names)

    for i in range(num_genes):
        for j in range(num_genes):
            if i != j:
                score = contribution_matrix[i, j]
                if score >= threshold:
                    G.add_edge(gene_names[i], gene_names[j], weight=score)

    return G

def find_strongly_connected_modules(G, min_size=5):
    """
    Detect strongly connected components (SCCs) in a directed graph using Tarjan's algorithm.

    Parameters:
        G: networkx.DiGraph
        min_size: int, minimum number of nodes per component to keep

    Returns:
        modules: list of sets, each representing a SCC with at least min_size nodes
    """
    sccs = list(nx.strongly_connected_components(G))
    filtered_sccs = [scc for scc in sccs if len(scc) >= min_size]
    return filtered_sccs

def plot_predicted_gene_embeddings_by_cluster(graph, node_names, scores, output_path, score_threshold=0.5):
    cluster_ids = graph.ndata['cluster_bio'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores.cpu().numpy()

    predicted_mask = scores >= score_threshold
    predicted_indices = np.where(predicted_mask)[0]

    if len(predicted_indices) == 0:
        print("‚ö†Ô∏è No predicted genes above threshold.")
        return

    predicted_embeddings = embeddings[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]

    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(predicted_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        coords = tsne_coords[cluster_mask]
        label = f"Cluster {c}"
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            label=label,
            alpha=0.8,
            edgecolors='k',
            s=60
        )

    plt.title("t-SNE of Predicted Genes Colored by Cluster", fontsize=16)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.legend(title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Clustered t-SNE plot of predicted genes saved to:\n{output_path}")

def plot_top_predicted_genes_tsne(graph, node_names, scores, output_path, top_k=1000):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    # Get top K predicted genes
    top_indices = np.argsort(scores)[-top_k:]
    top_embeddings = embeddings[top_indices]
    top_clusters = cluster_ids[top_indices]
    top_scores = scores[top_indices]
    top_names = [node_names[i] for i in top_indices]

    # t-SNE projection
    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(top_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    rcParams['pdf.fonttype'] = 42  # prevent font issues in vector graphics

    for c in np.unique(top_clusters):
        mask = top_clusters == c
        coords = tsne_coords[mask]
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            edgecolors='k',
            s=60,
            alpha=0.8
        )

        # Top 1 in this cluster (within top_k)
        # cluster_scores = top_scores[mask]
        # if cluster_scores.size > 0:
        #     top_idx_in_cluster = np.argmax(cluster_scores)
        #     name = np.array(top_names)[mask][top_idx_in_cluster]
        #     x, y = coords[top_idx_in_cluster]
        #     # Highlight the top 1 with a yellow circle (half the size of the original dot)
        #     plt.scatter(x, y, s=60, edgecolors='yellow', alpha=0.6, linewidth=1, marker='o', color='red')
        #     # Change label text color to red
        #     plt.text(x, y, name, fontsize=9, fontweight='bold', ha='center', va='center', color='black')

    plt.title("t-SNE of Top 1000 Predicted Genes by Cluster", fontsize=16)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    
    # Remove legend
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Top predicted gene t-SNE plot saved to:\n{output_path}")
 
def plot_top_predicted_genes_umap(graph, node_names, scores, output_path, top_k=1000):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores  # Assuming already on CPU or NumPy

    # Get top K predicted genes
    top_indices = np.argsort(scores)[-top_k:]
    top_embeddings = embeddings[top_indices]
    top_clusters = cluster_ids[top_indices]
    top_scores = scores[top_indices]
    top_names = [node_names[i] for i in top_indices]

    # UMAP projection
    reducer = umap.UMAP(n_components=2, random_state=42)
    umap_coords = reducer.fit_transform(top_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    rcParams['pdf.fonttype'] = 42  # Prevent font issues in vector graphics

    for c in np.unique(top_clusters):
        mask = top_clusters == c
        coords = umap_coords[mask]
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            edgecolors='k',
            s=60,
            alpha=0.8
        )

        # Top 1 in this cluster (within top_k)
        # cluster_scores = top_scores[mask]
        # if cluster_scores.size > 0:
        #     top_idx_in_cluster = np.argmax(cluster_scores)
        #     name = np.array(top_names)[mask][top_idx_in_cluster]
        #     x, y = coords[top_idx_in_cluster]
        #     plt.scatter(x, y, s=60, edgecolors='yellow', alpha=0.6, linewidth=1, marker='o', color='red')
        #     plt.text(x, y, name, fontsize=9, fontweight='bold', ha='center', va='center', color='black')

    plt.title("UMAP of Top 1000 Predicted Genes by Cluster", fontsize=16)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Top predicted gene UMAP plot saved to:\n{output_path}")

def plot_tsne_predicted_genes(graph, node_names, scores, output_path, args):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    predicted_mask = scores >= args.score_threshold
    predicted_indices = np.where(predicted_mask)[0]
    predicted_scores = scores[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]
    predicted_embeddings = embeddings[predicted_indices]

    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(predicted_embeddings)

    # Gather top 2 genes per cluster
    top_genes = []
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) == 0:
            continue
        top_indices = cluster_indices[np.argsort(predicted_scores[cluster_indices])[-2:]]  # top 2
        for idx in top_indices:
            top_genes.append((predicted_indices[idx], tsne_coords[idx], c))

    # Plot
    plt.figure(figsize=(10, 7))
    for idx, (node_idx, coord, cluster_id) in enumerate(top_genes):
        color = CLUSTER_COLORS.get(cluster_id, "#333333")
        plt.scatter(coord[0], coord[1], color=color, s=120, edgecolor='k')
        plt.text(coord[0]+1.5, coord[1], node_names[node_idx], fontsize=9, color=color)

    # Legend
    unique_predicted_clusters = np.unique(predicted_clusters)
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=f"Cluster {c}", markersize=10)
        for c, color in CLUSTER_COLORS.items()
        if c in unique_predicted_clusters
    ]
    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.title("t-SNE of Top 2 Predicted Genes per Cluster")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

def plot_umap_predicted_genes_top5(graph, node_names, scores, output_path, args):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores

    predicted_mask = scores >= args.score_threshold
    predicted_indices = np.where(predicted_mask)[0]
    predicted_scores = scores[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]
    predicted_embeddings = embeddings[predicted_indices]

    # UMAP projection
    reducer = umap.UMAP(n_components=2, random_state=42)
    umap_coords = reducer.fit_transform(predicted_embeddings)

    # Gather top 5 genes per cluster
    top_genes = []
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) == 0:
            continue
        top_indices = cluster_indices[np.argsort(predicted_scores[cluster_indices])[-5:]]  # top 5
        for idx in top_indices:
            top_genes.append((predicted_indices[idx], umap_coords[idx], c))

    # Plot
    plt.figure(figsize=(12, 8))
    for idx, (node_idx, coord, cluster_id) in enumerate(top_genes):
        color = CLUSTER_COLORS.get(cluster_id, "#333333")
        plt.scatter(coord[0], coord[1], color=color, s=120, edgecolor='k')
        plt.text(coord[0] + 1.5, coord[1], node_names[node_idx], fontsize=9, color=color)

    # Legend
    unique_predicted_clusters = np.unique(predicted_clusters)
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=f"Cluster {c}", markersize=10)
        for c, color in CLUSTER_COLORS.items()
        if c in unique_predicted_clusters
    ]
    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')

    # Remove frame and axes
    plt.axis('off')

    plt.title("UMAP of Top 5 Predicted Genes per Cluster")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ UMAP plot of top 5 predicted genes per cluster saved to:\n{output_path}")

def apply_spectral_biclustering_ori(graph, embeddings, node_names, predicted_cancer_genes, n_clusters, save_path, save_row_labels_path, save_total_genes_per_cluster_path, save_predicted_counts_path, output_path_genes_clusters):
    print(f"Running Spectral Biclustering with {n_clusters} row clusters...")

    node_features = embeddings.cpu().numpy()

    # Run Spectral Biclustering
    ##bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    n_rows, n_cols = node_features.shape
    bicluster = SpectralBiclustering(
        n_clusters=(min(n_clusters, n_rows), min(n_clusters, n_cols)),
        method='log',
        random_state=42
    )

    bicluster.fit(node_features)

    # Assign cluster labels to graph
    row_labels = bicluster.row_labels_
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long)

    # Original graph-saving method (kept unchanged)
    print("Spectral Biclustering complete. Cluster labels assigned to graph.")
    print(f"Saving graph to {save_path}")
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster': graph.ndata['cluster']
    }, save_path)

    # Save cluster labels separately
    save_row_labels(row_labels, save_row_labels_path)

    # Count total genes per cluster
    total_genes_per_cluster = {i: np.sum(row_labels == i) for i in range(n_clusters)}
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    # Count predicted cancer genes per cluster
    pred_counts = {i: 0 for i in range(n_clusters)}
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]

    for idx in predicted_indices:
        if 0 <= idx < len(row_labels):
            cluster_id = row_labels[idx]
            pred_counts[cluster_id] += 1
        else:
            print(f"Skipping invalid predicted gene index: {idx}")

    # Save predicted counts separately
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Visualize with t-SNE
    if node_features.shape[0] < 2 or node_features.shape[1] < 2:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")
    else:
        tsne = TSNE(n_components=2, perplexity=min(30, len(node_features) - 1), random_state=42)
        reduced_embeddings = tsne.fit_transform(node_features)

        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)

        for idx in predicted_indices:
            x, y = reduced_embeddings[idx]
            plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("Spectral Biclustering of Genes with Predicted Cancer Markers")
        plt.savefig(output_path_genes_clusters, bbox_inches="tight")
        plt.close()
        print(f"Cluster visualization saved to {output_path_genes_clusters}")


    return graph, row_labels, total_genes_per_cluster, pred_counts

def apply_spectral_biclustering_adapted(
    graph,
    embeddings,
    node_names,
    predicted_cancer_genes,
    n_clusters,
    save_path,
    save_row_labels_path,
    save_total_genes_per_cluster_path,
    save_predicted_counts_path,
    output_path_genes_clusters,
    n_trials=20
):
    print(f"Running Spectral Biclustering with {n_clusters} row clusters...")

    node_features = embeddings.cpu().numpy()
    n_rows, n_cols = node_features.shape

    # Balanced biclustering
    bicluster = best_balanced_biclustering(node_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels  # Invert for consistency

    # Assign cluster labels to graph
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long)

    # Save graph
    print("Spectral Biclustering complete. Cluster labels assigned to graph.")
    print(f"Saving graph to {save_path}")
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster': graph.ndata['cluster']
    }, save_path)

    # Save cluster labels
    save_row_labels(row_labels, save_row_labels_path)

    # Count total genes per cluster
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    # Count predicted cancer genes per cluster
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Visualize with t-SNE
    if node_features.shape[0] < 2 or node_features.shape[1] < 2:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")
    else:
        tsne = TSNE(n_components=2, perplexity=min(30, len(node_features) - 1), random_state=42)
        reduced_embeddings = tsne.fit_transform(node_features)

        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)

        for idx in predicted_indices:
            x, y = reduced_embeddings[idx]
            plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("Spectral Biclustering of Genes with Predicted Cancer Markers")
        plt.savefig(output_path_genes_clusters, bbox_inches="tight")
        plt.close()
        print(f"Cluster visualization saved to {output_path_genes_clusters}")

    return graph, row_labels, total_genes_per_cluster, pred_counts

def apply_spectral_biclustering(
    graph, embeddings, node_names, predicted_cancer_genes, n_clusters,
    save_path, save_row_labels_path, save_total_genes_per_cluster_path,
    save_predicted_counts_path, output_path_genes_clusters
):

    print(f"Running Spectral Biclustering with {n_clusters} row clusters...")

    node_features = embeddings#.cpu().numpy()
    n_rows, n_cols = node_features.shape

    bicluster = SpectralBiclustering(
        n_clusters=(min(n_clusters, n_rows), min(n_clusters, n_cols)),
        method='log',
        random_state=42
    )
    bicluster.fit(node_features)

    # Assign cluster labels to graph
    row_labels = bicluster.row_labels_
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long)

    print("Spectral Biclustering complete. Cluster labels assigned to graph.")
    print(f"Saving graph to {save_path}")
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster': graph.ndata['cluster']
    }, save_path)

    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = {i: np.sum(row_labels == i) for i in range(n_clusters)}
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts = {i: 0 for i in range(n_clusters)}
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]

    for idx in predicted_indices:
        if 0 <= idx < len(row_labels):
            cluster_id = row_labels[idx]
            pred_counts[cluster_id] += 1
        else:
            print(f"Skipping invalid predicted gene index: {idx}")

    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # t-SNE visualization of nodes (rows)
    if n_rows < 2 or n_cols < 2:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")
    else:
        tsne = TSNE(n_components=2, perplexity=min(30, n_rows - 1), random_state=42)
        reduced_embeddings = tsne.fit_transform(node_features)

        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)

        for idx in predicted_indices:
            x, y = reduced_embeddings[idx]
            plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("t-SNE of Gene Embeddings (Row Clusters)")
        plt.savefig(output_path_genes_clusters, bbox_inches="tight")
        plt.close()
        print(f"Row-cluster t-SNE saved to {output_path_genes_clusters}")

        # t-SNE visualization of features (columns)
        transposed_features = node_features.T  # shape: (n_features, n_nodes)
        feature_labels = bicluster.column_labels_
        n_feature_clusters = len(np.unique(feature_labels))

        tsne_feat = TSNE(n_components=2, perplexity=min(30, len(transposed_features) - 1), random_state=42)
        reduced_feats = tsne_feat.fit_transform(transposed_features)

        plt.figure(figsize=(12, 10))
        for fid in range(n_feature_clusters):
            idx = np.where(feature_labels == fid)[0]
            plt.scatter(reduced_feats[idx, 0], reduced_feats[idx, 1],
                        label=f"Feature Cluster {fid}",
                        edgecolor='k', s=100, alpha=0.8)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("t-SNE of Feature Embeddings (Column Clusters)")
        plt.legend()
        output_path_feat_clusters = output_path_genes_clusters.replace(".png", "_features.png")
        plt.savefig(output_path_feat_clusters, bbox_inches="tight")
        plt.close()
        print(f"Column-cluster t-SNE saved to {output_path_feat_clusters}")

    return graph, row_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_tsne(
        graph, summary_bio_features, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=20
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]

    # Run balanced spectral biclustering
    bicluster = best_balanced_biclustering(summary_bio_features, n_clusters, n_trials)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels  # optional reverse

    # Assign cluster labels to nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === t-SNE visualization with column clustering as color-coded feature summary ===
    if summary_bio_features.shape[0] >= 2 and summary_bio_features.shape[1] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features) - 1), random_state=42)
            reduced = tsne.fit_transform(summary_bio_features)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced[idx, 0], reduced[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")

            # Highlight predicted genes
            # if predicted_indices:
            #     pred_idx = [topk_node_indices.index(i) for i in predicted_indices if i in topk_node_indices]
            #     for idx in pred_idx:
            #         x, y = reduced[idx]
            #         plt.scatter(x, y, facecolors='none', edgecolors='red', s=80, linewidths=2)

            plt.xlabel("t-SNE Dimension 1", fontsize=18)
            plt.ylabel("t-SNE Dimension 2", fontsize=18)
            plt.title("t-SNE of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            plt.legend()
            plt.tight_layout()
            plt.savefig(output_path_genes_clusters, bbox_inches='tight')
            plt.close()
            print(f"t-SNE visualization saved to {output_path_genes_clusters}")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")
    else:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_with_tick_label(
        graph, summary_bio_features_initial, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=20
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features_initial.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_initial.shape[1]}"
    scaler = StandardScaler()
    #summary_bio_features = scaler.fit_transform(summary_bio_features)
    summary_bio_features_initial = normalize(summary_bio_features_initial, norm='l2', axis=1)

    row_sums = summary_bio_features_initial.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]

    # Run balanced spectral biclustering
    bicluster = best_balanced_biclustering(summary_bio_features_initial, n_clusters, n_trials)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels  # optional reverse

    # Assign cluster labels to nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features_initial.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === t-SNE Visualization ===
    if summary_bio_features_initial.shape[0] >= 2 and summary_bio_features_initial.shape[1] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_initial) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_initial)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced_tsne[idx, 0], reduced_tsne[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")
            plt.xlabel("t-SNE Dimension 1", fontsize=18)
            plt.ylabel("t-SNE Dimension 2", fontsize=18)
            plt.tick_params(axis='both', which='major', labelsize=18)
            #plt.title("t-SNE of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            #plt.legend()
            plt.tight_layout()
            tsne_path = output_path_genes_clusters.replace(".png", "_tsne_initial.png")
            plt.savefig(tsne_path, bbox_inches='tight')
            plt.close()
            print(f"t-SNE visualization saved to {tsne_path}")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")
    else:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")

    # === UMAP Visualization ===
    if summary_bio_features_initial.shape[0] >= 2 and summary_bio_features_initial.shape[1] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_initial)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced_umap[idx, 0], reduced_umap[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")
            plt.xlabel("UMAP Dimension 1", fontsize=18)
            plt.ylabel("UMAP Dimension 2", fontsize=18)
            plt.tick_params(axis='both', which='major', labelsize=18)
            #plt.title("UMAP of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            #plt.legend()
            plt.tight_layout()
            umap_path = output_path_genes_clusters.replace(".png", "_umap_initial.png")
            plt.savefig(umap_path, bbox_inches='tight')
            plt.close()
            print(f"UMAP visualization saved to {umap_path}")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")
    else:
        print("Not enough samples or features to run UMAP. Skipping visualization.")

    # === PCA Visualization ===
    if summary_bio_features_initial.shape[0] >= 2 and summary_bio_features_initial.shape[1] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_initial)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced_pca[idx, 0], reduced_pca[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")
            plt.xlabel(f"PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.1f}%)", fontsize=18)
            plt.ylabel(f"PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.1f}%)", fontsize=18)
            plt.tick_params(axis='both', which='major', labelsize=18)
            #plt.title("PCA of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            #plt.legend()
            plt.tight_layout()
            pca_path = output_path_genes_clusters.replace(".png", "_pca_initial.png")
            plt.savefig(pca_path, bbox_inches='tight')
            plt.close()
            print(f"PCA visualization saved to {pca_path}")
        except Exception as e:
            print(f"PCA visualization failed: {e}")
    else:
        print("Not enough samples or features to run PCA. Skipping visualization.")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_initial(
    graph, summary_bio_features_initial, node_names, 
    predicted_cancer_genes, n_clusters,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    topk_node_indices=None,
    n_trials=20,
    feature_names=None
):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    #assert summary_bio_features_initial.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_initial.shape[1]}"
    if feature_names:
        assert len(set(feature_names)) == len(feature_names), "‚ùå Duplicate feature names detected!"
        print("‚úÖ No duplicate feature names in summary bio features.")
            
    summary_bio_features_initial = normalize(summary_bio_features_initial, norm='l2', axis=1)
    row_sums = summary_bio_features_initial.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]

    bicluster = best_balanced_biclustering(summary_bio_features_initial, n_clusters, n_trials)
    row_labels = (n_clusters - 1) - bicluster.row_labels_  # Optional reverse
    col_labels = bicluster.column_labels_

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features_initial.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_bio_features_initial.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_initial) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_initial)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_initial.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_bio_features_initial.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_initial)
            plot_embedding(reduced_umap, "UMAP", "_umap_initial.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_bio_features_initial.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_initial)
            plot_embedding(reduced_pca, "PCA", "_pca_initial.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo_initial(
        graph, summary_topo_features_initial, node_names,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_topo_features_initial.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features_initial.shape[1]}"
    
    # === Normalize topo features
    ##summary_topo_features = StandardScaler().fit_transform(summary_topo_features)    
    summary_topo_features_initial = normalize(summary_topo_features_initial, norm='l2', axis=1)
    row_sums = summary_topo_features_initial.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]


    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features_initial)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_topo_features_initial.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_topo_features_initial) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_topo_features_initial)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_initial.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_topo_features_initial.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_topo_features_initial)
            plot_embedding(reduced_umap, "UMAP", "_umap_initial.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_topo_features_initial.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_topo_features_initial)
            plot_embedding(reduced_pca, "PCA", "_pca_initial.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")
    
    # === Save original (unclustered) heatmap before biclustering
    ##output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_trained_(
    graph, summary_bio_features_trained, node_names, 
    predicted_cancer_genes, n_clusters,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    topk_node_indices=None,
    n_trials=20,
    feature_names=None
):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    #assert summary_bio_features_trained.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_trained.shape[1]}"
    
    summary_bio_features_trained = normalize(summary_bio_features_trained, norm='l2', axis=1)
    # row_sums = summary_bio_features_trained.sum(axis=1)
    # threshold = np.percentile(row_sums, 20)
    # high_saliency_indices = np.where(row_sums > threshold)[0]

    bicluster = best_balanced_biclustering(summary_bio_features_trained, n_clusters, n_trials)
    row_labels = (n_clusters - 1) - bicluster.row_labels_  # Optional reverse
    col_labels = bicluster.column_labels_

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features_trained.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_trained) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_trained.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_umap, "UMAP", "_umap_trained.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_pca, "PCA", "_pca_trained.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_trained(
    graph,
    summary_bio_features_trained,
    node_names,
    predicted_cancer_genes,
    n_clusters,
    save_path,
    save_row_labels_path,
    save_total_genes_per_cluster_path,
    save_predicted_counts_path,
    output_path_genes_clusters,
    output_path_heatmap,
    topk_node_indices=None,
    n_trials=20,
    feature_names=None  # Optional: for labeling heatmap axes
):
    print(f"üîç Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")

    # assert summary_bio_features_trained.shape[1] == 64, \
    #     f"Expected 64 summary features, got {summary_bio_features_trained.shape[1]}"
    
    if feature_names:
        assert len(set(feature_names)) == len(feature_names), "‚ùå Duplicate feature names detected!"
        print("‚úÖ No duplicate feature names in summary bio features.")

    # Normalize features
    summary_bio_features_trained = normalize(summary_bio_features_trained, norm='l2', axis=1)

    # Run spectral biclustering
    bicluster = best_balanced_biclustering(summary_bio_features_trained, n_clusters, n_trials)
    row_labels = (n_clusters - 1) - bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # Assign cluster labels to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to rows in summary_bio_features_trained.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Plotting helper
    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(
                reduced_data[idx, 0], reduced_data[idx, 1],
                color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                edgecolor='k', s=100, alpha=0.8
            )
        plt.xticks([]); plt.yticks([])
        plt.xlabel(""); plt.ylabel(""); plt.box(False)
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"üìà {method_name} visualization saved to {path}")

    # t-SNE
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_trained) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_trained.png")
        except Exception as e:
            print(f"‚ö†Ô∏è t-SNE failed: {e}")

    # UMAP
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_umap, "UMAP", "_umap_trained.png")
        except Exception as e:
            print(f"‚ö†Ô∏è UMAP failed: {e}")

    # PCA
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_pca, "PCA", "_pca_trained.png")
        except Exception as e:
            print(f"‚ö†Ô∏è PCA failed: {e}")

    # Optional: heatmap visualization
    try:
        import seaborn as sns
        import pandas as pd
        df = pd.DataFrame(summary_bio_features_trained, columns=feature_names if feature_names else None)
        row_order = np.argsort(row_labels)
        sns.clustermap(df.iloc[row_order], col_cluster=False, row_cluster=False, figsize=(16, 12), cmap="viridis")
        plt.savefig(output_path_heatmap, bbox_inches='tight')
        plt.close()
        print(f"üî• Heatmap saved to {output_path_heatmap}")
    except Exception as e:
        print(f"‚ö†Ô∏è Heatmap generation failed: {e}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo_trained(
        graph, summary_topo_features_trained, node_names,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_topo_features_trained.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features_trained.shape[1]}"
    
    # === Normalize topo features
    ##summary_topo_features = StandardScaler().fit_transform(summary_topo_features)    
    summary_topo_features_trained = normalize(summary_topo_features_trained, norm='l2', axis=1)
    # row_sums = summary_topo_features_trained.sum(axis=1)
    # threshold = np.percentile(row_sums, 20)
    # high_saliency_indices = np.where(row_sums > threshold)[0]


    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features_trained)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_topo_features_trained.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_topo_features_trained) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_topo_features_trained)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_trained.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_topo_features_trained.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_topo_features_trained)
            plot_embedding(reduced_umap, "UMAP", "_umap_trained.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_topo_features_trained.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_topo_features_trained)
            plot_embedding(reduced_pca, "PCA", "_pca_trained.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")
    
    # === Save original (unclustered) heatmap before biclustering
    ##output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_gene_feature_contributions_bio_ori(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_bio(df, barplot_path)

    # Prepare for heatmap
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    # Handle gene name and score
    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_topo_ori(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    # Barplot of all 64 topo features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_topo(df, barplot_path)

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def save_and_plot_confirmed_genes_bio(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="bio",
    confirmed_gene_path="data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their biological feature contributions.
    """

    
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]

    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics}:{cancer}" for omics in omics_order for cancer in cancer_names]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    # for gene_name in confirmed_genes:
    #     idx = node_names_topk.index(gene_name)
    #     relevance_vector = summary_feature_relevance[idx]
    #     score = get_scalar_score(node_scores_topk[idx])
    #     cluster_id = row_labels_topk[idx].item()

    #     output_path = os.path.join(
    #         "results/gene_prediction/bio_confirmed_feature_contributions/",
    #         f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_feature_contributions_epo{args.num_epochs}.png"
    #     )

    #     plot_gene_feature_contributions_bio(
    #         gene_name=gene,
    #         relevance_vector=relevance_vector,
    #         feature_names=feature_names,
    #         score=score,
    #         cluster_id=cluster_id,
    #         output_path=output_path
    #     )
    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_bio(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/bio_confirmed_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )

def save_and_plot_confirmed_genes_topo(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="topo",
    confirmed_gene_path="data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their topological feature contributions.
    """

    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{cancer}_{omics}" for cancer in cancer_names for omics in omics_order]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    ##summary_feature_relevance = extract_summary_features_np_topo(summary_feature_relevance)

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_topo(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/topo_confirmed_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )
    
def plot_model_performance(args):
    """
    Generates and saves a scatter plot comparing AUROC and AUPRC values 
    for different models across multiple networks.

    Parameters:
    - models: List of model names.
    - networks: List of network names.
    - auroc: 2D list of AUROC scores (rows: models, cols: networks).
    - auprc: 2D list of AUPRC scores (rows: models, cols: networks).
    - args: Arguments containing model and training configuration.
    - output_dir: Directory to save the plot.
    """


    # Define models and networks
    models = ["GRAIL", "HGDC", "EMOGI", "MTGCN", "GCN", "GAT", "GraphSAGE", "GIN", "Chebnet"]
    networks = ["CPDB", "STRING", "HIPPIE"]

    # AUPRC values for ONGene and OncoKB for each model (rows: models, cols: networks)
    auroc = [
        [0.9652, 0.9578, 0.9297],  # ACGNN ACGNN & 0.9652 & 0.9783 & 0.9578 & 0.9738 & 0.9297 & 0.9597 \\
        [0.6776, 0.7133, 0.6525],  # HGDC
        [0.6735, 0.8184, 0.6672],  # EMOGI
        [0.6862, 0.7130, 0.6762],  # MTGCN
        [0.6915, 0.6688, 0.6708],  # GCN
        [0.6670, 0.8166, 0.6478],  # GAT
        [0.6664, 0.6166, 0.6571],  # GraphSAGE
        [0.5836, 0.5173, 0.5844],  # GIN
        [0.8017, 0.8777, 0.7409]   # Chebnet
    ]

    auprc = [
        [0.9783, 0.9738, 0.9597],  # ACGNN
        [0.7288, 0.7740, 0.7634],  # HGDC
        [0.7230, 0.8737, 0.7960],  # EMOGI
        [0.7712, 0.7878, 0.7785],  # MTGCN
        [0.7730, 0.7681, 0.7675],  # GCN
        [0.7086, 0.8791, 0.7496],  # GAT
        [0.7522, 0.7182, 0.7624],  # GraphSAGE
        [0.6405, 0.5918, 0.6791],  # GIN
        [0.8622, 0.9159, 0.8443]   # Chebnet
    ]

    # Compute averages for each model
    avg_auroc = np.mean(auroc, axis=1)
    avg_auprc = np.mean(auprc, axis=1)

    # Define colors for models and unique shapes for networks
    colors = ['red', 'grey', 'blue', 'green', 'purple', 'orange', 'cyan', 'brown', 'pink']
    network_markers = ['P', '^', 's']  # One shape for each network
    avg_marker = 'o'  # Marker for average points

    # Create the plot
    plt.figure(figsize=(8, 7))

    # Plot individual points for each model and network
    for i, model in enumerate(models):
        for j, network in enumerate(networks):
            plt.scatter(auprc[i][j], auroc[i][j], color=colors[i], 
                        marker=network_markers[j], s=90, alpha=0.6)

    # Add average points for each model
    for i, model in enumerate(models):
        plt.scatter(avg_auprc[i], avg_auroc[i], color=colors[i], marker=avg_marker, 
                    s=240, edgecolor='none', alpha=0.5)

    # Create legends for models (colors) and networks (shapes)
    model_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], 
                            markersize=14, label=models[i], alpha=0.5) for i in range(len(models))]
    network_legend = [Line2D([0], [0], marker=network_markers[i], color='k', linestyle='None', 
                            markersize=8, label=networks[i]) for i in range(len(networks))]

    # Add legends
    network_legend_artist = plt.legend(handles=network_legend, loc='lower right', title="Networks", fontsize=12, title_fontsize=16, frameon=True)
    plt.gca().add_artist(network_legend_artist)
    plt.legend(handles=model_legend, loc='upper left', fontsize=12, frameon=True)

    # Labels and title
    plt.ylabel("AUPRC", fontsize=16)
    plt.xlabel("AUROC", fontsize=16)

    # Save the plot
    comp_output_path = os.path.join('results/gene_prediction/', f'{args.model_type}_{args.net_type}_comp_plot_epo{args.num_epochs}_infeats{args.in_feats}.jpeg')
    plt.savefig(comp_output_path, bbox_inches='tight')
    
    print(f"Comparison plot saved to {comp_output_path}")

    # Show plot
    plt.tight_layout()
    plt.close()

def train_with_relevance_tracking(args, model, graph, embeddings, labels, train_mask, output_dir):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    graph = graph.to(device)
    features = embeddings.to(device)
    labels = labels.to(device).float()
    train_mask = train_mask.to(device)

    loss_fn = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    epoch_times, cpu_usages, gpu_usages = [], [], []
    bio_scores, topo_scores = [], []

    for epoch in tqdm(range(args.num_epochs), desc="Training Progress", unit="epoch"):
        epoch_start = time.time()
        cpu_usage = psutil.cpu_percent(interval=None)
        gpu_usage = torch.cuda.memory_allocated(device) / 2048**2 if torch.cuda.is_available() else 0.0

        model.train()
        logits = model(graph, features).squeeze()
        loss = loss_fn(logits[train_mask], labels[train_mask])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_times.append(time.time() - epoch_start)
        cpu_usages.append(cpu_usage)
        gpu_usages.append(gpu_usage)

        tqdm.write(f"Epoch {epoch + 1}/{args.num_epochs}, Loss: {loss.item():.4f}, CPU: {cpu_usage}%, GPU: {gpu_usage:.2f} MB")

        # Relevance score tracking
        model.eval()
        with torch.no_grad():
            logits = model(graph, features).squeeze()
            probs = torch.sigmoid(logits).cpu()

        cancer_pred_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
        if cancer_pred_indices.ndim == 0:
            cancer_pred_indices = cancer_pred_indices.unsqueeze(0)

        if len(cancer_pred_indices) > 0:
            relevance = compute_relevance_scores(model, graph, features, node_indices=cancer_pred_indices)
            bio_mean = relevance[:, 0:1024].mean().item()
            topo_mean = relevance[:, 1024:2048].mean().item()
            bio_scores.append(bio_mean)
            topo_scores.append(topo_mean)
        else:
            bio_scores.append(0.0)
            topo_scores.append(0.0)

    heatmap_path_relevance_scores = os.path.join(
                output_dir, f"{args.model_type}_{args.net_type}_relevance_scores_epo{args.num_epochs}.png"
            )

    # Relative score conversion
    if bio_scores[0] == 0:
        bio_scores_rel = bio_scores  # or set to all zeros, or handle however you prefer
        print("Warning: bio_scores[0] is zero; skipping relative normalization.")
    else:
        bio_scores_rel = [score / bio_scores[0] for score in bio_scores]
    if topo_scores[0] == 0:
        topo_scores_rel = topo_scores  # or set to all zeros, or handle however you prefer
        print("Warning: bio_scores[0] is zero; skipping relative normalization.")
    else:
        topo_scores_rel = [score / topo_scores[0] for score in topo_scores]

    # Plotting
    plt.figure(figsize=(8, 5))
    plt.plot(bio_scores_rel, label='Bio', color='#1f77b4')
    plt.plot(topo_scores_rel, label='Topo', color='#ff7f0e')

    plt.xlabel('Epoch')
    plt.ylabel('Relative Mean Relevance Score')
    plt.title('Relative Relevance Scores per Feature Group')
    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))

    # Remove legend frame
    plt.legend(frameon=False, loc='upper left', fontsize=18)
    
    plt.xticks(fontsize=18)
    plt.yticks(fontsize=18)
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
    sns.despine()

    plt.tight_layout()
    plt.savefig(os.path.join(heatmap_path_relevance_scores))
    plt.close()


    # ‚úÖ RETURN FINAL SCORES
    return probs.numpy()  # This is the final sigmoid output per node

def get_adjacency_matrix(X, delta=0.3):
    dists = squareform(pdist(X, metric='correlation'))
    dists[dists <= delta] = 0
    return dists

def eigen_decomposition(A, topK=5):
    L = csgraph.laplacian(A, normed=True)
    eigenvalues, eigenvectors = LA.eig(L)
    idx = eigenvalues.argsort()
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    index_largest_gap = np.argsort(np.diff(eigenvalues))[::-1][:topK]
    nb_clusters = index_largest_gap + 1
    return nb_clusters, eigenvalues, eigenvectors

def eigengap_analysis(relevance_matrix, args, output_dir, name_prefix='bio', delta_row=1.185, delta_col=1.054):
    """
    Perform eigengap analysis for both rows and columns of the relevance matrix.

    Parameters:
        relevance_matrix (np.ndarray): The input relevance matrix (e.g. biological relevance scores)
        args: Namespace containing model_dir, model_type, net_type, num_epochs, etc.
        name_prefix (str): Prefix for file and log output (e.g. 'bio' or 'topo')
        delta_row (float): Distance threshold for row affinity
        delta_col (float): Distance threshold for column affinity

    Returns:
        dict: Contains row/column eigenvalues, eigenvectors, and estimated cluster counts
    """
    results = {}

    # Row-wise analysis
    affinity_row = get_adjacency_matrix(relevance_matrix, delta=delta_row)
    k_row, evals_row, evecs_row = eigen_decomposition(affinity_row)
    print(f'[{name_prefix}] Optimal number of row clusters: {k_row}')
    results['k_row'] = k_row
    results['evals_row'] = evals_row
    results['evecs_row'] = evecs_row

    # Column-wise analysis
    affinity_col = get_adjacency_matrix(relevance_matrix.T, delta=delta_col)
    k_col, evals_col, evecs_col = eigen_decomposition(affinity_col)
    print(f'[{name_prefix}] Optimal number of column clusters: {k_col}')
    results['k_col'] = k_col
    results['evals_col'] = evals_col
    results['evecs_col'] = evecs_col

    # Plot eigenvalues
    fig = plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    sns.scatterplot(x=np.arange(len(evals_row)), y=evals_row, label='Eigenvalues Rows',
                    s=100, color='black', alpha=0.6)
    plt.title(f'Row Eigengap ({name_prefix})')
    plt.legend(loc='lower right')

    plt.subplot(1, 2, 2)
    sns.scatterplot(x=np.arange(len(evals_col)), y=evals_col, label='Eigenvalues Columns',
                    s=100, color='black', alpha=0.6)
    plt.title(f'Column Eigengap ({name_prefix})')
    plt.legend(loc='lower right')
    plt.close()

    plot_path = os.path.join(
        output_dir, 
        f"{args.model_type}_{args.net_type}_eigengap_plot_{name_prefix}_epo{args.num_epochs}.png"
    )
    fig.savefig(plot_path)
    print(f'Saved eigengap plot to: {plot_path}')
    
    return results

def apply_full_spectral_biclustering_bio(
        graph, summary_bio_features, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=100  # New param to control how many seeds to try
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]
    filtered_features = summary_bio_features[high_saliency_indices]

    relevance_matrix = filtered_features

    # === Use balanced biclustering
    bicluster = best_balanced_biclustering(summary_bio_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional: Heatmaps and t-SNE
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo(
        graph, summary_topo_features, node_names_topk,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on topo features with {n_clusters} clusters...")
    assert summary_topo_features.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features.shape[1]}"

    # === Normalize topo features
    summary_topo_features = StandardScaler().fit_transform(summary_topo_features)

    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names_topk, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)
    
    # === Save original (unclustered) heatmap before biclustering
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def compute_laplacian_eigen(similarity_matrix):
    """Compute normalized Laplacian and its eigenvalues/vectors using stable decomposition."""
    L, _ = csgraph.laplacian(similarity_matrix, normed=True, return_diag=True)
    eigenvals, eigenvecs = eigh(L)
    return eigenvals, eigenvecs

def estimate_clusters_by_eigengap(eigenvals, max_k=25):
    """Estimate number of clusters using eigengap heuristic."""
    gaps = np.diff(eigenvals[1:max_k + 1])
    best_k = np.argmax(gaps) + 1
    return best_k

def eigengap_analysis(relevance_matrix, args, output_dir, name_prefix='bio', delta_row=1.185, delta_col=1.054):
    """
    Perform eigengap analysis for both rows and columns of the relevance matrix.

    Parameters:
        relevance_matrix (np.ndarray): The input relevance matrix (e.g. biological relevance scores)
        args: Namespace containing model_dir, model_type, net_type, num_epochs, etc.
        name_prefix (str): Prefix for file and log output (e.g. 'bio' or 'topo')
        delta_row (float): Not used (kept for interface compatibility)
        delta_col (float): Not used (kept for interface compatibility)

    Returns:
        dict: Contains row/column eigenvalues, eigenvectors, and estimated cluster counts
    """
    results = {}

    # Row-wise analysis using RBF kernel
    similarity_row = rbf_kernel(relevance_matrix, gamma=0.5)
    evals_row, evecs_row = compute_laplacian_eigen(similarity_row)
    k_row = estimate_clusters_by_eigengap(evals_row)
    print(f'[{name_prefix}] Optimal number of row clusters: {k_row}')
    results['k_row'] = k_row
    results['evals_row'] = evals_row
    results['evecs_row'] = evecs_row

    # Column-wise analysis using RBF kernel
    similarity_col = rbf_kernel(relevance_matrix.T, gamma=0.5)
    evals_col, evecs_col = compute_laplacian_eigen(similarity_col)
    k_col = estimate_clusters_by_eigengap(evals_col)
    print(f'[{name_prefix}] Optimal number of column clusters: {k_col}')
    results['k_col'] = k_col
    results['evals_col'] = evals_col
    results['evecs_col'] = evecs_col

    # Plot eigenvalues
    fig = plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    sns.scatterplot(x=np.arange(len(evals_row)), y=evals_row, label='Eigenvalues Rows',
                    s=100, color='black', alpha=0.6)
    plt.axvline(x=k_row, color='red', linestyle='--', label=f'k={k_row}')
    plt.title(f'Row Eigengap ({name_prefix})')
    plt.legend(loc='lower right')

    plt.subplot(1, 2, 2)
    sns.scatterplot(x=np.arange(len(evals_col)), y=evals_col, label='Eigenvalues Columns',
                    s=100, color='black', alpha=0.6)
    plt.axvline(x=k_col, color='red', linestyle='--', label=f'k={k_col}')
    plt.title(f'Column Eigengap ({name_prefix})')
    plt.legend(loc='lower right')

    fig.tight_layout()
    plt.close()

    # Save plot
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_eigengap_plot_{name_prefix}_epo{args.num_epochs}.png"
    )
    fig.savefig(plot_path)
    print(f'Saved eigengap plot to: {plot_path}')

    return results

def extract_summary_features_np_bio(bio_embeddings_np):
    """
    Extracts summary features from just the 1024 biological features (bio only).

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = bio_embeddings_np.shape
    summary_features = []

    assert num_features == 1024, f"Expected 1024 bio features, got {num_features}"

    for o_idx in range(4):  # 4 omics types
        for c_idx in range(16):  # 16 cancer types
            base = o_idx * 16 * 16 + c_idx * 16
            group = bio_embeddings_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)

def extract_summary_features_np_topo(topo_features_np):
    """
    Extracts summary features from the topological embedding section (features 1024‚Äì2047)
    by computing the max over each 16-dimensional segment.

    Args:
        features_np (np.ndarray): shape [num_nodes, 2048]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = topo_features_np.shape
    assert num_features == 1024, f"Expected 2048 features, got {num_features}"

    # Select topological features only
    ##topo_features = features_np[:, 1024:]  # shape: [num_nodes, 1024]
    ##topo_features = topo_features_np[:, 1024:2048]
    topo_features = topo_features_np  # already 1024 features


    summary_features = []

    # Pool over 64 chunks of 16 features
    for i in range(64):
        start = i * 16
        end = start + 16
        group = topo_features[:, start:end]  # shape: [num_nodes, 16]
        max_vals = group.max(axis=1, keepdims=True)  # shape: [num_nodes, 1]
        summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # shape: [num_nodes, 64]

def eigengap_analysis(feature_matrix, max_clusters=25, normalize=True, plot_path=None):
    # Step 1: Normalize features (optional)
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Print eigenvalues
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({"Eigenvalue Index": np.arange(len(eigenvals)), "Eigenvalue": eigenvals}).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Optional plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]
        
        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        gaps = np.diff(eigenvals[1:max_clusters + 1])
        best_k = np.argmax(gaps) + 1  # +1 because diff shifts index

        plt.axvline(
            x=best_k,
            color='pink',
            linestyle='--',
            label=f'Eigengap ‚Üí k={best_k}'
        )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=16)

        # Set integer x-axis ticks
        plt.xticks(range(1, max_clusters + 1))

        # Remove grid
        plt.grid(False)

        # Remove legend frame
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()
    else:
        # If no plot is saved, still compute best_k
        gaps = np.diff(eigenvals[1:max_clusters + 1])
        best_k = np.argmax(gaps) + 1

    return best_k, eigenvals

def eigengap_analysis(feature_matrix, max_clusters=20, normalize=True, plot_path=None, top_k=5):
    # Step 1: Normalize features (optional)
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Eigengap Computation
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    top_k_indices = np.argsort(gaps)[-top_k:][::-1] + 1  # +1 to correct index offset from diff
    top_k_values = gaps[np.argsort(gaps)[-top_k:][::-1]]

    top_k_dict = dict(zip(top_k_indices, top_k_values))

    # Save top k to CSV
    if plot_path:
        topk_path = os.path.splitext(plot_path)[0] + "_topk.csv"
        pd.DataFrame({
            "Rank": np.arange(1, top_k + 1),
            "k": top_k_indices,
            "Eigengap": top_k_values
        }).to_csv(topk_path, index=False)
        print(f"Top-{top_k} eigengap values saved to: {topk_path}")

    # Plotting
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in top_k_indices:
            plt.axvline(
                x=k,
                linestyle='--',
                alpha=0.5,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=16)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)

        # Remove duplicate legends
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        legend = plt.legend(by_label.values(), by_label.keys())
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return top_k_indices.tolist(), eigenvals

def run_eigengap_row_col(relevance_matrix, output_dir, args):
    # Paths for plots
    plot_path_row = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_row_epo{args.num_epochs}.png")
    plot_path_col = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_col_epo{args.num_epochs}.png")

    # Paths for saving top-k
    save_topk_row_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_top5k_row_epo{args.num_epochs}.txt")
    save_topk_col_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_top5k_col_epo{args.num_epochs}.txt")

    # Row eigengap
    top5_k_row, _ = eigengap_analysis_topk(
        relevance_matrix,
        max_clusters=25,
        normalize=True,
        plot_path=plot_path_row,
        top_k=5
    )

    # Column eigengap
    top5_k_col, _ = eigengap_analysis_topk(
        relevance_matrix.T,
        max_clusters=25,
        normalize=True,
        plot_path=plot_path_col,
        top_k=5
    )

    # Save row top-k
    with open(save_topk_row_path, 'w') as f:
        for k in top5_k_row:
            f.write(f"{k}\n")
    print(f"Saved top-5 row cluster ks to: {save_topk_row_path}")

    # Save column top-k
    with open(save_topk_col_path, 'w') as f:
        for k in top5_k_col:
            f.write(f"{k}\n")
    print(f"Saved top-5 column cluster ks to: {save_topk_col_path}")

    return top5_k_row, top5_k_col

def apply_full_spectral_biclustering_bio_no_progress_bar(
    graph, summary_bio_features, node_names, 
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None,
    n_trials=100,
    max_clusters=25,
    top_k=5
):

    print("Running Spectral Biclustering with eigengap-based top-k cluster search...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]
    filtered_features = summary_bio_features[high_saliency_indices]

    # === Eigengap for rows ===
    plot_path_row = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_row_epo{args.num_epochs}.png")
    top5_k_row, _ = eigengap_analysis_topk(
        filtered_features,
        max_clusters=max_clusters,
        normalize=True,
        plot_path=plot_path_row,
        top_k=top_k
    )

    # === Eigengap for columns ===
    plot_path_col = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_col_epo{args.num_epochs}.png")
    top5_k_col, _ = eigengap_analysis_topk(
        filtered_features.T,
        max_clusters=max_clusters,
        normalize=True,
        plot_path=plot_path_col,
        top_k=top_k
    )

    # Choose final n_clusters
    n_clusters = top5_k_row[0]  # or use mean/top vote/criteria
    print(f"‚Üí Using n_clusters = {n_clusters} from top-5 eigengap analysis")

    # === Use balanced biclustering
    bicluster = best_balanced_biclustering(filtered_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices[high_saliency_indices]] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional: Heatmaps and t-SNE
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def eigengap_analysis_topk_include_k_1(feature_matrix, max_clusters=25, normalize=True, plot_path=None, top_k=5):
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, _ = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Step 5: Compute eigengaps
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    eigengap_indices = np.argsort(gaps)[::-1][:top_k]  # top-k largest gaps
    topk = (eigengap_indices + 1).tolist()  # shift index because of diff

    # Step 6: Save eigenvalues
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 7: Plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in topk:
            plt.axvline(
                x=k,
                color='pink',
                linestyle='--',
                alpha=0.7,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=16)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)
        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return topk, eigenvals

def eigengap_analysis_topk_skip_less_5(feature_matrix, max_clusters=25, normalize=True, plot_path=None, top_k=5):
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, _ = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Step 5: Compute eigengaps
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    eigengap_indices = np.argsort(gaps)[::-1][:top_k]  # top-k largest gaps
    topk = (eigengap_indices + 1).tolist()  # shift index because of diff

    # Filter k > 4
    topk = [k for k in topk if k > 4]

    # Step 6: Save eigenvalues
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 7: Plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in topk:
            plt.axvline(
                x=k,
                color='pink',
                linestyle='--',
                alpha=0.7,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=16)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)
        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return topk, eigenvals

def get_optimal_column_clusters(score_matrix, max_clusters=25, top_k=5, normalize=True, plot_path=None):
    """
    Returns column cluster labels and sorted indices using optimal k from eigengap.
    """
    topk, _ = eigengap_analysis_topk(score_matrix.T, max_clusters=max_clusters, normalize=normalize, plot_path=plot_path, top_k=top_k)
    best_k = topk[0] if topk else 6  # fallback to 6 clusters if eigengap fails

    model = SpectralBiclustering(n_clusters=(1, best_k), method='log', random_state=0)
    model.fit(score_matrix)

    col_labels = model.column_labels_              # shape: [n_features]
    col_order = np.argsort(col_labels)

    return col_order, col_labels

def eigengap_analysis_topk(feature_matrix, max_clusters=25, normalize=True, plot_path=None, top_k=5):
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.metrics.pairwise import rbf_kernel
    from scipy.sparse.csgraph import laplacian
    from scipy.linalg import eigh
    import os

    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, _ = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Step 5: Compute eigengaps
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    all_indices = np.argsort(gaps)[::-1] + 1  # shift index because of diff

    # Step 6: Filter for k > 4 and take top_k
    topk = [k for k in all_indices if k > 4][:top_k]

    # Step 7: Save eigenvalues
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 8: Plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in topk:
            plt.axvline(
                x=k,
                color='pink',
                linestyle='--',
                alpha=0.7,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=16)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)
        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return topk, eigenvals

def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names, 
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None,
    n_trials=10,
    max_clusters_row=25,
    max_clusters_col=10,
    top_k=5
):
    print("üß™ Running Spectral Biclustering with eigengap-based top-k cluster search...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Saliency-based filtering
    row_sums = summary_bio_features.sum(axis=1)
    ##threshold = np.percentile(row_sums, 20)
    # threshold = np.percentile(row_sums, 0)
    # high_saliency_indices = np.where(row_sums > threshold)[0]
    ##filtered_features = summary_bio_features[high_saliency_indices]
    filtered_features = summary_bio_features

    # === Eigengap for rows
    plot_path_row = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_row_epo{args.num_epochs}.png")
    top5_k_row, _ = eigengap_analysis_topk(
        filtered_features,
        max_clusters=max_clusters_row,
        normalize=True,
        plot_path=plot_path_row,
        top_k=top_k
    )

    # === Eigengap for columns
    plot_path_col = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_col_epo{args.num_epochs}.png")
    top5_k_col, _ = eigengap_analysis_topk(
        filtered_features.T,
        max_clusters=max_clusters_col,
        normalize=True,
        plot_path=plot_path_col,
        top_k=top_k
    )

    # Choose final number of clusters (rows only)
    ##n_clusters = top5_k_row[0]
    # Filter top-k row cluster candidates to ensure k >= 5
    valid_ks = [k for k in top5_k_col if k >= 5]
    if not valid_ks:
        raise ValueError(f"No valid row cluster counts ‚â• 5 found in top-k candidates: {top5_k_row}")

    n_clusters = valid_ks[0]
    print(f"‚Üí Using n_clusters = {n_clusters} (filtered to k ‚â• 5) from top-5 eigengap analysis")

    ##print(f"‚Üí Using n_clusters = {n_clusters} from top-5 eigengap analysis")

    # === Use balanced biclustering with progress bar

    best_model = None
    best_score = np.inf  # lower MSE is better

    print("üîÅ Running biclustering trials:")
    for i in tqdm(range(n_trials), desc="Biclustering Trials", ncols=80):
        model = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=i)
        model.fit(filtered_features)

        # Reconstruct the data matrix from the clustering
        reconstructed = filtered_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        
        # Calculate reconstruction error
        mse = mean_squared_error(filtered_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model


    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    #row_labels_tensor[topk_node_indices[high_saliency_indices]] = torch.tensor(row_labels, dtype=torch.long)
    #indices_to_update = torch.tensor(topk_node_indices[high_saliency_indices], dtype=torch.long)
    # indices_to_update = torch.tensor(np.array(topk_node_indices)[high_saliency_indices], dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)

    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional heatmap & t-SNE (uncomment as needed)
    # output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None,
    sorted_node_indices=None  # corresponds to topk_node_indices[high_saliency_indices]
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.colors import LinearSegmentedColormap, to_rgba

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Normalize scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    # Sort rows by cluster labels
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    col_order, col_labels = get_optimal_column_clusters(sorted_scores)
    
    raw_feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        raw_feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    if col_labels is not None:
        sorted_scores = sorted_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [raw_feature_colors[i] for i in col_order]
    else:
        feature_colors = raw_feature_colors


    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(x=i + 0.5, height=val, width=1.0, color=color, edgecolor='black', linewidth=0.5, alpha=0.3 + 0.7 * val)

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency side curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)),
        0,
        saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bar under feature bar
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end + 1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_spectral_biclustering_heatmap_topk(features, node_names, feature_names, pred,
                                             cgc_path, model_dir,
                                             n_row_clusters=17, n_col_clusters=5,
                                             top_k=1000,
                                             output_name='biclustering_input_topk.png'):
    """
    Performs spectral biclustering on the top-K predicted genes and plots a heatmap.

    Args:
        features (ndarray): Full feature matrix of shape (n_genes, n_features)
        node_names (ndarray): Array with gene names, shape (n_genes, 2), index 1 holds gene symbols
        feature_names (list): List of feature names
        pred (pd.DataFrame): DataFrame with predicted genes, must contain column 'Name'
        cgc_path (str): Path to Cancer Gene Census file with 'Gene Symbol' and 'Role in Cancer'
        model_dir (str): Output directory to save the heatmap
        n_row_clusters (int): Number of gene (row) clusters
        n_col_clusters (int): Number of feature (column) clusters
        top_k (int): Number of top predictions to include
        output_name (str): Name of the output PNG file
    """
    # Step 1: Prepare top-K gene-feature matrix
    X_df = pd.DataFrame(features, index=node_names[:, 1], columns=feature_names)
    X_topk = X_df[X_df.index.isin(pred.head(top_k).Name)]
    print(f"Selected {X_topk.shape[0]} Genes for Spectral Biclustering")

    # Step 2: Normalize
    X_norm = pd.DataFrame(StandardScaler().fit_transform(X_topk),
                          index=X_topk.index, columns=X_topk.columns)

    # Step 3: Biclustering
    model = SpectralBiclustering(n_clusters=(5,5),
                                 method='bistochastic',
                                 svd_method='randomized',
                                 n_jobs=-1, random_state=0)
    model.fit(X_norm)

    # Step 4: Row/column orders
    row_labels = pd.Series(model.row_labels_, index=X_norm.index)
    col_labels = pd.Series(model.column_labels_, index=X_norm.columns)

    ordered_rows = row_labels.sort_values().index.tolist()
    ordered_cols = col_labels.sort_values().index.tolist()

    X_plot = X_norm.loc[ordered_rows, ordered_cols]

    # Step 5: Cancer gene annotations
    cgc = pd.read_csv(cgc_path).set_index('Gene Symbol')

    gene_roles = pd.Series(0, index=X_norm.index, name='CancerGeneCensus')
    gene_roles[cgc['Role in Cancer'].str.contains('oncogene', na=False)] = 1
    gene_roles[cgc['Role in Cancer'].str.contains('TSG', na=False)] = 2
    gene_roles = gene_roles.loc[X_norm.index]

    # Step 6: Color palettes
    row_cluster_palette = dict(zip(range(n_row_clusters), sns.color_palette("deep", n_colors=n_row_clusters)))
    col_cluster_palette = dict(zip(range(n_col_clusters), sns.color_palette("muted", n_colors=n_col_clusters)))
    gene_role_palette = {0: 'grey', 1: 'darkred', 2: 'darkgreen'}

    row_colors = pd.DataFrame({
        'Role': gene_roles.map(gene_role_palette),
        'Cluster': row_labels.map(row_cluster_palette)
    }).loc[ordered_rows]

    col_colors = col_labels.map(col_cluster_palette).loc[ordered_cols]

    # Step 7: Plot heatmap
    cm = sns.clustermap(
        X_plot,
        method=None,
        metric='correlation',
        cmap='RdBu_r',
        row_cluster=False,
        col_cluster=False,
        row_colors=row_colors,
        col_colors=col_colors,
        yticklabels=7,
        xticklabels=1,
        figsize=(20, 30),
        center=0
    )

    # Step 8: Save
    os.makedirs(model_dir, exist_ok=True)
    out_path = os.path.join(model_dir, output_name)
    cm.savefig(out_path, dpi=300)
    print(f"Saved biclustering heatmap to: {out_path}")

    # Optionally return results
    return {
        "bicluster_model": model,
        "row_labels": row_labels,
        "col_labels": col_labels,
        "gene_roles": gene_roles,
        "X_plot": X_plot
    }

def plot_biclustering_input_feature_heatmap(
    feature_matrix: np.ndarray,
    node_names_topk: list,
    output_path: str,
    title: str = "Biclustering Heatmap (Input Features)",
    cmap: str = "viridis"
):
    """
    Applies spectral biclustering to input features of top-k predicted genes
    and visualizes the result as a heatmap.

    Parameters:
    - feature_matrix: np.ndarray, shape [n_genes, n_features]
        Input summary features (e.g., 64-dim bio or topo features)
    - node_names_topk: list of str
        Gene names for top-k predicted genes
    - output_path: str
        File path to save the heatmap
    - title: str
        Plot title
    - cmap: str
        Colormap for heatmap
    """
    assert feature_matrix.shape[0] == len(node_names_topk), \
        f"Mismatch: {feature_matrix.shape[0]} rows vs {len(node_names_topk)} gene names"

    # Normalize the feature matrix
    feature_matrix_norm = normalize(feature_matrix, axis=1)

    # Apply spectral biclustering
    n_clusters = 10
    model = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=0)
    model.fit(feature_matrix_norm)

    # Reorder matrix
    row_order = np.argsort(model.row_labels_)
    col_order = np.argsort(model.column_labels_)
    clustered_matrix = feature_matrix_norm[row_order][:, col_order]

    # Reorder gene names for heatmap annotation
    reordered_gene_names = [node_names_topk[i] for i in row_order]

    # Plot
    plt.figure(figsize=(10, 8))
    sns.heatmap(clustered_matrix, cmap=cmap, yticklabels=reordered_gene_names, xticklabels=False)
    plt.title(title, fontsize=16)
    plt.xlabel("Features")
    plt.ylabel("Top-k Genes (Reordered)")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

    print(f"[Saved] Input feature biclustering heatmap ‚Üí {output_path}")

def plot_spectral_biclustering_heatmap_topk_(
    scores,
    row_labels,
    col_labels,
    row_order,
    col_order,
    omics_splits,
    output_path,
    omics_colors=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.colors import LinearSegmentedColormap, to_rgba
    import numpy as np

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Reorder the matrix
    sorted_scores = scores[np.ix_(row_order, col_order)]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))
    feature_colors = [feature_colors[i] for i in col_order]
    feature_names = [feature_names[i] for i in col_order]

    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(x=i + 0.5, height=val, width=1.0, color=color, edgecolor='black', linewidth=0.5, alpha=0.3 + 0.7 * val)

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Row cluster sizes
    unique_clusters, cluster_sizes = np.unique(row_labels[row_order], return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Cluster stripe
    for i, cluster in enumerate(row_labels[row_order]):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # X-axis tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency side curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)),
        0,
        saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bar under feature bar
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, col_order.index(start):col_order.index(end)+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None
):  
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from sklearn.cluster import SpectralBiclustering
    from matplotlib.colors import LinearSegmentedColormap

    # üîπ Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Biclustering: 12 row clusters, 7 column clusters
    model = SpectralBiclustering(n_clusters=(5, 5), method='bistochastic', random_state=0)
    model.fit(relevance_scores)

    # Reorder matrix by clustered indices
    data_reordered = relevance_scores[model.row_order_, :][:, model.column_order_]
    feature_names_reordered = [feature_names[i] for i in model.column_order_]

    # Build feature_colors for reordered columns
    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))
    feature_colors = [feature_colors[i] for i in model.column_order_]

    # Begin plotting
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # Top bar chart
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names_reordered))
    ax_bar.set_ylim(0, 1.1)

    feature_means = data_reordered.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(data_reordered, 99)

    sns.heatmap(
        data_reordered,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Show tick labels with small tick marks
    ax.set_xticks(np.arange(len(feature_names_reordered)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names_reordered], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # Saliency curve (row-wise sums)
    saliency_sums = data_reordered.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    reordered_omics_splits = {omics: [] for omics in omics_order}
    for i, name in enumerate(feature_names_reordered):
        omics = name.split(":")[0].lower()
        reordered_omics_splits[omics].append(i)

    for omics in omics_order:
        if reordered_omics_splits[omics]:
            start = min(reordered_omics_splits[omics])
            end = max(reordered_omics_splits[omics])
            group_center = (start + end) / 2 + 0.5
            mean_val = data_reordered[:, start:end+1].mean()
            norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
            ax.bar(
                x=group_center,
                height=0.15,
                width=end - start + 1,
                bottom=len(relevance_scores) + 1.5,
                color=omics_colors[omics],
                edgecolor='black',
                linewidth=1,
                alpha=min(1.0, 0.3 + 0.7 * norm_mean)
            )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_no_column_color_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # üîπ Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # üîÅ Reorder based on biclustering if provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def apply_full_spectral_biclustering_bio_pass(
    graph, summary_bio_features, node_names, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    print("üß™ Running Spectral Biclustering with fixed (10, 7) clusters...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Biclustering
    n_clusters_row = 5
    n_clusters_col = 5
    n_clusters=(5,5)

    best_model = None
    best_score = np.inf

    print("üîÅ Running biclustering trials:")
    for i in range(10):  # or use tqdm
        model = SpectralBiclustering(n_clusters=(5,5), method='bistochastic',
                             svd_method='randomized', random_state=0
                            )
        model.fit(summary_bio_features)

        reconstructed = summary_bio_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # Assign cluster labels to top-k nodes
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features,#relevance_scores_topk.detach().cpu().numpy(),
        #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
        #gene_names=node_names_topk,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )
    
    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap_unsort_use_clustermap_no_clustering(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min() + 1e-8) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    # Column (feature) colors by omics type
    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))
    feature_colors = [feature_colors[i] for i in original_col_indices]

    # Define row_colors (if labels provided)
    row_colors = None
    if row_labels is not None:
        row_labels = np.array(row_labels)
        unique_row_labels = np.unique(row_labels)
        row_palette = sns.color_palette("Set2", len(unique_row_labels))
        row_lut = dict(zip(unique_row_labels, row_palette))
        row_colors = pd.Series(row_labels).map(row_lut).to_numpy()

    # Define col_colors (if labels provided)
    col_colors = None
    if col_labels is not None:
        col_labels = np.array(col_labels)
        unique_col_labels = np.unique(col_labels)
        col_palette = sns.color_palette("tab20", len(unique_col_labels))
        col_lut = dict(zip(unique_col_labels, col_palette))
        col_colors = pd.Series(col_labels).map(col_lut).to_numpy()
    else:
        col_colors = pd.Series(feature_colors)

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # Create clustermap
    g = sns.clustermap(
        relevance_scores,
        # row_cluster=True,
        # col_cluster=True,
        row_cluster=False,
        col_cluster=False,
        row_colors=row_colors,
        col_colors=col_colors,
        cmap=bluish_gray_gradient,
        xticklabels=False,
        yticklabels=False,
        figsize=(18, 17),
        cbar_pos=(0.03, 0.8, 0.02, 0.15)  # adjust colorbar position
    )

    # Improve x-axis tick label coloring
    ax = g.ax_heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([name.split(": ")[1] for name in feature_names], rotation=90, fontsize=10)
    for label, color in zip(ax.get_xticklabels(), col_colors):
        label.set_color(color)

    # Optional: label color bar axes
    g.cax.set_ylabel("Relevance Score", fontsize=16)
    g.cax.yaxis.label.set_color("#85929e")
    g.cax.tick_params(colors="#85929e", labelsize=12)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_clustermap_no_top_cluster_color_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Normalize the relevance scores to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    # Default omics colors
    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If there are fewer features, trim names
    feature_names = feature_names[:relevance_scores.shape[1]]
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Column colors for omics (used as a colorbar)
    col_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        color = omics_colors[omics]
        col_colors.extend([color] * (end - start + 1))
    col_colors = col_colors[:relevance_scores.shape[1]]

    # Reorder rows/columns if labels are provided
    if row_labels is not None:
        df = df.iloc[np.argsort(row_labels)]
    if col_labels is not None:
        df = df.iloc[:, np.argsort(col_labels)]
        col_colors = [col_colors[i] for i in np.argsort(col_labels)]


    # Plot clustermap without clustering
    cg = sns.clustermap(
        df,
        cmap="Blues",
        row_cluster=False,
        col_cluster=False,
        col_colors=col_colors,
        xticklabels=True,
        yticklabels=False,
        figsize=(18, 14),
        cbar_pos=(0.02, 0.8, 0.02, 0.18),
        cbar_kws={"label": "Relevance Score"},
        dendrogram_ratio=(0.01, 0.01)
    )

    # Customize axis label colors based on omics type
    for label in cg.ax_heatmap.get_xticklabels():
        label_text = label.get_text()
        for omics in omics_order:
            if omics.upper() in label_text:
                label.set_color(omics_colors[omics])
                label.set_rotation(90)
                label.set_fontsize(10)
                break

    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_no_row_cluster_color_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # üîÅ Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_clusterbar = fig.add_subplot(gs[0, 2:45])  # üî∑ Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:45])
    ax = fig.add_subplot(gs[2:14, 2:45])
    ax_curve = fig.add_subplot(gs[2:14, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[6:10, 49])

    # üî∑ Draw column cluster color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        n_clusters = len(unique_clusters)
        palette = sns.color_palette("tab20", n_colors=n_clusters)
        cluster_color_map = {cl: palette[i] for i, cl in enumerate(unique_clusters)}
        cluster_colors = [cluster_color_map[cl] for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Normalize the relevance scores to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 100

    # Default omics colors
    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If there are fewer features, trim names
    feature_names = feature_names[:relevance_scores.shape[1]]
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Column colors for omics (used as a colorbar)
    col_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        color = omics_colors[omics]
        col_colors.extend([color] * (end - start + 1))
    col_colors = col_colors[:relevance_scores.shape[1]]

    # Reorder rows/columns if labels are provided
    if row_labels is not None:
        df = df.iloc[np.argsort(row_labels)]
    if col_labels is not None:
        df = df.iloc[:, np.argsort(col_labels)]
        col_colors = [col_colors[i] for i in np.argsort(col_labels)]


    # Plot clustermap without clustering
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    cg = sns.clustermap(
        df,
        #cmap="Blues",
        cmap=bluish_gray_gradient,
        row_cluster=False,
        col_cluster=False,
        col_colors=col_colors,
        xticklabels=True,
        yticklabels=False,
        figsize=(18, 14),
        cbar_pos=(0.02, 0.8, 0.02, 0.18),
        cbar_kws={"label": "Relevance Score"},
        dendrogram_ratio=(0.01, 0.01)
    )

    # Customize axis label colors based on omics type
    for label in cg.ax_heatmap.get_xticklabels():
        label_text = label.get_text()
        for omics in omics_order:
            if omics.upper() in label_text:
                label.set_color(omics_colors[omics])
                label.set_rotation(90)
                label.set_fontsize(10)
                break

    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_no_col_cluster_color_bar(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap, to_rgba

    # üîπ Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # üîπ Reorder rows by cluster
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 18))
    gs = fig.add_gridspec(nrows=16, ncols=50, wspace=0.0, hspace=0.0)

    ax_row_clusterbar = fig.add_subplot(gs[2:14, 0])     # üîπ Left cluster color bar
    ax_bar            = fig.add_subplot(gs[0, 2:45])     # üîπ Top bar plot
    ax                = fig.add_subplot(gs[2:14, 2:45])   # üîπ Heatmap
    ax_curve          = fig.add_subplot(gs[2:14, 45:48], sharey=ax)  # üîπ Saliency curve
    ax_cbar           = fig.add_subplot(gs[6:10, 49])     # üîπ Colorbar

    # üîπ Top bar plot of feature means
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Row cluster color bar
    unique_clusters = np.unique(sorted_clusters)
    cluster_palette = sns.color_palette("tab20", len(unique_clusters))
    cluster_color_map = {cluster: cluster_palette[i] for i, cluster in enumerate(unique_clusters)}
    cluster_colors = [cluster_color_map[cl] for cl in sorted_clusters]

    ax_row_clusterbar.imshow(np.array(cluster_colors).reshape(-1, 1), aspect='auto')
    ax_row_clusterbar.axis("off")

    # üîπ Feature tick labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)

    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    saliency_sums = saliency_sums[cluster_order]
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    # üîπ Omics mean bar (bottom of heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_group_by_omics(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # üîπ Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # üîπ Reorder rows by cluster
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 18))
    gs = fig.add_gridspec(nrows=20, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_bar        = fig.add_subplot(gs[0, 2:45])       # üîπ Top bar (omics color bar)
    ax_top_means      = fig.add_subplot(gs[1, 2:45])       # üîπ Top bar plot of feature means
    ax_row_clusterbar = fig.add_subplot(gs[2:16, 0])       # üîπ Row cluster color bar
    ax                = fig.add_subplot(gs[2:16, 2:45])     # üîπ Heatmap
    ax_curve          = fig.add_subplot(gs[2:16, 45:48], sharey=ax)  # üîπ Saliency curve
    ax_cbar           = fig.add_subplot(gs[6:10, 49])       # üîπ Colorbar

    # üîπ Omics type color bar (columns)
    ax_col_bar.axis("off")
    ax_col_bar.set_xlim(0, len(feature_names))
    ax_col_bar.set_ylim(0, 1)
    for i, color in enumerate(feature_colors):
        ax_col_bar.bar(
            x=i + 0.5,
            height=1,
            width=1.0,
            color=color,
            edgecolor='black',
            linewidth=0.2,
            align='center'
        )

    # üîπ Top bar plot of feature means
    ax_top_means.axis("off")
    ax_top_means.set_xlim(0, len(feature_names))
    ax_top_means.set_ylim(0, 1.1)

    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_top_means.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Row cluster color bar
    unique_clusters = np.unique(sorted_clusters)
    cluster_palette = sns.color_palette("tab20", len(unique_clusters))
    cluster_color_map = {cluster: cluster_palette[i] for i, cluster in enumerate(unique_clusters)}
    cluster_colors = [cluster_color_map[cl] for cl in sorted_clusters]

    ax_row_clusterbar.imshow(np.array(cluster_colors).reshape(-1, 1), aspect='auto')
    ax_row_clusterbar.axis("off")

    # üîπ Feature tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)

    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    saliency_sums = saliency_sums[cluster_order]
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5,
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    # üîπ Omics mean bar (bottom of heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_not_good(
    args,
    relevance_scores,
    omics_splits,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # üîπ Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    # üîπ Sort rows by cluster
    row_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[row_order]
    sorted_row_clusters = row_labels[row_order]

    # üîπ No column sorting ‚Äî preserve original order
    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    row_labels = np.array(row_labels)[original_col_indices]

    # üîπ Column color map by cluster ID
    unique_col_clusters = np.unique(row_labels)
    col_palette = sns.color_palette("tab10", len(unique_col_clusters))
    col_color_map = {cluster: col_palette[i] for i, cluster in enumerate(unique_col_clusters)}
    column_colors = [col_color_map[c] for c in row_labels]

    # üîπ Row cluster colors
    unique_row_clusters = np.unique(sorted_row_clusters)
    row_palette = sns.color_palette("tab20", len(unique_row_clusters))
    row_color_map = {cluster: row_palette[i] for i, cluster in enumerate(unique_row_clusters)}
    row_colors = [row_color_map[c] for c in sorted_row_clusters]

    # üîπ Column cluster color bar (top of heatmap)
    if col_labels is not None:
        col_labels = np.array(col_labels)
        unique_col_clusters = np.unique(col_labels)
        col_palette = sns.color_palette("tab10", len(unique_col_clusters))
        col_color_map = {cluster: col_palette[i] for i, cluster in enumerate(unique_col_clusters)}
        column_cluster_colors = [col_color_map[c] for c in col_labels]
    else:
        column_cluster_colors = ['#cccccc'] * relevance_scores.shape[1]  # default gray

    fig = plt.figure(figsize=(18, 18))
    gs = fig.add_gridspec(nrows=20, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_bar        = fig.add_subplot(gs[0, 2:45])       # üîπ Column cluster color bar
    ax_top_means      = fig.add_subplot(gs[1, 2:45])       # üîπ Top feature mean bar
    ax_row_clusterbar = fig.add_subplot(gs[2:16, 0])       # üîπ Row cluster color bar
    ax                = fig.add_subplot(gs[2:16, 2:45])    # üîπ Heatmap
    ax_curve          = fig.add_subplot(gs[2:16, 45:48], sharey=ax)  # üîπ Saliency curve
    ax_cbar           = fig.add_subplot(gs[6:10, 49])      # üîπ Colorbar


    # üîπ Column cluster color bar
    ax_col_bar.imshow([column_cluster_colors], aspect='auto')
    ax_col_bar.axis("off")

    # üîπ Feature mean bar (same as before)
    ax_top_means.axis("off")
    ax_top_means.set_xlim(0, len(column_colors))
    ax_top_means.set_ylim(0, 1.1)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, column_colors)):
        ax_top_means.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    # üîπ Heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Row cluster color bar
    ax_row_clusterbar.imshow(np.array(row_colors).reshape(-1, 1), aspect='auto')
    ax_row_clusterbar.axis("off")

    # üîπ Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    saliency_sums = saliency_sums[row_order]
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5,
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_only_top_cluster_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # üîÅ Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_clusterbar = fig.add_subplot(gs[0, 2:45])  # üî∑ Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:45])
    ax = fig.add_subplot(gs[2:14, 2:45])
    ax_curve = fig.add_subplot(gs[2:14, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[6:10, 49])

    # üî∑ Draw column cluster color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        n_clusters = len(unique_clusters)
        palette = sns.color_palette("tab20", n_colors=n_clusters)
        cluster_color_map = {cl: palette[i] for i, cl in enumerate(unique_clusters)}
        cluster_colors = [cluster_color_map[cl] for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # üîÅ Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=52, wspace=0.0, hspace=0.0)

    ax_row_clusterbar = fig.add_subplot(gs[2:14, 0])         # ‚¨ÖÔ∏è Row cluster color bar
    ax_col_clusterbar = fig.add_subplot(gs[0, 2:47])         # üî∑ Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:47])                    # Bar chart
    ax = fig.add_subplot(gs[2:14, 2:47])                     # Heatmap
    ax_curve = fig.add_subplot(gs[2:14, 47:50], sharey=ax)   # Right saliency curve
    ax_cbar = fig.add_subplot(gs[6:10, 51])                  # Colorbar

    # üî∑ Draw column cluster color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        n_clusters = len(unique_clusters)
        palette = sns.color_palette("tab20", n_colors=n_clusters)
        cluster_color_map = {cl: palette[i] for i, cl in enumerate(unique_clusters)}
        cluster_colors = [cluster_color_map[cl] for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")

    # üî∑ Draw row cluster color bar
    if row_labels is not None:
        unique_row_clusters = np.unique(row_labels)
        n_row_clusters = len(unique_row_clusters)
        row_palette = sns.color_palette("tab20", n_colors=n_row_clusters)
        row_cluster_color_map = {cl: row_palette[i] for i, cl in enumerate(unique_row_clusters)}
        row_cluster_colors = [row_cluster_color_map[cl] for cl in row_labels[row_order]]
        ax_row_clusterbar.imshow(np.array(row_cluster_colors).reshape(-1, 1), aspect='auto')
        ax_row_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()


def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    print("üß™ Running Spectral Biclustering with fixed (10, 7) clusters...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Biclustering
    n_clusters_row = 16
    n_clusters_col = 10
    #n_clusters=(5,5)

    best_model = None
    best_score = np.inf

    print("üîÅ Running biclustering trials:")
    for i in range(10):  # or use tqdm
        model = SpectralBiclustering(n_clusters=(n_clusters_row,n_clusters_col), method='bistochastic',
                             svd_method='randomized', random_state=0
                            )
        model.fit(summary_bio_features)

        reconstructed = summary_bio_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # Assign cluster labels to top-k nodes
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)



    plot_bio_biclustering_heatmap(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )
    
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features,#relevance_scores_topk.detach().cpu().numpy(),
        #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
        #gene_names=node_names_topk,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )
    
    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap_unsort_not_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Custom cluster colors
    CLUSTER_COLORS = {
        0: '#0077B6',   1: '#0000FF',   2: '#00B4D8',   3: '#48EAC4',
        4: '#F1C0E8',   5: '#B9FBC0',   6: '#32CD32',   7: '#bee1e6',
        8: '#8A2BE2',   9: '#E377C2',  10: '#8EECF5',  11: '#A3C4F3',
        12: '#FFB347', 13: '#FFD700',  14: '#FF69B4',  15: '#CD5C5C',
        16: '#7FFFD4', 17: '#FF7F50',  18: '#C71585',  19: '#20B2AA'
    }

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # üîÅ Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=52, wspace=0.0, hspace=0.0)

    ax_row_clusterbar = fig.add_subplot(gs[2:14, 0])         # ‚¨ÖÔ∏è Row cluster color bar
    ax_col_clusterbar = fig.add_subplot(gs[0, 2:47])         # üî∑ Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:47])                    # Bar chart
    ax = fig.add_subplot(gs[2:14, 2:47])                     # Heatmap
    ax_curve = fig.add_subplot(gs[2:14, 47:50], sharey=ax)   # Right saliency curve
    ax_cbar = fig.add_subplot(gs[6:10, 51])                  # Colorbar


    if col_labels is not None:
        cluster_colors = [to_rgb(CLUSTER_COLORS.get(cl, '#808080')) for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")



    # ‚¨ÖÔ∏è Row cluster color bar
    if row_labels is not None:
        row_cluster_colors = [CLUSTER_COLORS.get(int(cl), '#000000') for cl in row_labels[row_order]]
        ax_row_clusterbar.imshow(np.array(row_cluster_colors).reshape(-1, 1), aspect='auto')
        ax_row_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None,
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Define cluster colors
    CLUSTER_COLORS = {
        0: '#0077B6', 1: '#0000FF', 2: '#00B4D8', 3: '#48EAC4', 4: '#F1C0E8',
        5: '#B9FBC0', 6: '#32CD32', 7: '#bee1e6', 8: '#8A2BE2', 9: '#E377C2',
        10: '#8EECF5', 11: '#A3C4F3', 12: '#FFB347', 13: '#FFD700', 14: '#FF69B4',
        15: '#CD5C5C', 16: '#7FFFD4', 17: '#FF7F50', 18: '#C71585', 19: '#20B2AA'
    }

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min() + 1e-8) * 20

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Reorder based on provided labels
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
        row_labels = np.array(row_labels)[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    # Cluster colors for rows
    if row_labels is not None:
        row_color_labels = [CLUSTER_COLORS[int(lbl)] for lbl in row_labels]
    else:
        row_color_labels = None

    # Column color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        col_cluster_color_map = {cl: CLUSTER_COLORS[cl % len(CLUSTER_COLORS)] for cl in unique_clusters}
        col_color_labels = [col_cluster_color_map[cl] for cl in col_labels]
    else:
        col_color_labels = feature_colors

    # Create dataframe for clustermap
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Plot clustermap without clustering
    g = sns.clustermap(
        df,
        row_cluster=False,
        col_cluster=False,
        row_colors=row_color_labels,
        col_colors=col_color_labels,
        cmap="Blues",
        xticklabels=False,
        yticklabels=False,
        figsize=(18, 17),
        cbar_kws={"label": "Relevance Score"},
    )

    # Color bar ticks
    g.cax.yaxis.label.set_size(18)
    g.cax.tick_params(labelsize=14)

    # Optional: save and show
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_x(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )


    # Normalize again just before plotting (optional)
    sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min() + 1e-6)

    # Create DataFrame for better label control
    import pandas as pd
    df_scores = pd.DataFrame(sorted_scores, index=[f"Gene {i}" for i in ordered_row_indices], columns=feature_names)

    # Create column colors based on omics types
    col_colors = pd.Series(feature_colors, index=df_scores.columns)

    # Generate clustermap
    g = sns.clustermap(
        df_scores,
        row_cluster=False,
        col_cluster=False,
        col_colors=col_colors,
        cmap=bluish_gray_gradient,
        xticklabels=True,
        yticklabels=False,
        figsize=(18, 14),
        vmin=vmin,
        vmax=vmax,
        cbar_kws={"label": "Relevance Score"},
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_predicted_genes_distribution(pred_counts, output_path):
    """
    Visualize the number of predicted cancer genes in each cluster.

    Args:
        pred_counts (dict): Dictionary mapping cluster_id -> number of predicted genes
        output_path (str): Path to save the plot
    """
    clusters = list(pred_counts.keys())
    counts = [pred_counts[c] for c in clusters]
    colors = [CLUSTER_COLORS.get(c, "#808080") for c in clusters]

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(clusters, counts, color=colors, edgecolor='black')

    ax.set_title("Predicted Cancer Genes per Cluster", fontsize=16)
    ax.set_xlabel("Cluster ID", fontsize=16)
    ax.set_ylabel("Number of Predicted Genes", fontsize=16)
    ax.set_xticks(clusters)
    ax.set_xticklabels(clusters, rotation=0, fontsize=12)
    ax.tick_params(axis='y', labelsize=12)

    # Add value labels on top
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax.annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=10)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_clustermap_pass(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None,
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Define cluster colors
    CLUSTER_COLORS = {
        0: '#0077B6', 1: '#0000FF', 2: '#00B4D8', 3: '#48EAC4', 4: '#F1C0E8',
        5: '#B9FBC0', 6: '#32CD32', 7: '#bee1e6', 8: '#8A2BE2', 9: '#E377C2',
        10: '#8EECF5', 11: '#A3C4F3', 12: '#FFB347', 13: '#FFD700', 14: '#FF69B4',
        15: '#CD5C5C', 16: '#7FFFD4', 17: '#FF7F50', 18: '#C71585', 19: '#20B2AA'
    }

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min() + 1e-8) * 20

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Reorder based on provided labels
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
        row_labels = np.array(row_labels)[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    # Cluster colors for rows
    if row_labels is not None:
        row_color_labels = [CLUSTER_COLORS[int(lbl)] for lbl in row_labels]
    else:
        row_color_labels = None

    # Column color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        col_cluster_color_map = {cl: CLUSTER_COLORS[cl % len(CLUSTER_COLORS)] for cl in unique_clusters}
        col_color_labels = [col_cluster_color_map[cl] for cl in col_labels]
    else:
        col_color_labels = feature_colors

    # Create dataframe for clustermap
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Plot clustermap without clustering
    g = sns.clustermap(
        df,
        row_cluster=False,
        col_cluster=False,
        row_colors=row_color_labels,
        col_colors=col_color_labels,
        cmap="Blues",
        xticklabels=False,
        yticklabels=False,
        figsize=(18, 17),
        cbar_kws={"label": "Relevance Score"},
    )

    # Color bar ticks
    g.cax.yaxis.label.set_size(18)
    g.cax.tick_params(labelsize=14)

    # Optional: save and show
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Construct DataFrame with optional gene and feature labels
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]
    col_feature_names = feature_names

    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=col_feature_names)

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix saved to {csv_output_path}")

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def apply_full_spectral_biclustering_bio_(
    graph, summary_bio_features, node_names, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    import numpy as np
    import torch
    from sklearn.metrics import mean_squared_error
    from sklearn.cluster import SpectralBiclustering

    print("üß™ Running Spectral Biclustering with fixed (16, 10) clusters...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"
    assert topk_node_indices is not None, "`topk_node_indices` must be provided"

    n_clusters_row, n_clusters_col = 10, 6
    best_model, best_score = None, np.inf

    print("üîÅ Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col),
                                     method='bistochastic',
                                     svd_method='randomized',
                                     random_state=i)
        model.fit(summary_bio_features)

        reconstructed = summary_bio_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    assert len(topk_node_indices) == len(row_labels), "Mismatch between top-k indices and row_labels"

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    # Generate visualizations with distinct paths
    plot_bio_biclustering_heatmap(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap.replace(".png", "_heatmap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap.replace(".png", "_unsorted.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap.replace(".png", "_clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names_topk, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    # import torch
    # import numpy as np
    # from sklearn.metrics import mean_squared_error
    # from sklearn.cluster import SpectralBiclustering
    # from utils import (  # Replace with actual locations if needed
    #     save_graph_with_clusters, save_row_labels, compute_total_genes_per_cluster,
    #     save_total_genes_per_cluster, count_predicted_genes_per_cluster, save_predicted_counts
    # )
    # from plotting import (
    #     plot_bio_biclustering_heatmap_unsort,
    #     plot_bio_biclustering_clustermap,
    #     plot_predicted_genes_distribution
    # )
    # import os

    print("üß™ Running Spectral Biclustering with fixed (16, 10) clusters...")

    # === ‚úÖ Step 1: Filter top-k nodes
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    ##summary_bio_features_topk = summary_bio_features[topk_node_indices]
    summary_bio_features_topk = summary_bio_features  # Already top-k
    #node_names_topk = node_names
    #node_names_topk = [node_names[i] for i in topk_node_indices]

    assert summary_bio_features_topk.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_topk.shape[1]}"

    # === ‚úÖ Step 2: Run Biclustering
    n_clusters_row = 10
    n_clusters_col = 5

    best_model = None
    best_score = np.inf

    print("üîÅ Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='bistochastic',
                                     svd_method='randomized', random_state=i)
        
        model.fit(summary_bio_features_topk)

        reconstructed = summary_bio_features_topk[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features_topk, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === ‚úÖ Step 3: Assign row cluster labels back to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering complete.")

    # === ‚úÖ Step 4: Save clustering outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === ‚úÖ Step 5: Plot heatmaps and distributions
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_topo_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    

    """
    Plots a spectral biclustering heatmap for topological embeddings (1024‚Äì2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # üîπ Extract 64D summary of topological features
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # üîπ Create topo feature names (01‚Äì64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # üîπ Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # üîπ Sort by cluster ‚Üí then by gene-wise relevance within cluster
    ordered_row_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        ordered_row_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in ordered_row_indices]

    # üîπ Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # üîπ Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    
    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    # cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    # os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")


    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # üîπ Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # üîπ Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # üîπ Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # üîπ Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # üîπ Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # üîπ Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved spectral clustering heatmap to {output_path}")

    # üîπ Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_bio_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    # cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    # os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # üîπ Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_novel_predicted_cancer_genes(
    clusters,
    novel_predicted_cancer_genes,
    total_genes_per_cluster,
    node_names,
    row_labels,
    output_path):
    """
    Plots the percentage of novel predicted cancer genes per cluster.
    """

    # Convert to array and sort clusters
    clusters = np.array(sorted(total_genes_per_cluster.keys()))
    total_genes_array = np.array([total_genes_per_cluster[c] for c in clusters])

    # Count novel predicted genes per cluster
    cluster_to_novel_count = {c: 0 for c in clusters}
    for i, name in enumerate(node_names):
        if name in novel_predicted_cancer_genes:
            cluster = row_labels[i]
            cluster_to_novel_count[cluster] += 1

    # Get counts in cluster order
    predicted_counts = np.array([cluster_to_novel_count.get(c, 0) for c in clusters])
    percent_predicted = np.divide(predicted_counts, total_genes_array, where=total_genes_array > 0)

    # Prepare bar colors
    colors = [CLUSTER_COLORS.get(c, '#333333') for c in clusters]

    # Plot
    fig, ax = plt.subplots(figsize=(8, 5))
    bars = ax.bar(clusters, percent_predicted, color=colors, edgecolor='black')

    # Annotate bars
    for bar, cluster_id in zip(bars, clusters):
        height = bar.get_height()
        count = cluster_to_novel_count.get(cluster_id, 0)
        ax.text(bar.get_x() + bar.get_width() / 2, height, str(count),
                ha='center', va='bottom', fontsize=16, fontweight='bold')

    ax.set_xlim(-0.55, len(clusters) - 0.65)
    ax.set_ylabel("Percent of NPCGs", fontsize=20)
    plt.xticks(clusters, fontsize=16)
    plt.yticks(fontsize=16)
    ax.set_ylim(0, max(percent_predicted) + 0.1)

    sns.despine()
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"‚úÖ Novel PCG plot saved to {output_path}")

def process_cluster_results_(
    tag,
    graph,
    row_labels,
    top_gene_indices,
    predicted_counts,
    total_genes_per_cluster,
    relevance_scores_subset,
    node_names,
    node_names_topk,
    ground_truth_cancer_genes,
    name_to_index,
    output_dir,
    args
):
    # Assign cluster labels to graph
    graph.ndata[f'cluster_{tag}'] = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    graph.ndata[f'cluster_{tag}'][top_gene_indices] = torch.tensor(row_labels, dtype=torch.long)

    # Check cluster size condition
    cluster_to_genes = {}
    for idx, label in zip(top_gene_indices, row_labels):
        cluster_to_genes.setdefault(label, []).append(idx)

    if not cluster_to_genes or not all(len(indices) >= 10 for indices in cluster_to_genes.values()):
        print(f"‚ö†Ô∏è Skipping cluster analysis for tag='{tag}': some clusters have <10 genes.")
        return

    # Visualize UMAP/t-SNE embeddings
    G_tmp = graph.to_networkx(node_attrs=[f'cluster_{tag}'])
    plot_relevance_tsne_umap(
        relevance_scores_subset,
        G_tmp,
        cluster_key=f'cluster_{tag}',
        method='umap',
        title_suffix=f"({tag.capitalize()})"
    )

    # Plot predicted PCG % bar chart
    plot_pcg_cancer_genes(
        clusters=range(len(predicted_counts)),
        predicted_cancer_genes_count=predicted_counts,
        total_genes_per_cluster=total_genes_per_cluster,
        node_names=node_names_topk,
        row_labels=row_labels,
        output_path=os.path.join(
            output_dir,
            f'{args.model_type}_{args.net_type}_pcg_percent_{tag}_epo{args.num_epochs}.png'
        )
    )

    # Prepare data for interactions
    row_labels_np = np.array(row_labels)
    degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

    gt_indices = set(name_to_index[name] for name in ground_truth_cancer_genes if name in name_to_index)
    kcg_nodes = [i for i, name in enumerate(node_names_topk) if name in ground_truth_cancer_genes]

    kcg_data = pd.DataFrame({
        "Cluster": row_labels_np[kcg_nodes],
        "Interactions": degrees_np[top_gene_indices][kcg_nodes]
    })

    pcg_data = pd.DataFrame({
        "Cluster": row_labels_np,
        "Interactions": degrees_np[top_gene_indices]
    })

    plot_interactions_with_kcgs(
        kcg_data,
        os.path.join(output_dir, f"{args.model_type}_{args.net_type}_kcgs_interaction_{tag}_epo{args.num_epochs}.png")
    )

    plot_interactions_with_pcgs(
        pcg_data,
        os.path.join(output_dir, f"{args.model_type}_{args.net_type}_pcgs_interaction_{tag}_epo{args.num_epochs}.png")
    )

    # Count KCGs per cluster
    kcg_counts = {
        i: sum((row_labels_np == i) & np.isin(range(len(row_labels)), list(gt_indices)))
        for i in range(len(predicted_counts))
    }

    # Plot % KCGs per cluster
    plot_kcg_cancer_genes(
        clusters=range(len(predicted_counts)),
        kcg_count=kcg_counts,
        total_genes_per_cluster=total_genes_per_cluster,
        node_names=node_names_topk,
        row_labels=row_labels,
        output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcg_percent_{tag}_epo{args.num_epochs}.png')
    )

    # Optionally, add tag-specific extra logic
    if tag == 'bio':
        print("‚úÖ Finished BIO-specific cluster visualizations.")
        # Add any extra bio-specific logic here if needed
    elif tag == 'topo':
        print("‚úÖ Finished TOPO-specific cluster visualizations.")
        # Add any extra topo-specific logic here if needed

def plot_bio_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    row_labels=None,
    col_labels=None,
):
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Grid layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])       # top bar
    ax        = fig.add_subplot(gs[1:13, 2:45])     # main heatmap
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)  # saliency curve
    ax_cbar   = fig.add_subplot(gs[5:9, 49])       # colorbar

    # Top feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)


    # Saliency curve 
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars (right below feature bar, above heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    row_labels=None,
    col_labels=None,
):  
    
    # üîπ Extract and normalize relevance scores
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # üîπ Feature color mapping
    '''feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color'''

    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    # Normalize per-feature means
    #feature_means = relevance_scores.mean(axis=0)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Sort by cluster only (no intra-cluster sorting)
    '''cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]'''

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1, 
            linewidth=0, 
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), 
            clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # üîπ Add cluster size labels
    # for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
    #     ax.text(
    #         -2.0, center_y, f"{count}",
    #         va='center', ha='right', fontsize=18, fontweight='bold'
    #     )
        
    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    '''mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )'''

    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )
        
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    
    # üîπ Extract and normalize relevance scores
    # relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # Build feature color bar
    feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    # Normalize per-feature means
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Sort by cluster only (no intra-cluster sorting)
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=16)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # LRP curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_kcg(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None,
    kcg_list=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # ‚û§ Filter only known cancer genes
    if kcg_list is not None:
        kcg_mask = [name in kcg_list for name in row_gene_names]
        sorted_scores = sorted_scores[kcg_mask]
        sorted_clusters = sorted_clusters[kcg_mask]
        row_gene_names = [name for name, keep in zip(row_gene_names, kcg_mask) if keep]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    # cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    # os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # üîπ Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_bio_biclustering_heatmap_npcg(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None,
    kcg_list=None,                  # List of known cancer gene symbols
    plot_only_novel=True 
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)


    # Construct row labels: use gene names if available, else fallback
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # ‚û§ Filter to plot only novel predicted cancer genes
    if kcg_list is not None and plot_only_novel:
        novel_mask = [name not in kcg_list for name in row_gene_names]
        sorted_scores = sorted_scores[novel_mask]
        sorted_clusters = sorted_clusters[novel_mask]
        row_gene_names = [name for name, keep in zip(row_gene_names, novel_mask) if keep]


    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    # cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    # os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # üîπ Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_topo_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    

    """
    Plots a spectral biclustering heatmap for topological embeddings (1024‚Äì2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # üîπ Extract 64D summary of topological features
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # üîπ Create topo feature names (01‚Äì64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # üîπ Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # üîπ Sort by cluster ‚Üí then by gene-wise relevance within cluster
    ordered_row_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        ordered_row_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in ordered_row_indices]

    # üîπ Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # üîπ Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    
    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    # cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    # os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")


    # üîπ Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # üîπ Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # üîπ Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # üîπ Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # üîπ Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # üîπ Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # üîπ Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # üîπ Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # üîπ Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # üîπ Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # üîπ X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # üîπ Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # üîπ Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # üîπ Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úÖ Saved spectral clustering heatmap to {output_path}")

    # üîπ Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names_topk, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    # import torch
    # import numpy as np
    # from sklearn.metrics import mean_squared_error
    # from sklearn.cluster import SpectralBiclustering
    # from utils import (  # Replace with actual locations if needed
    #     save_graph_with_clusters, save_row_labels, compute_total_genes_per_cluster,
    #     save_total_genes_per_cluster, count_predicted_genes_per_cluster, save_predicted_counts
    # )
    # from plotting import (
    #     plot_bio_biclustering_heatmap_unsort,
    #     plot_bio_biclustering_clustermap,
    #     plot_predicted_genes_distribution
    # )
    # import os

    print("üß™ Running Spectral Biclustering with fixed (16, 10) clusters...")

    # === ‚úÖ Step 1: Filter top-k nodes
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    ##summary_bio_features_topk = summary_bio_features[topk_node_indices]
    summary_bio_features_topk = summary_bio_features  # Already top-k
    #node_names_topk = node_names
    #node_names_topk = [node_names[i] for i in topk_node_indices]

    assert summary_bio_features_topk.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_topk.shape[1]}"

    # === ‚úÖ Step 2: Run Biclustering
    n_clusters_row = 10
    n_clusters_col = 5

    best_model = None
    best_score = np.inf

    print("üîÅ Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='bistochastic',
                                     svd_method='randomized', random_state=i)
        
        model.fit(summary_bio_features_topk)

        reconstructed = summary_bio_features_topk[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features_topk, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === ‚úÖ Step 3: Assign row cluster labels back to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering complete.")

    # === ‚úÖ Step 4: Save clustering outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === ‚úÖ Step 5: Plot heatmaps and distributions
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    # cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    # os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # Ridge-style violin plot for relevance score distributions
    # df_ridge = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)
    # df_ridge['Cluster'] = sorted_clusters
    # df_ridge['Gene'] = df_ridge.index
    # df_ridge_melt = df_ridge.melt(id_vars=["Cluster", "Gene"], var_name="Feature", value_name="Score")

    # ridge_fig = plt.figure(figsize=(12, 6))
    # sns.violinplot(
    #     data=df_ridge_melt,
    #     x="Score",
    #     y="Cluster",
    #     scale="width",
    #     inner="quartile",
    #     linewidth=0.6,
    #     palette="coolwarm",
    #     cut=0
    # )
    # plt.title("Ridge-style Distribution of Relevance Scores by Cluster")
    # plt.xlabel("Relevance Score")
    # plt.ylabel("Cluster")
    # plt.tight_layout()

    # ridge_output_path = output_path.replace(".png", "_ridgeplot.png")
    # ridge_fig.savefig(ridge_output_path, dpi=300)
    # plt.close()
    # print(f"Ridge plot saved to {ridge_output_path}")

    # Ridge-style violin plots per omics type
    for omics_type, (start_idx, end_idx) in new_omics_splits.items():
        omics_features = feature_names[start_idx:end_idx + 1]
        df_omics = pd.DataFrame(sorted_scores[:, start_idx:end_idx + 1], columns=omics_features)
        df_omics['Cluster'] = sorted_clusters
        df_omics['Gene'] = row_gene_names
        df_omics_melt = df_omics.melt(id_vars=["Cluster", "Gene"], var_name="Feature", value_name="Score")

        fig_omics = plt.figure(figsize=(12, 6))
        sns.violinplot(
            data=df_omics_melt,
            x="Score",
            y="Cluster",
            scale="width",
            inner="quartile",
            linewidth=0.6,
            palette="viridis",
            cut=0
        )
        plt.title(f"Ridge-style Distribution of {omics_type.upper()} Relevance Scores")
        plt.xlabel("Relevance Score")
        plt.ylabel("Cluster")
        plt.tight_layout()

        omics_plot_path = output_path.replace(".png", f"_ridgeplot_{omics_type}.png")
        fig_omics.savefig(omics_plot_path, dpi=300)
        plt.close()
        print(f"Saved {omics_type.upper()} ridge plot to {omics_plot_path}")

    # üîπ Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def save_and_plot_enriched_pathways(enrichment_results, args, output_dir):
    # === Prepare Data ===
    heatmap_data = pd.DataFrame()

    for cluster_type in ['bio', 'topo']:
        for cid, df in enrichment_results[cluster_type].items():
            colname = f"{cluster_type.capitalize()}_{cid}"
            vals = {}
            for _, row in df.iterrows():
                p = row['p_value']
                name = row['name']
                if p < 0.05 and len(name) <= 60:
                    term = f"{name} ({row['source']})"
                    vals[term] = -np.log10(p)
            heatmap_data[colname] = pd.Series(vals)

    # Clean and filter
    heatmap_data = heatmap_data.fillna(0)
    heatmap_data = heatmap_data[heatmap_data.max(axis=1) > 1]

    # Save full enrichment data to CSV
    enrichment_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enrichment_matrix_epo{args.num_epochs}.csv"
    )
    heatmap_data.to_csv(enrichment_csv_path, index_label='Enriched Pathway')

    # === Save Topo Cluster ‚Üí Top Enriched Terms to CSV ===
    topo_terms = []
    for cid, df in enrichment_results['topo'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                topo_terms.append({
                    "Cluster": f"Topo_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    topo_terms_df = pd.DataFrame(topo_terms)
    topo_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    topo_terms_df.to_csv(topo_terms_path, index=False)

    # === Save Bio Cluster ‚Üí Top Enriched Terms to CSV ===
    bio_terms = []
    for cid, df in enrichment_results['bio'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                bio_terms.append({
                    "Cluster": f"Bio_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    bio_terms_df = pd.DataFrame(bio_terms)
    bio_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    bio_terms_df.to_csv(bio_terms_path, index=False)

    # === Select 50 evenly spaced rows for plotting ===
    if heatmap_data.shape[0] > 50:
        step = max(1, heatmap_data.shape[0] // 50)
        selected_indices = heatmap_data.index[::step][:50]
        heatmap_data = heatmap_data.loc[selected_indices]

    # === Normalize for color contrast ===
    norm_data = heatmap_data.copy()
    norm_data = norm_data / norm_data.max().replace(0, 1)

    # === Apply group-wise colormaps ===
    colormaps = {
        'bio': get_cmap('Blues'),
        'topo': get_cmap('YlOrRd'),
    }

    colors = np.zeros((heatmap_data.shape[0], heatmap_data.shape[1], 4))  # RGBA
    col_types = []

    for i, col in enumerate(norm_data.columns):
        group = 'bio' if col.lower().startswith("bio") else 'topo'
        col_types.append(group)
        cmap = colormaps[group]
        colors[:, i, :] = cmap(norm_data[col].values)

    # === Plot ===
    fig, ax = plt.subplots(figsize=(0.5 * len(norm_data.columns), 0.2 * len(norm_data)))

    ax.imshow(colors, aspect='auto')
    ax.set_xticks(np.arange(len(norm_data.columns)))
    ax.set_xticklabels(norm_data.columns, rotation=90, fontsize=13)
    ax.set_yticks(np.arange(len(norm_data.index)))
    ax.set_yticklabels(norm_data.index, fontsize=16)
    ax.set_ylabel("Enriched Pathway", fontsize=18, labelpad=20)

    # Color x-axis labels
    for xtick, col in zip(ax.get_xticklabels(), col_types):
        xtick.set_color('darkblue' if col == 'bio' else 'darkred')

    ax.set_title("Top Enriched Pathways per Cluster (p < 0.05)", fontsize=16, pad=16)
    ax.set_xlabel("Cluster", fontsize=16)

    legend_patches = [
        Patch(color='cornflowerblue', label='Bio'),
        Patch(color='salmon', label='Topo')
    ]
    fig.legend(handles=legend_patches, loc='lower center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))

    sns.despine(ax=ax, trim=True)
    ax.tick_params(axis='both', which='both', length=0)
    plt.tight_layout(rect=[0, 0, 0.95, 0.93])

    # Save plot
    enriched_terms_heatmap_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enriched_terms_heatmap_epo{args.num_epochs}.png"
    )
    plt.savefig(enriched_terms_heatmap_path, dpi=300)
    plt.close()

    # === Return DataFrames for downstream analysis ===
    return heatmap_data, topo_terms_df, bio_terms_df

def plot_gene_feature_contributions_topo_(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")

    # Barplot of all 64 topo features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_topo(df, str(barplot_path))

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()


    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=16)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=16)
    if isinstance(score, np.ndarray):
        score = score.item()

    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=16)
    plt.title(f"{gene_name}", fontsize=16)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_bio_(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")


    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_bio(df, str(barplot_path))

    # Prepare for heatmap
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=16)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=16)
    if isinstance(score, np.ndarray):
        score = score.item()

    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=16)
    plt.title(f"{gene_name}", fontsize=16)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_bio(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")


    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_bio(df, str(barplot_path))
    
    # Barplot of all 64 features
    # df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    # barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    # plot_omics_barplot_bio(df, barplot_path)

    # Prepare for heatmap
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    # Handle gene name and score
    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_topo(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics √ó 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")


    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_topo(df, str(barplot_path))

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    # Capitalize the first letter of each y-tick (cancer name)
    plt.yticks(
        ticks=plt.yticks()[0], 
        labels=[label.get_text().capitalize() for label in plt.gca().get_yticklabels()],
        rotation=0, fontsize=10
    )

    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_collapsed_clusterfirst_multilevel_sankey_bio_(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):


    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break
    confirmed_genes = combined_genes

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = defaultdict(list)
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue
        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]
        cluster_label = f"Cluster {gene_cluster}"
        cluster_to_genes[cluster_label].append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score
        neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

    source, target, value, link_colors = [], [], [], []

    for cluster_label, genes in cluster_to_genes.items():
        if not genes:
            continue
        cluster_id = int(cluster_label.split()[-1])
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                color = CLUSTER_COLORS.get(row_labels[rel_idx], "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)
                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]
            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(cluster_id, "#000000"), 0.4))

            for neighbor_idx, neighbor_score in gene_to_neighbors[gene].items():
                neighbor_name = node_id_to_name[neighbor_idx]
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)
                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)
                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Sankey saved to: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è PNG export failed: {e}")

    # === Sankey Structural Analysis
    sankey_stats = analyze_sankey_structure(
        source, target, value,
        label_to_idx, node_names,
        cluster_to_genes, gene_to_neighbors,
        row_labels, name_to_index, scores
    )

    # === Entropy Plot
    entropy_df = pd.DataFrame(list(sankey_stats["cluster_entropy"].items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === Centrality Plot
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_collapsed_clusterfirst_multilevel_sankey_bio_pa(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BCL6", "CUL1", "CDK4", "E2F3", "FOXM1", "SKP2", "BRCA2", "HDAC2", "ETS1", "ATR"]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break

    # Ensure genes have neighbors
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)
    combined_genes = [gene for gene in combined_genes if neighbors_dict.get(gene)]  # üî• filter out genes without neighbors
    confirmed_genes = combined_genes

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            neighbors = gene_to_neighbors.get(gene, {})

            # ‚úÖ Avoid self-loop if no neighbors
            if neighbors:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

                for neighbor_idx, neighbor_score in neighbors.items():
                    neighbor_name = node_id_to_name[neighbor_idx]
                    neighbor_cluster = row_labels[neighbor_idx]
                    neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                    if neighbor_name not in label_to_idx:
                        label_to_idx[neighbor_name] = len(all_labels)
                        all_labels.append(neighbor_name)

                        saliency = relevance_scores[neighbor_idx].sum().item()
                        color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                        all_colors.append(color)
                        font_sizes.append(16 if saliency > 0.5 else 10)

                        if saliency > 0.5:
                            highlight_node_indices.append(label_to_idx[neighbor_name])
                            highlight_node_saliency.append(saliency)

                    neighbor_node_idx = label_to_idx[neighbor_name]

                    if neighbor_cluster_label not in label_to_idx:
                        label_to_idx[neighbor_cluster_label] = len(all_labels)
                        all_labels.append(neighbor_cluster_label)
                        all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                        font_sizes.append(18)

                    neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")

                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_gene_neighbor_cluster_sankey_only(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 12 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    combined_genes = []
    seen = set()
    for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)
    combined_genes = [gene for gene in combined_genes if neighbors_dict.get(gene)]
    confirmed_genes = combined_genes

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    source = []
    target = []
    value = []
    link_colors = []

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue
        gene_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[gene_idx]

        if gene not in label_to_idx:
            label_to_idx[gene] = len(all_labels)
            all_labels.append(gene)
            saliency = relevance_scores[gene_idx].sum().item()
            color = CLUSTER_COLORS.get(gene_cluster, "#000000")
            all_colors.append(color)
            font_sizes.append(18 if saliency > 0.5 else 10)
            if saliency > 0.5:
                highlight_node_indices.append(label_to_idx[gene])
                highlight_node_saliency.append(saliency)

        gene_node_idx = label_to_idx[gene]

        for neighbor_name in neighbors_dict.get(gene, []):
            if neighbor_name not in topk_name_to_index:
                continue
            neighbor_idx = topk_name_to_index[neighbor_name]
            neighbor_cluster = row_labels[neighbor_idx]
            saliency = relevance_scores[neighbor_idx].sum().item()

            if neighbor_name not in label_to_idx:
                label_to_idx[neighbor_name] = len(all_labels)
                all_labels.append(neighbor_name)
                color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(16 if saliency > 0.5 else 10)
                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[neighbor_name])
                    highlight_node_saliency.append(saliency)

            neighbor_node_idx = label_to_idx[neighbor_name]

            cluster_label = f"nCluster {neighbor_cluster}"
            if cluster_label not in label_to_idx:
                label_to_idx[cluster_label] = len(all_labels)
                all_labels.append(cluster_label)
                all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                font_sizes.append(18)

            cluster_node_idx = label_to_idx[cluster_label]

            rel_score = relevance_scores[neighbor_idx].sum().item()

            # Gene ‚Üí Neighbor
            source.append(gene_node_idx)
            target.append(neighbor_node_idx)
            value.append(rel_score)
            link_colors.append("rgba(160,160,160,0.5)")

            # Neighbor ‚Üí Cluster
            source.append(neighbor_node_idx)
            target.append(cluster_node_idx)
            value.append(rel_score)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_gene_neighbor_cluster_sankey_only/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_gene_neighbor_cluster_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Gene-Neighbor-Cluster Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_path = save_path.replace(".html", ".png")
        fig.write_image(png_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

def plot_collapsed_clusterfirst_multilevel_sankey_bio_novel(
    args,
    graph,
    node_names,
    name_to_index,
    novel_predicted_genes, #confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        novel_predicted_genes,  # or confirmed_genes
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Exclude MED subunits
    top_scored_genes = [
        g for g in top_scored_genes
        if not g.startswith("MED")
    ]


    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    
    selected_known_genes = ["BRCA2", "HDAC2"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    # for g in selected_novel_genes + selected_known_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 20:
            break


    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_collapsed_clusterfirst_multilevel_sankey_topo(
    args,
    graph,
    node_names,
    name_to_index,
    novel_predicted_genes, #confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):


    output_dir = "results/gene_prediction/topo_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)
    
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        novel_predicted_genes,  # or confirmed_genes
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Exclude MED subunits
    top_scored_genes = [
        g for g in top_scored_genes
        if not g.startswith("MED")
    ]


    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    
    selected_known_genes = ["BRCA2", "HDAC2"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    # for g in selected_novel_genes + selected_known_genes:
    for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 20:
            break

    #combined_genes = [g for g in combined_genes if g != "IRF2"]

    #confirmed_genes = combined_genes

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_topo_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_neighbor_relevance_itself_neighbor(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter top neighbors
    filtered = {k: v for k, v in neighbor_scores.items() if v > 0.0}
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            node_names_topkint(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)


    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved neighbor relevance plot to {output_path}")

def save_and_plot_confirmed_genes_bio(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="bio",
    confirmed_gene_path="data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their biological feature contributions.
    """

    
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]

    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics}:{cancer}" for omics in omics_order for cancer in cancer_names]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    # for gene_name in confirmed_genes:
    #     idx = node_names_topk.index(gene_name)
    #     relevance_vector = summary_feature_relevance[idx]
    #     score = get_scalar_score(node_scores_topk[idx])
    #     cluster_id = row_labels_topk[idx].item()

    #     output_path = os.path.join(
    #         "results/gene_prediction/bio_confirmed_feature_contributions/",
    #         f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_feature_contributions_epo{args.num_epochs}.png"
    #     )

    #     plot_gene_feature_contributions_bio(
    #         gene_name=gene,
    #         relevance_vector=relevance_vector,
    #         feature_names=feature_names,
    #         score=score,
    #         cluster_id=cluster_id,
    #         output_path=output_path
    #     )
    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_bio(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/bio_confirmed_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )

def save_and_plot_novel_genes_bio(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    novel_genes_save_path,
    row_labels_topk,
    tag="bio",
    confirmed_gene_path="data/ncg_8886.txt"
):
    """
    Finds novel predicted cancer genes (not in known list) and plots their biological feature contributions.
    """

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics}:{cancer}" for omics in omics_order for cancer in cancer_names]

    # Load confirmed genes
    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    # Filter for novel genes
    novel_predicted_genes = [g for g in node_names_topk if g not in known_cancer_genes]

    # Save novel genes
    with open(novel_genes_save_path, "w") as f:
        for gene in novel_predicted_genes:
            f.write(f"{gene}\n")

    # Plot directory for NPCGs
    plot_dir = os.path.join(output_dir, f"{tag}_novel_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    for gene_name in novel_predicted_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_bio(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/bio_novel_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )

def plot_collapsed_clusterfirst_multilevel_sankey_bio_linked_loop(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk, #confirmed_genes,
    ## scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 

    # Top 10 by model score
    # top_scored_genes = sorted(
    #     node_names_topk,  # or confirmed_genes
    #     key=lambda g: scores[name_to_index[g]],
    #     reverse=True
    # )
    # Top 10 by mean saliency score
    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(), #.mean(),
        reverse=True
    )

    # Exclude MED subunits
    top_scored_genes = [
        g for g in top_scored_genes
        if not g.startswith("MED")
    ]


    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    
    selected_known_genes = ["EGFR", "SRC", "BRCA2", "HDAC2"]
    selected_known_genes = ["BRCA2", "HDAC2"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
    # for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break


    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        # genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]], reverse=True)[:10]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]


        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                # font_sizes.append(18 if saliency > 0.5 else 10)

                # if saliency > 0.5:
                #     highlight_node_indices.append(label_to_idx[gene])
                #     highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    # if highlight_node_indices:
    #     x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
    #     y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

    #     fig.add_trace(go.Scatter(
    #         x=x_positions,
    #         y=y_positions,
    #         mode='none',
    #         marker=dict(
    #             size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
    #             color="rgba(255,0,0,0.3)",
    #             line=dict(width=2, color="rgba(255,0,0,0.7)"),
    #             sizemode='diameter'
    #         ),
    #         hoverinfo='skip',
    #         showlegend=False
    #     ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_collapsed_clusterfirst_multilevel_sankey_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["BRCA2", "HDAC2"]
    selected_known_genes = ["EGFR", "SRC", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]
        cluster_label = f"Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    existing_edges = set()  # Track (source, target) to avoid A‚ÜíB and B‚ÜíA cycles

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)

            gene_idx = label_to_idx[gene]

            # Cluster ‚Üí Gene
            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]

                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                # Gene ‚Üí Neighbor (skip if reverse exists)
                if (neighbor_node_idx, gene_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                # Neighbor ‚Üí Cluster (skip if reverse exists)
                if (neighbor_cluster_idx, neighbor_node_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_neighbor_relevance_filt_by_index(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Remove the gene itself from neighbors (by checking name match)
    gene_index = None
    for idx, name in node_id_to_name.items():
        if name == gene_name:
            gene_index = idx
            break

    # Filter out self and zero-relevance neighbors
    filtered = {
        k: v for k, v in neighbor_scores.items()
        if v > 0.0 and (str(k) != str(gene_index))
    }

    # Keep top 10
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            node_names_topkint(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved neighbor relevance plot to {output_path}")

def plot_neighbor_relevance_(
    neighbor_scores,
    # gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    Filters out the gene itself from being plotted as its own neighbor (by name).
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter out self-links by comparing names
    filtered = {
        k: v for k, v in neighbor_scores.items()
        if v > 0.0 and node_id_to_name.get(k, "") != gene_name
    }

    # Keep top 10
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            cid = int(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved neighbor relevance plot to {output_path}")

def plot_neighbor_relevance(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    Filters out the gene itself from being plotted as its own neighbor (by name).
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter out self-links by comparing names
    filtered = {
        k: v for k, v in neighbor_scores.items()
        if v > 0.0 and node_id_to_name.get(k, "") != gene_name
    }

    # Keep top 10
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            try:
                nid_int = int(nid)
                if nid_int < len(row_labels):
                    cid = int(row_labels[nid_int])
                    cluster_ids.append(cid)
                    colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
                else:
                    print(f"‚ö†Ô∏è nid {nid_int} is out of bounds (row_labels size: {len(row_labels)}).")
                    colors.append("gray")
            except Exception as e:
                print(f"‚ö†Ô∏è Skipping nid={nid} due to error: {e}")
                colors.append("gray")
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        if real_neighbors:
            try:
                # Try extracting cluster from gene_name string, if it's in format: "GENE (Cluster X)"
                import re
                match = re.search(r'\(Cluster (\d+)\)', gene_name)
                if match:
                    main_cluster_id = int(match.group(1))
                else:
                    main_cluster_id = 0

            except Exception:
                main_cluster_id = 0
        else:
            main_cluster_id = 0

        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved neighbor relevance plot to {output_path}")

def plot_confirmed_neighbors_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    # scores,
    row_labels,
    total_clusters,
    relevance_scores):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in topk_name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        # gene_score = relevance_scores[node_idx]
        gene_score = relevance_scores[node_idx].sum().item()  # OR .mean().item()

        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
        # gene_cluster = int(row_labels[node_idx])

        print(f"{gene} ‚Üí Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        # Filter only those neighbors present in top-k
        neighbor_scores_dict = {}
        for n in neighbors:
            if n == gene:
                continue  # ‚úÖ Skip self
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score


        if not neighbor_scores_dict:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/bio_neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

def plot_collapsed_clusterfirst_multilevel_sankey_bio_sankey_color_not_consistant(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):


    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "SRC", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    combined_genes = []
    seen = set()
    # for g in top_scored_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        graph_node_idx = name_to_index[gene]
        gene_cluster = cluster_assignments[graph_node_idx]
        cluster_label = f"Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores


    source = []
    target = []
    value = []
    link_colors = []

    existing_edges = set()  # Track (source, target) to avoid A‚ÜíB and B‚ÜíA cycles

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)

            gene_idx = label_to_idx[gene]

            # Cluster ‚Üí Gene
            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]

                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                # Gene ‚Üí Neighbor (skip if reverse exists)
                if (neighbor_node_idx, gene_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                # Neighbor ‚Üí Cluster (skip if reverse exists)
                if (neighbor_cluster_idx, neighbor_node_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_feature_importance_bio(relevance_vector, feature_names, node_name=None, output_path="plots"):
    """
    Plot biological feature importance in OMICS:CANCER format using alphabetical order of cancer names.

    Parameters:
        relevance_vector (array-like): Relevance scores.
        feature_names (list of str): Feature names in the format Cancer_Omics.
        node_name (str, optional): Name of the node (used for title).
        output_path (str): Path to save the plot.
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import os

    # Define mapping from TCGA codes to cancer names
    cancer_map = {
        'BLCA': 'Bladder', 'BRCA': 'Breast', 'CESC': 'Cervix', 'COAD': 'Colon',
        'ESCA': 'Esophagus', 'HNSC': 'HeadNeck', 'KIRC': 'KidneyCC', 'KIRP': 'KidneyPC',
        'LIHC': 'Liver', 'LUAD': 'LungAD', 'LUSC': 'LungSC', 'PRAD': 'Prostate',
        'READ': 'Rectum', 'STAD': 'Stomach', 'THCA': 'Thyroid', 'UCEC': 'Uterus'
    }

    # Sort cancer codes by alphabetical order of their full names
    sorted_cancer_codes = sorted(cancer_map.keys(), key=lambda k: cancer_map[k])
    omics_order = ['cna', 'ge', 'meth', 'mf']
    column_labels = [f"{omics.upper()}:{cancer}" for omics in omics_order for cancer in sorted_cancer_codes]

    # Validate input
    if len(relevance_vector) != len(feature_names):
        raise ValueError(f"Mismatch: {len(relevance_vector)} values vs {len(feature_names)} names")

    # Build DataFrame
    df = pd.DataFrame({
        "feature": feature_names,
        "relevance": relevance_vector
    })

    df["omics_type"] = df["feature"].apply(lambda x: x.split("_")[1].lower())
    df["cancer_type"] = df["feature"].apply(lambda x: x.split("_")[0])
    df["formatted_label"] = df.apply(lambda row: f"{row['omics_type'].upper()}:{row['cancer_type']}", axis=1)

    # Reorder using column_labels
    df["order"] = df["formatted_label"].apply(lambda x: column_labels.index(x) if x in column_labels else -1)
    df = df[df["order"] != -1].sort_values("order")

    # Color mapping
    omics_color = {
        'cna': '#1F77B4',
        'ge': '#9467BD',
        'meth': '#2CA02C',
        'mf': '#D62728'
    }
    df["bar_color"] = df["omics_type"].map(omics_color)

    # Plotting
    plt.figure(figsize=(24, 5))
    ax = sns.barplot(x="formatted_label", y="relevance", data=df, palette=df["bar_color"].tolist())

    num_bars = len(df)
    margin = 0.75
    ax.set_xlim(-margin, num_bars - 1 + margin)

    ax.set_title(node_name if node_name else "", fontsize=16)
    ax.set_ylabel("Saliency score", fontsize=16)
    ax.set_xlabel("Feature (Omics: Cancer)", fontsize=16)

    # Color tick labels
    for tick, omics in zip(ax.get_xticklabels(), df["omics_type"]):
        tick.set_color(omics_color.get(omics, "black"))

    plt.xticks(rotation=90, fontsize=16)
    plt.yticks(fontsize=16)
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved BIO plot to {output_path}")

def plot_ridge_violin_by_cluster_(
    sorted_scores,
    row_gene_names,
    sorted_clusters,
    feature_names,
    title="Ridge-style Distribution of Relevance Scores by Cluster",
    save_path=None,
    figsize=(12, 6),
    palette="coolwarm"
):
    """
    Plot ridge-style violin plot showing distribution of relevance scores across clusters.

    Parameters:
    - sorted_scores: np.ndarray of shape (n_genes, n_features)
    - row_gene_names: List of gene names matching rows of sorted_scores
    - sorted_clusters: List or array of cluster assignments for each gene
    - feature_names: List of feature names matching columns of sorted_scores
    - title: Title of the plot
    - save_path: If given, saves the plot to this path
    - figsize: Tuple defining figure size
    - palette: Color palette for clusters

    Returns:
    - matplotlib Figure object
    """

    # Prepare DataFrame
    df_ridge = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)
    df_ridge['Cluster'] = sorted_clusters
    df_ridge['Gene'] = df_ridge.index

    df_ridge_melt = df_ridge.melt(
        id_vars=["Cluster", "Gene"],
        var_name="Feature",
        value_name="Score"
    )

    # Create violin plot
    fig = plt.figure(figsize=figsize)
    sns.violinplot(
        data=df_ridge_melt,
        x="Score",
        y="Cluster",
        scale="width",
        inner="quartile",
        linewidth=0.6,
        palette=palette,
        cut=0
    )

    plt.title(title, fontsize=16)
    plt.xlabel("Relevance Score", fontsize=16)
    plt.ylabel("Cluster", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()

    # Save if path is provided
    if save_path:
        plt.savefig(save_path, dpi=300)
        print(f"‚úÖ Ridge violin plot saved to: {save_path}")

    return fig

def plot_ridge_violin_by_cluster(
    sorted_scores,
    row_gene_names,
    sorted_clusters,
    feature_names,
    title="Ridge-style Distribution of Relevance Scores by Cluster",
    save_path=None,
    figsize=(12, 6),
    palette="coolwarm"
):
    """
    Plot ridge-style violin plot showing distribution of relevance scores across clusters.

    Parameters:
    - sorted_scores: np.ndarray of shape (n_genes, n_features)
    - row_gene_names: List of gene names matching rows of sorted_scores
    - sorted_clusters: List or array of cluster assignments for each gene
    - feature_names: List of feature names matching columns of sorted_scores
    - title: Title of the plot
    - save_path: If given, saves the plot to this path
    - figsize: Tuple defining figure size
    - palette: Color palette for clusters

    Returns:
    - matplotlib Figure object
    """

    # Prepare long-form DataFrame with progress
    print("üîÑ Melting scores into long-form dataframe...")
    melt_rows = []
    for i in tqdm(range(len(row_gene_names)), desc="Melting genes"):
        gene = row_gene_names[i]
        cluster = sorted_clusters[i]
        for j in range(len(feature_names)):
            melt_rows.append({
                "Gene": gene,
                "Cluster": cluster,
                "Feature": feature_names[j],
                "Score": sorted_scores[i][j]
            })

    df_ridge_melt = pd.DataFrame(melt_rows)

    # Create violin plot
    fig = plt.figure(figsize=figsize)
    sns.violinplot(
        data=df_ridge_melt,
        x="Score",
        y="Cluster",
        scale="width",
        inner="quartile",
        linewidth=0.6,
        palette=palette,
        cut=0
    )

    plt.title(title, fontsize=16)
    plt.xlabel("Relevance Score", fontsize=16)
    plt.ylabel("Cluster", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300)
        print(f"‚úÖ Ridge violin plot saved to: {save_path}")

    return fig

def plot_bio_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    # cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    # os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=16)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # plot_ridge_violin_by_cluster(
    #     sorted_scores,
    #     row_gene_names,
    #     sorted_clusters,
    #     feature_names,
    #     save_path="results/gene_prediction/ridge_violin_relevance_by_cluster.png"
    # )

    # üîπ Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_dot_plots_for_all_sources(terms_by_source, cancer_type, top_n=20):
    """
    terms_by_source: dict where key = source (e.g. REAC, KEGG), value = list of [Term, GeneRatio, LogP, Cluster]
    """
    for source, terms in terms_by_source.items():
        print(f"üìä Plotting dot+bar for: {source}")
        draw_dot_plot_with_ratio(
            terms=terms,
            cancer_type=cancer_type,
            source=source,
            top_n=top_n
        )

def plot_confirmed_neighbors_bio(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes,
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores,
    top_k=1000  # ‚Üê Use top 1000 genes
):
    import csv
    import os

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # ‚ú® Filter predicted genes that exist in relevance_scores
    valid_genes = [g for g in predicted_cancer_genes if g in topk_name_to_index]
    
    # ‚ú® Sort by total saliency (descending)
    valid_genes_sorted = sorted(
        valid_genes,
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    # ‚ú® Select top 1000
    selected_genes = valid_genes_sorted[:top_k]

    # ‚úÖ Get neighbor dictionary
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, selected_genes)

    all_neighbor_rows = []

    for gene in selected_genes:
        node_idx = topk_name_to_index[gene]
        gene_score = relevance_scores[node_idx].sum().item()
        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()

        print(f"{gene} ‚Üí Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        neighbor_scores = get_top_neighbors(
            gene,
            neighbors,
            name_to_index,
            topk_name_to_index,
            relevance_scores,
            node_id_to_name,
            output_dir=None,
            k=10
        )

        if not neighbor_scores:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        for neighbor_idx, score in sorted(neighbor_scores.items(), key=lambda x: -x[1]):
            neighbor_name = node_id_to_name.get(neighbor_idx, "UNK")
            all_neighbor_rows.append([
                gene,
                neighbor_name,
                neighbor_idx,
                score,
                gene_cluster
            ])

        # Optional: comment out to skip plotting for 1000 genes
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )
        plot_neighbor_relevance(
            neighbor_scores=neighbor_scores,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

    # ‚úÖ Save one CSV for all gene-neighbor relationships
    combined_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_top{top_k}_confirmed_neighbors_epo{args.num_epochs}.csv"
    )

    with open(combined_csv_path, "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"])
        writer.writerows(all_neighbor_rows)

    print(f"üìÑ Combined confirmed neighbor CSV saved to: {combined_csv_path}")

def get_top_neighbors(
    gene,
    neighbors,
    name_to_index,
    topk_name_to_index,
    relevance_scores,
    node_id_to_name,
    output_dir,
    k=5
):
    """
    Returns a dictionary of top-k neighbor indices and scores,
    skipping self-loops and sorting by total relevance.
    """
    neighbor_scores = {}
    for n in neighbors:
        if n == gene:
            continue  # Skip self
        if n in topk_name_to_index:
            rel_idx = topk_name_to_index[n]
            if rel_idx < relevance_scores.shape[0]:
                rel_score = relevance_scores[rel_idx].sum().item()
                neighbor_scores[rel_idx] = rel_score

    top_k = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:k])
    return top_k

def plot_collapsed_clusterfirst_multilevel_sankey_bio_(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    
    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)
    
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )
    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1"]

    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
        if g in topk_name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_id_to_name, topk_name_to_index, combined_genes)

    all_neighbor_rows = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        graph_node_idx = name_to_index[gene]
        gene_cluster = cluster_assignments[graph_node_idx]

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = get_top_neighbors(
            gene,
            neighbors,
            name_to_index,
            topk_name_to_index,
            relevance_scores,
            node_id_to_name,
            output_dir=None,
            k=10
        )

        for neighbor_idx, score in neighbor_scores.items():
            neighbor_name = node_id_to_name.get(neighbor_idx, f"IDX_{neighbor_idx}")
            try:
                neighbor_cluster = int(graph.ndata["cluster_bio"][neighbor_idx].item())
            except:
                neighbor_cluster = -1  # Default fallback if cluster missing

            # ‚úÖ Skip if cluster is -1 (unassigned)
            if neighbor_cluster == -1:
                continue

            try:
                score_value = float(score)
            except:
                score_value = ""

            neighbor_idx_str = str(neighbor_idx) if isinstance(neighbor_idx, int) else ""

            all_neighbor_rows.append([
                gene,
                neighbor_name,
                neighbor_idx_str,
                score_value,
                neighbor_cluster
            ])


    os.makedirs(output_dir, exist_ok=True)
    neighbors_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_gene_neighbors_epo{args.num_epochs}.csv"
    )
    with open(neighbors_csv_path, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Neighbor_Cluster"])
        writer.writerows(all_neighbor_rows)

def plot_collapsed_clusterfirst_multilevel_sankey_bio_not_topk_HDAC2(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    
    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)


    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1"]

    combined_genes = []
    seen = set()
    # for g in top_scored_genes:
    for g in selected_known_genes + top_scored_genes:
    # for g in selected_known_genes + selected_novel_genes:
        if g in topk_name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_id_to_name, topk_name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        graph_node_idx = name_to_index[gene]
        gene_cluster = cluster_assignments[graph_node_idx]
        cluster_label = f"Confirmed Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        # neighbor_scores = {}

        # for n in neighbors:
        #     if n in topk_name_to_index:
        #         rel_idx = topk_name_to_index[n]
        #         if rel_idx < relevance_scores.shape[0]:
        #             rel_score = relevance_scores[rel_idx].sum().item()
        #             neighbor_scores[rel_idx] = rel_score
        neighbor_scores = get_top_neighbors(
            gene,
            neighbors,
            name_to_index,
            topk_name_to_index,
            relevance_scores,
            node_id_to_name,
            output_dir=output_dir,
            k=10
        )
        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

        # üìÅ Create CSV of all gene-neighbor pairs with saliency score
        neighbors_csv_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_gene_neighbors_epo{args.num_epochs}.csv"
        )
        with open(neighbors_csv_path, "w", newline="") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(["Gene", "Neighbor", "Relevance_Score"])
            for gene, neighbors in gene_to_neighbors.items():
                for neighbor_idx, score in neighbors.items():
                    neighbor_name = node_id_to_name.get(neighbor_idx, f"IDX_{neighbor_idx}")
                    writer.writerow([gene, neighbor_name, score])
        print(f"üìÑ Saved gene-neighbor relevance data to: {neighbors_csv_path}")

    source = []
    target = []
    value = []
    link_colors = []

    existing_edges = set()  # Track (source, target) to avoid A‚ÜíB and B‚ÜíA cycles

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                # saliency = relevance_scores[rel_idx].sum().item()
                # gene_cluster = row_labels[rel_idx]
                # color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                # all_colors.append(color)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                graph_node_idx = name_to_index[gene]
                gene_cluster = cluster_assignments[graph_node_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)

            gene_idx = label_to_idx[gene]

            # Cluster ‚Üí Gene
            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]

                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                # Gene ‚Üí Neighbor (skip if reverse exists)
                if (neighbor_node_idx, gene_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                # Neighbor ‚Üí Cluster (skip if reverse exists)
                if (neighbor_cluster_idx, neighbor_node_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats


def draw_dot_plot_with_ratio(terms, cancer_type, source, top_n=20):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    # Select top N by LogP
    top_terms = df.sort_values("LogP", ascending=False)["Term"].unique()[:top_n]
    df = df[df["Term"].isin(top_terms)]

    # Fix order bottom to top
    term_order = list(reversed(top_terms))
    df["Term"] = pd.Categorical(df["Term"], categories=term_order, ordered=False)

    fig = plt.figure(figsize=(18, 0.6 * len(term_order)))
    gs = fig.add_gridspec(1, 2, width_ratios=[1.0, 1], wspace=0.05)

    # === Dot Plot (Left) ===
    ax1 = fig.add_subplot(gs[0])
    sns.scatterplot(
        data=df,
        x="GeneRatio", y="Term",
        size="LogP", hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black", linewidth=0.5,
        ax=ax1,
        legend=False
    )

    ax1.set_xscale("log")
    ax1.set_xlabel("\nGene Ratio", fontsize=32)
    # ax1.set_ylabel("Enriched Pathway", fontsize=24)
    ax1.set_ylabel("", fontsize=24)
    # ax1.set_title(f"{cancer_type.upper()} ‚Äî {source}\n", fontsize=26)
    ax1.set_title(f"{source}\n", fontsize=36)
    ax1.tick_params(axis='both', labelsize=24)
    ax1.grid(axis="x", linestyle="--", alpha=0.5)
    ax1.set_yticklabels(ax1.get_yticklabels(), fontsize=32)
    

    # === Stacked Bar Plot (Right) ===
    ax2 = fig.add_subplot(gs[1], sharey=ax1)

    LABEL_PAD = 0.8
    MIN_BAR_FOR_INSIDE_LABEL = 1.2

    for term in term_order:
        clusters = df[df["Term"] == term].sort_values("LogP", ascending=True)
        y = term
        x_start = 0

        # Determine cluster with largest p-value (i.e., smallest LogP)
        cluster_min_logp_row = clusters.loc[clusters["LogP"].idxmin()]
        min_cluster = cluster_min_logp_row["Cluster"]

        for _, row in clusters.iterrows():
            logp = row["LogP"]
            cluster = row["Cluster"]
            color = CLUSTER_COLORS[cluster]

            ax2.barh(
                y=y,
                width=logp,
                left=x_start,
                height=0.6,
                color=color,
                edgecolor="black"
            )

            # Label color: black only for segment with largest p-value
            label_color = "black" if cluster == min_cluster else "white"

            if logp >= MIN_BAR_FOR_INSIDE_LABEL:
                ax2.text(
                    x=x_start + logp / 2,
                    y=y,
                    s=f"{logp:.1f}",
                    va="center",
                    ha="center",
                    fontsize=16,
                    color=label_color,
                    fontweight="bold"
                )
            else:
                ax2.text(
                    x=x_start - LABEL_PAD,
                    y=y,
                    s=f"{logp:.1f}",
                    va="center",
                    ha="right",
                    fontsize=16,
                    color=label_color,
                    fontweight="bold"
                )

            x_start += logp

    # Extend x-axis
    xmin = min(0, df.groupby("Term")["LogP"].sum().min() - 1.0)
    xmax = df.groupby("Term")["LogP"].sum().max() + 1.0
    ax2.set_xlim(left=xmin, right=xmax)

    ax2.set_xlabel("\n-log10(p-value)", fontsize=32)
    ax2.tick_params(axis='x', labelsize=20)
    ax2.tick_params(axis='y', labelleft=False)
    ax2.grid(axis='x', linestyle='--', alpha=0.5)
    ax2.set_xticklabels(ax2.get_xticklabels(), fontsize=24)

    plt.subplots_adjust(left=0.3, wspace=0.05, top=0.98, bottom=0.02)
    ax1.set_ylim(-0.5, len(term_order) - 0.5)
    ax2.set_ylim(-0.5, len(term_order) - 0.5)

    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved stacked bar + dot plot aligned: {out_path}")

def plot_survival_by_cluster(cluster_assignments, survival_file, save_path=None):
    """
    Plot Kaplan-Meier survival curves for each cluster.

    Args:
        cluster_assignments (dict): sample_id (TCGA barcode) ‚Üí cluster_id
        survival_file (str): path to TSV file with columns ['sample', 'OS.time', 'OS', '_PATIENT']
        save_path (str): optional path to save the plot
    """
    surv_df = pd.read_csv(survival_file, sep='\t')

    # Convert sample barcodes to match those in cluster_assignments
    surv_df['sample'] = surv_df['sample'].str[:15]  # standard TCGA barcode length
    
    # Add cluster info
    surv_df['cluster'] = surv_df['sample'].map(cluster_assignments)
    surv_df = surv_df.dropna(subset=['cluster'])

    kmf = KaplanMeierFitter()
    plt.figure(figsize=(10, 7))

    for cluster in sorted(surv_df['cluster'].unique()):
        cluster_df = surv_df[surv_df['cluster'] == cluster]
        kmf.fit(durations=cluster_df['OS.time'], event_observed=cluster_df['OS'], label=f"Cluster {cluster}")
        kmf.plot_survival_function(ci_show=True)

    plt.title("Survival Curves by Cluster")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival probability")
    plt.legend()
    if save_path:
        plt.savefig(save_path)
    plt.close()

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",  # or "mean"
    clusters_to_plot=[0, 1, 2, 3]
):
    """
    Performs survival analysis for gene clusters based on average gene expression per patient.

    Parameters:
    - survival_path: path to TCGA survival file (with columns: sample, OS.time, OS, _PATIENT)
    - expr_path: path to gene expression matrix (rows=genes, cols=patients)
    - node_names_topk: list of gene names corresponding to top-k predicted genes
    - row_labels: cluster labels for each gene (from biclustering)
    - output_dir: directory to save survival plots and Cox results
    - cluster_threshold: method to split patient cohort ("median" or "mean")
    - clusters_to_plot: which clusters to visualize with Kaplan-Meier plots
    """
    
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)  # rows=genes, cols=patients
    expr_df.columns = expr_df.columns.str.upper().str[:12]  # Standardize patient IDs

    # Map each cluster to gene list
    cluster_to_genes = defaultdict(list)
    for gene, cl in zip(node_names_topk, row_labels):
        cluster_to_genes[cl].append(gene)

    # Compute average cluster expression per patient
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Merge survival data with expression
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    T = merged["OS.time"]
    E = merged["OS"]

    # Kaplan-Meier plots
    for cl in clusters_to_plot:
        col = f"cluster_{cl}"
        if col not in merged.columns:
            continue

        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        merged["group"] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        plt.figure(figsize=(7, 5))
        for group in [0, 1]:
            label = f"{col} {'Low' if group == 0 else 'High'}"
            kmf.fit(T[merged["group"] == group], E[merged["group"] == group], label=label)
            kmf.plot_survival_function(ci_show=True)

        plt.title(f"Survival by {col} Expression")
        plt.xlabel("Time (Days)")
        plt.ylabel("Survival Probability")
        plt.tight_layout()
        plt.savefig(f"{output_dir}/km_cluster_{cl}.png", dpi=300)
        plt.close()

    # Cox Proportional Hazards
    cox_df = merged[["OS.time", "OS"] + [c for c in patient_cluster_scores.columns if c in merged.columns]]
    cox_df.columns = ["time", "event"] + list(patient_cluster_scores.columns)

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.print_summary()

    cph.summary.to_csv(f"{output_dir}/cox_summary.csv")

    print("‚úÖ Survival analysis completed.")

def apply_full_spectral_biclustering_bio_survival(
    graph, summary_bio_features, node_names_topk, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None,
    survival_path=None,
    expr_path=None
):
    print("üß™ Running Spectral Biclustering with fixed (16, 10) clusters...")

    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    summary_bio_features_topk = summary_bio_features
    assert summary_bio_features_topk.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_topk.shape[1]}"

    n_clusters_row = 10
    n_clusters_col = 5
    best_model = None
    best_score = np.inf

    print("üîÅ Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='bistochastic',
                                     svd_method='randomized', random_state=i)
        model.fit(summary_bio_features_topk)
        reconstructed = summary_bio_features_topk[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features_topk, reconstructed)
        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("‚úÖ Spectral Biclustering complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    # === Survival Analysis (optional)
    if survival_path and expr_path:
        print("üìà Running survival analysis...")
        survival_df = pd.read_csv(survival_path, sep="\t")
        survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

        expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
        expr_df.columns = expr_df.columns.str.upper().str[:12]

        from collections import defaultdict
        cluster_to_genes = defaultdict(list)
        for gene, cl in zip(node_names_topk, row_labels):
            cluster_to_genes[cl].append(gene)

        patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
        for cl, genes in cluster_to_genes.items():
            valid_genes = list(set(genes) & set(expr_df.index))
            if not valid_genes:
                continue
            patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

        merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
        T = merged["OS.time"]
        E = merged["OS"]

        for cl in range(n_clusters_row):
            col = f"cluster_{cl}"
            if col not in merged.columns:
                continue

            threshold = merged[col].median()
            merged["group"] = (merged[col] >= threshold).astype(int)

            kmf = KaplanMeierFitter()
            plt.figure(figsize=(7, 5))
            for group in [0, 1]:
                label = f"{col} {'Low' if group == 0 else 'High'}"
                kmf.fit(T[merged["group"] == group], E[merged["group"] == group], label=label)
                kmf.plot_survival_function(ci_show=True)

            plt.title(f"Survival by {col} Expression")
            plt.xlabel("Time (Days)")
            plt.ylabel("Survival Probability")
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f"km_cluster_{cl}.png"), dpi=300)
            plt.close()

        # CoxPH
        cox_df = merged[["OS.time", "OS"] + [c for c in patient_cluster_scores.columns if c in merged.columns]]
        cox_df.columns = ["time", "event"] + list(patient_cluster_scores.columns)

        cph = CoxPHFitter()
        cph.fit(cox_df, duration_col="time", event_col="event")
        cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def perform_survival_analysis_(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    output_path: str
):
    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str[:15]  # Harmonize TCGA barcode format
    
    # Load expression matrix (patients x features)
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.columns = expr_df.columns.str[:15]  # Harmonize columns
    
    # Intersect patient IDs
    common_patients = list(set(expr_df.columns) & set(survival_df['sample']))
    expr_df = expr_df[common_patients]
    survival_df = survival_df[survival_df['sample'].isin(common_patients)]
    survival_df = survival_df.set_index('sample').loc[expr_df.columns]  # align

    # Transpose to get patients x features
    patient_expr = expr_df.T
    patient_expr['cluster'] = cluster_labels

    survival_df['cluster'] = cluster_labels

    kmf = KaplanMeierFitter()
    plt.figure(figsize=(10, 6))
    for cluster_id in sorted(np.unique(cluster_labels)):
        mask = survival_df['cluster'] == cluster_id
        cluster_data = survival_df.loc[mask, ['OS.time', 'OS']].dropna()

        if len(cluster_data) > 0:
            T = cluster_data['OS.time']
            E = cluster_data['OS']
            kmf.fit(T, E, label=f"Cluster {cluster_id}")
            kmf.plot(ci_show=False)

    # kmf = KaplanMeierFitter()
    # plt.figure(figsize=(10, 6))
    # for cluster_id in sorted(np.unique(cluster_labels)):
    #     mask = survival_df['cluster'] == cluster_id
    #     T = survival_df.loc[mask, 'OS.time']
    #     E = survival_df.loc[mask, 'OS']
    #     if len(T) > 0:
    #         kmf.fit(T, E, label=f"Cluster {cluster_id}")
    #         kmf.plot(ci_show=False)

    plt.title("Survival by Gene Cluster")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_path)
    print(f"‚úÖ Saved survival plot to {output_path}")

def train__(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    output_dir = 'results/gene_prediction/'
    os.makedirs(output_dir, exist_ok=True)

    omics_types = ['cna', 'ge', 'meth', 'mf']
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names = [f"{cancer}_{omics}" for omics in omics_types for cancer in cancer_names]
    topo_feat_names = [f"Topo_{i}" for i in range(64)]

    feature_groups = {"bio": (0, 1024), "topo": (1024, 2048)}
    omics_splits = {'cna': (0, 15), 'ge': (16, 31), 'meth': (32, 47), 'mf': (48, 63)}

    epoch_times, cpu_usages, gpu_usages = [], [], []

    data_path = os.path.join('../gat/data/multiomics_meth/', f'{args.net_type}_omics_ppi_embeddings_graph_2048.json')
    nodes, edges, embeddings, labels = load_graph_data(data_path)

    node_names = list(nodes.keys())
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    graph = dgl.graph(edges, num_nodes=len(nodes))

    graph.ndata['feat'] = embeddings
    graph.ndata['label'] = labels
    graph.ndata['train_mask'] = labels != -1
    graph.ndata['test_mask'] = torch.ones_like(labels, dtype=torch.bool)
    graph = dgl.add_self_loop(graph)

    assert len(node_names) == graph.num_nodes(), "Node names length mismatch!"

    ground_truth_cancer_genes = load_ground_truth_cancer_genes('data/ncg_8886.txt')

    in_feats = embeddings.shape[1]
    model = choose_model(args.model_type, in_feats, args.hidden_feats, 1).to(device)
    if hasattr(model, 'set_graph'):
        model.set_graph(graph)

    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    graph = graph.to(device)
    features = embeddings.to(device)
    labels = labels.to(device).float()
    train_mask = graph.ndata['train_mask'].to(device)

    for epoch in tqdm(range(args.num_epochs), desc="Training Progress", unit="epoch"):
        epoch_start = time.time()
        cpu_usage = psutil.cpu_percent(interval=None)
        gpu_usage = torch.cuda.memory_allocated(device) / 2048**2 if torch.cuda.is_available() else 0.0

        model.train()
        logits = model(graph, features).squeeze()
        loss = loss_fn(logits[train_mask], labels[train_mask])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_times.append(time.time() - epoch_start)
        cpu_usages.append(cpu_usage)
        gpu_usages.append(gpu_usage)

        tqdm.write(f"Epoch {epoch + 1}/{args.num_epochs}, Loss: {loss.item():.4f}, CPU: {cpu_usage}%, GPU: {gpu_usage:.2f} MB")

    model.eval()
    with torch.no_grad():
        logits = model(graph, features).squeeze()
        scores = torch.sigmoid(logits).cpu().numpy()

    non_labeled_nodes = [i for i, label in enumerate(labels) if label == -1]
    non_labeled_scores = [(node_names[i], scores[i]) for i in non_labeled_nodes]
    ranking = sorted(non_labeled_scores, key=lambda x: x[1], reverse=True)

    process_predictions(
        ranking, args,
        "data/796_drivers.txt",
        "data/oncokb_1172.txt",
        "data/ongene_803.txt",
        "data/ncg_8886.txt",
        "data/intogen_23444.txt",
        node_names,
        non_labeled_nodes
    )

    predicted_cancer_genes = [i for i, _ in ranking[:1000]]
    top_gene_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]
    graph.ndata['degree'] = graph.in_degrees().float().unsqueeze(1)

    if top_gene_indices:
        avg_degree = graph.ndata['degree'][top_gene_indices].float().mean().item()
        print(f"Average degree of top predicted nodes: {avg_degree:.2f}")
    else:
        print("No top nodes predicted above the threshold.")

    print("Generating feature importance plots...")

    bio_feats = graph.ndata['feat'][:, :1024]
    topo_feats = graph.ndata['feat'][:, 1024:]

    bio_embeddings_np = bio_feats.cpu().numpy()
    summary_bio_features = extract_summary_features_np_bio(bio_embeddings_np)

    topo_embeddings_np = topo_feats.cpu().numpy()
    summary_topo_features = extract_summary_features_np_topo(topo_embeddings_np)

    relevance_scores = compute_relevance_scores(model, graph, features)
    relevance_scores_bio = relevance_scores[:, :1024]
    relevance_scores_topo = relevance_scores[:, 1024:]

    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    relevance_scores_topk_bio = relevance_scores_bio[topk_node_indices_tensor]
    relevance_matrix_bio = extract_summary_features_np_bio(relevance_scores_topk_bio.detach().cpu().numpy())

    # Save expression matrix for survival analysis
    expression_output_path = 'data/expression_matrix.tsv'
    os.makedirs(os.path.dirname(expression_output_path), exist_ok=True)
    with open(expression_output_path, 'w') as f:
        header = ['gene'] + [f'bio_feat_{i}' for i in range(relevance_matrix_bio.shape[1])]
        f.write('\t'.join(header) + '\n')
        for idx, row in enumerate(relevance_matrix_bio):
            line = [node_names[top_gene_indices[idx]]] + list(map(str, row))
            f.write('\t'.join(line) + '\n')

    print(f"‚úÖ Expression matrix saved to: {expression_output_path}")


    best_k=10
    print(f"Best number of clusters (k > 1): {best_k}")

    # Step 2: Convert to numpy
    #relevance_matrix = summary_bio_topk#.detach().cpu().numpy()
    #relevance_matrix = normalize(summary_bio_topk, axis=1)

    # Step 3: Get gene names for top-k nodes
    # node_names_topk = [node_names[i] for i in top_gene_indices]
    
    output_path_genes_clusters = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_clusters_epo{args.num_epochs}.png')
    output_path_predicted_genes = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_genes.csv')

    spectral_biclustering_graph_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_clustered_graph.pth")
    row_labels_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_row_labels.npy")
    total_genes_per_cluster_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_total_genes_per_cluster.npy")
    pred_counts_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_pred_counts.npy")

    # graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
    #     graph=graph,
    #     summary_bio_features=relevance_matrix_bio,  # shape (1000, 64)
    #     node_names=node_names,
    #     predicted_cancer_genes=predicted_cancer_genes,
    #     n_clusters=best_k,
    #     save_path=bio_graph_path,
    #     save_row_labels_path=bio_row_labels_path,
    #     save_total_genes_per_cluster_path=bio_total_per_cluster_path,
    #     save_predicted_counts_path=bio_pred_counts_path,
    #     output_path_genes_clusters=bio_output_img,
    #     output_path_heatmap=bio_output_path_heatmap,
    #     topk_node_indices=top_gene_indices  # <-- add this!
    # )

    graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
        graph=graph,
        summary_bio_features=relevance_matrix_bio,
        node_names_topk=node_names_topk,
        omics_splits=omics_splits,
        predicted_cancer_genes=predicted_cancer_genes,
        save_path=bio_graph_path,
        save_row_labels_path=bio_row_labels_path,
        save_total_genes_per_cluster_path=bio_total_per_cluster_path,
        save_predicted_counts_path=bio_pred_counts_path,
        output_path_genes_clusters=bio_output_img,
        output_path_heatmap=bio_output_path_heatmap,
        topk_node_indices=top_gene_indices,
        output_dir=output_dir,  # <-- new argument
        args=args               # <-- new argument
    )
    
    # Save expression matrix from relevance
    expression_matrix_path = os.path.join('data/', 'expression_matrix.tsv')

    # Save survival plot
    survival_plot_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_survival_by_cluster.png")
    survival_data_path = 'data/TCGA-KIRC.survival.tsv'

    # Call survival analysis
    perform_survival_analysis(
        survival_path=survival_data_path,
        expression_matrix_path=expression_matrix_path,
        cluster_labels=row_labels_bio,  # shape: [#genes] (assigned during clustering)
        output_path=survival_plot_path
    )

def generate_expression_matrix_file(predicted_gene_names, summary_bio_topk, cancer_types, output_path='expression_matrix.tsv'):
    """
    Generates expression_matrix.tsv: rows are genes, columns are cancer types,
    and values are summary bio relevance scores.
    """
    assert summary_bio_topk.shape[1] == len(cancer_types) * 4, "Mismatch in summary bio shape"
    
    # Create dataframe
    rows = []
    for i, gene in enumerate(predicted_gene_names):
        for j, cancer in enumerate(cancer_types):
            for k, omics in enumerate(['cna', 'ge', 'meth', 'mf']):
                col_idx = j * 4 + k
                rows.append({
                    'gene': gene,
                    'cancer': cancer,
                    'omics': omics,
                    'relevance_score': summary_bio_topk[i, col_idx]
                })

    df = pd.DataFrame(rows)
    df.to_csv(output_path, sep='\t', index=False)
    print(f"Saved: {output_path}")
    return df

def run_survival_analysis_by_cluster_(row_labels_bio, node_names_topk, survival_file, expression_file, output_dir):
    print("Running survival analysis for clusters...")

    # Load survival data
    survival_df = pd.read_csv(survival_file, sep='\t')
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # Load expression matrix
    expr_df = pd.read_csv(expression_file, sep='\t', index_col=0)  # rows: genes, cols: samples
    expr_df.index = expr_df.index.str.upper()  # Ensure gene symbols are upper-case
    expr_df = expr_df.loc[expr_df.index.intersection(node_names_topk)]  # Only top genes

    if expr_df.empty:
        print("No overlapping genes found in expression matrix.")
        return

    # Create mapping from gene name to cluster
    cluster_map = {}
    for idx, gene in enumerate(node_names_topk):
        cluster_map[gene.upper()] = row_labels_bio[idx]

    expr_df['cluster'] = expr_df.index.map(cluster_map)

    # Iterate over clusters
    for cluster_id in sorted(set(row_labels_bio)):
        genes_in_cluster = expr_df[expr_df['cluster'] == cluster_id].drop(columns=['cluster'])
        if genes_in_cluster.empty:
            continue

        # Collapse expression for each patient (e.g., mean)
        cluster_expr = genes_in_cluster.mean(axis=0)

        # Match patients in survival
        common_samples = set(survival_df['sample']).intersection(cluster_expr.index)
        if len(common_samples) < 20:
            print(f"Skipping cluster {cluster_id} (too few patients: {len(common_samples)})")
            continue

        surv_subset = survival_df[survival_df['sample'].isin(common_samples)].copy()
        # cluster_expr = cluster_expr[common_samples]
        cluster_expr = cluster_expr[list(common_samples)]

        
        # Split patients by median expression
        median_expr = cluster_expr.median()
        surv_subset['expression'] = cluster_expr
        surv_subset['group'] = (surv_subset['expression'] >= median_expr).map({True: 'High', False: 'Low'})

        # Kaplan-Meier
        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(6, 5))
        for group_name, grouped in surv_subset.groupby('group'):
            kmf.fit(grouped['OS.time'], grouped['OS'], label=group_name)
            kmf.plot_survival_function(ax=ax)

        # Log-rank test
        groups = surv_subset['group']
        ix = groups == 'High'
        result = logrank_test(
            surv_subset.loc[ix, 'OS.time'], surv_subset.loc[~ix, 'OS.time'],
            event_observed_A=surv_subset.loc[ix, 'OS'],
            event_observed_B=surv_subset.loc[~ix, 'OS']
        )

        plt.title(f"Cluster {cluster_id} (p = {result.p_value:.4f})")
        plt.xlabel("Time (Days)")
        plt.ylabel("Survival probability")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"survival_cluster_{cluster_id}.png"))
        plt.close()

def run_survival_analysis_from_relevance(
    expr_file,
    survival_file,
    node_names_topk,
    relevance_matrix_bio,
    output_dir=None,
    plot_title="Survival Analysis: Relevance-weighted Expression"
):
    """
    Perform survival analysis using relevance-weighted gene expression.

    Args:
        expr_file (str): Path to TCGA-KIRC.expression.tsv.
        survival_file (str): Path to TCGA-KIRC.survival.tsv.
        node_names_topk (List[str]): Names of top-k predicted genes.
        relevance_matrix_bio (np.ndarray): Relevance matrix [num_genes, 64].
        output_dir (str or None): Directory to save plots. If None, plots are shown directly.
        plot_title (str): Title for KM plot.
    """
    # Load expression data [genes x samples]
    expr = pd.read_csv(expr_file, sep='\t', index_col=0)

    # Subset to top-k genes
    genes_found = expr.index.intersection(node_names_topk)
    if len(genes_found) == 0:
        raise ValueError("No top-k genes found in expression file.")

    expr = expr.loc[genes_found]
    expr = expr.T  # Now: [samples x genes]

    # Match relevance weights to the genes in the same order
    gene_to_idx = {gene: idx for idx, gene in enumerate(node_names_topk)}
    matched_indices = [gene_to_idx[gene] for gene in expr.columns]
    rel = relevance_matrix_bio[matched_indices]  # shape: [genes_found, 64]
    relevance_weights = rel.mean(axis=1)

    # Normalize weights
    relevance_weights = (relevance_weights - relevance_weights.min()) / (relevance_weights.max() - relevance_weights.min())

    # Calculate risk score: weighted sum of expression √ó relevance
    X_expr = expr.to_numpy()
    risk_scores = X_expr @ relevance_weights  # shape: [samples]

    # Load survival data
    surv = pd.read_csv(survival_file, sep='\t')
    surv = surv.set_index("sample")

    # Align expression and survival data
    common_samples = expr.index.intersection(surv.index)
    if len(common_samples) < 10:
        raise ValueError("Too few overlapping samples between expression and survival data.")

    risk_scores = pd.Series(risk_scores, index=expr.index).loc[common_samples]
    surv = surv.loc[common_samples]

    # ===== Kaplan-Meier Plot =====
    median = risk_scores.median()
    risk_group = (risk_scores > median).astype(int)

    plt.figure(figsize=(8, 6))
    kmf = KaplanMeierFitter()
    for label in [0, 1]:
        kmf.fit(
            surv["OS.time"][risk_group == label],
            event_observed=surv["OS"][risk_group == label],
            label=f"Group {label}"
        )
        kmf.plot_survival_function(ci_show=False)

    plt.title(plot_title)
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.legend()
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        plt.savefig(os.path.join(output_dir, "km_plot_by_relevance_risk.png"), dpi=300)
        plt.close()
    else:
        plt.close()

    # ===== Cox Proportional Hazards =====
    cox_df = pd.DataFrame({
        "risk": risk_scores,
        "time": surv["OS.time"],
        "event": surv["OS"]
    })

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    # Save or print summary
    if output_dir:
        with open(os.path.join(output_dir, "cox_summary.txt"), "w") as f:
            cph.print_summary(stream=f)
    else:
        cph.print_summary()

def perform_survival_analysis_high_low_curves(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # Same order as cluster_labels
    output_path: str
):
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from lifelines import KaplanMeierFitter

    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]

    # Load expression matrix (genes x patients)
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.strip().str[:15]

    # Filter genes to only those in cluster_labels
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    # Map gene to cluster
    gene_to_cluster = {
        gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)
    }
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Loop over clusters
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.figure(figsize=(10, 6))
    kmf = KaplanMeierFitter()

    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if genes.empty:
            continue

        cluster_expr = genes.mean(axis=0)  # mean expression per patient
        common = cluster_expr.index.intersection(survival_df['sample'])
        if len(common) < 10:
            continue

        surv = survival_df[survival_df['sample'].isin(common)].copy()
        cluster_expr = cluster_expr[common]
        surv['expr'] = cluster_expr.values
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            continue

        for name, grp in surv.groupby('group'):
            kmf.fit(grp['OS.time'], grp['OS'], label=f"Cluster {cluster_id} - {name}")
            kmf.plot(ci_show=True)

    plt.title("Survival by Gene Cluster Expression")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_path)
    print(f"‚úÖ Saved survival plot to {output_path}")

def perform_survival_analysis_per_cluster_pa(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # same order as cluster_labels
    output_dir: str
):
    print("üîç Starting survival analysis per cluster...")

    # Make sure output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # Load expression matrix (genes x patients)
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    # Filter for top-k genes
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # Map genes to clusters
    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Summary for output CSV
    summary_records = []

    # Loop over clusters
    for cluster_id in sorted(np.unique(cluster_labels)):
        print(f"üß¨ Cluster {cluster_id}...")

        cluster_genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if cluster_genes.empty:
            print(f"‚ö†Ô∏è Cluster {cluster_id} has no genes. Skipping.")
            continue

        cluster_expr = cluster_genes.mean(axis=0)  # average expression per patient

        # Align patient IDs
        common_samples = cluster_expr.index.intersection(survival_df['sample'])
        if len(common_samples) < 10:
            print(f"‚ö†Ô∏è Too few samples in cluster {cluster_id} ({len(common_samples)}). Skipping.")
            continue

        surv = survival_df[survival_df['sample'].isin(common_samples)].copy()
        cluster_expr = cluster_expr[common_samples]
        surv['expr'] = cluster_expr.values

        # Check for low variance
        if surv['expr'].std() < 1e-5:
            print(f"‚ö†Ô∏è Cluster {cluster_id} has near-constant expression. Skipping.")
            continue

        # Split into High / Low by median expression
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            print(f"‚ö†Ô∏è Cluster {cluster_id} has only one group. Skipping.")
            continue

        # Kaplan-Meier + Log-rank test
        kmf = KaplanMeierFitter()
        plt.figure(figsize=(6, 5))
        for name, grp in surv.groupby('group'):
            if len(grp) < 2:
                continue
            kmf.fit(grp['OS.time'], grp['OS'], label=name)
            kmf.plot(ci_show=True)

        try:
            ix = surv['group'] == 'High'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            continue

        plt.title(f"Cluster {cluster_id} (p = {pval:.4f})")
        plt.xlabel("Time (Days)")
        plt.ylabel("Survival Probability")
        plt.legend()
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"KM_cluster_{cluster_id}.png")
        plt.savefig(plot_path)
        plt.close()
        print(f"‚úÖ Saved KM plot for cluster {cluster_id} to {plot_path}")

        # Append to summary
        summary_records.append({
            'cluster_id': cluster_id,
            'n_samples': len(surv),
            'p_value': pval
        })

    # Save p-value summary
    summary_df = pd.DataFrame(summary_records)
    summary_csv_path = os.path.join(output_dir, "survival_cluster_summary.csv")
    summary_df.to_csv(summary_csv_path, index=False)
    print(f"\nüìÑ Summary saved to: {summary_csv_path}")

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",  # or "mean"
    clusters_to_plot=[0, 1, 2, 3]
):
    """
    Performs survival analysis for gene clusters based on average gene expression per patient.
    """

    os.makedirs(output_dir, exist_ok=True)

    # Load survival and expression data
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]  # patient IDs

    # Map genes to clusters
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # Compute per-patient expression scores for each cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Merge with survival
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    # KM plots
    for cl in clusters_to_plot:
        col = f"cluster_{cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(7, 5))
        for group in [0, 1]:
            label = f"{col} {'Low' if group == 0 else 'High'}"
            kmf.fit(T[merged[group_col] == group], E[merged[group_col] == group], label=label)
            kmf.plot_survival_function(ci_show=True, ax=ax)

        # Remove legend if it exists
        legend = ax.get_legend()
        if legend:
            legend.remove()

        ax.set_title(f"Survival by {col} Expression")
        ax.set_xlabel("Time (Days)")
        ax.set_ylabel("Survival Probability")
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()


    # Cox regression
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cox_summary_path = os.path.join(output_dir, "cox_summary.csv")
    cph.summary.to_csv(cox_summary_path)
    print(f"üìÑ Saved Cox summary: {cox_summary_path}")

    return cph.summary, merged, patient_cluster_scores

def perform_survival_analysis_one_curve_per_cluster_with_ci_with_legend(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # same order as cluster_labels
    output_dir: str,
    min_samples_per_cluster=10
):
    print("üîç Starting survival analysis - High group only with CI & p-values...")
    os.makedirs(output_dir, exist_ok=True)

    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # Load expression matrix
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    # Filter for top-k genes
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # Map genes to clusters
    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Average cluster expression per patient
    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    text_lines = []
    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_cluster or low_mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        # Fit only High group for curve
        kmf.fit(T_high, E_high, label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=True,
                 color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank test: High vs Low
        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        text_lines.append(f"Cluster {cluster_id}: p = {pval:.4g}, CI = {ci_width:.3f}")

    # Annotate p + CI summary
    for i, line in enumerate(text_lines):
        ax.text(
            0.62 * ax.get_xlim()[1],
            0.9 - i * 0.08,
            line,
            fontsize=10,
            transform=ax.get_xaxis_transform()
        )

    plt.title("Survival Curves (High Expression Only)")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.legend(fontsize='small')
    plt.tight_layout()

    save_path = os.path.join(output_dir, "km_high_expression_per_cluster.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {save_path}")

def perform_survival_analysis_one_curve_per_cluster_with_ci(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_cluster=10
):
    print("üîç Starting survival analysis - High expression curves only...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    text_y_start = 0.9
    text_y_step = 0.04
    annotation_idx = 0

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_cluster or low_mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        # Fit KM only for high expression
        kmf.fit(T_high, E_high, label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=False,
                 color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank test: High vs Low
        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # Annotate using color patch
        patch_x = 0.60 * ax.get_xlim()[1]
        patch_y = text_y_start - annotation_idx * text_y_step

        ax.add_patch(Rectangle(
            (patch_x, patch_y), 30, 0.05 * ax.get_ylim()[1],
            transform=ax.transData,
            color=CLUSTER_COLORS.get(cluster_id, '#333333'),
            clip_on=False
        ))

        ax.text(
            patch_x + 35,
            patch_y + 0.01 * ax.get_ylim()[1],
            f"p = {pval:.4g}, CI = {ci_width:.3f}",
            fontsize=10,
            verticalalignment='bottom'
        )

        annotation_idx += 1

    plt.title("Survival Curves (High Expression Only)")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()

    save_path = os.path.join(output_dir, "km_high_expression_per_cluster.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {save_path}")

def perform_survival_analysis_with_legend(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # Same order as cluster_labels
    output_path: str
):
    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]

    # Load expression matrix (genes x patients)
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.strip().str[:15]

    # Filter genes to only those in cluster_labels
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    # Map gene to cluster
    gene_to_cluster = {
        gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)
    }
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Setup plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    text_y_pos = 0.9  # y-position for annotation text
    y_step = 0.08     # space between lines

    for i, cluster_id in enumerate(sorted(np.unique(cluster_labels))):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if genes.empty:
            continue

        cluster_expr = genes.mean(axis=0)
        common = cluster_expr.index.intersection(survival_df['sample'])
        if len(common) < 10:
            continue

        surv = survival_df[survival_df['sample'].isin(common)].copy()
        cluster_expr = cluster_expr[common]
        surv['expr'] = cluster_expr.values
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            continue

        grp = surv[surv['group'] == 'High']
        if len(grp) < 2:
            continue

        kmf.fit(grp['OS.time'], grp['OS'], label=f"Cluster {cluster_id}")
        kmf.plot(ci_show=True, ax=ax, color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # --- Calculate CI width ---
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # --- Log-rank p-value between high and low ---
        try:
            ix = surv['group'] == 'High'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception:
            pval = np.nan

        # --- Annotate inside plot ---
        text = f"Cluster {cluster_id}: p = {pval:.4g}, CI = {ci_width:.3f}"
        ax.text(
            0.65 * ax.get_xlim()[1],
            text_y_pos - i * y_step,
            text,
            fontsize=10,
            transform=ax.get_xaxis_transform()
        )

    plt.title("Survival Curves for High-Expression Patients by Cluster")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.legend(title="Cluster", fontsize="small")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {output_path}")

def perform_survival_analysis_high(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # Same order as cluster_labels
    output_path: str
):
    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]

    # Load expression matrix (genes x patients)
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.strip().str[:15]

    # Filter genes to only those in cluster_labels
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    # Map gene to cluster
    gene_to_cluster = {
        gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)
    }
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Setup plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    text_y_pos = 0.9  # y-position for annotation text
    y_step = 0.08     # space between lines

    patch_size_x = 30  # in data units on x axis
    patch_size_y = 0.05 * (ax.get_ylim()[1] - ax.get_ylim()[0])  # 5% of y axis height

    for i, cluster_id in enumerate(sorted(np.unique(cluster_labels))):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if genes.empty:
            continue

        cluster_expr = genes.mean(axis=0)
        common = cluster_expr.index.intersection(survival_df['sample'])
        if len(common) < 10:
            continue

        surv = survival_df[survival_df['sample'].isin(common)].copy()
        cluster_expr = cluster_expr[common]
        surv['expr'] = cluster_expr.values
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            continue

        grp = surv[surv['group'] == 'High']
        if len(grp) < 2:
            continue

        kmf.fit(grp['OS.time'], grp['OS'], label=f"Cluster {cluster_id}")
        kmf.plot(ci_show=False, ax=ax, color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # Calculate CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank p-value between high and low groups
        try:
            ix = surv['group'] == 'High'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception:
            pval = np.nan

        # Coordinates for annotation text and patch
        x_pos = 0.65 * ax.get_xlim()[1]
        y_pos = text_y_pos - i * y_step

        # Draw color patch (square) left of the text
        ax.add_patch(Rectangle(
            (x_pos - patch_size_x, y_pos - patch_size_y / 2),
            patch_size_x,
            patch_size_y,
            color=CLUSTER_COLORS.get(cluster_id, '#333333'),
            transform=ax.transData,
            clip_on=False
        ))

        # Text annotation next to patch
        text = f"p = {pval:.4g}, CI = {ci_width:.3f}"
        ax.text(
            x_pos,
            y_pos,
            text,
            fontsize=10,
            verticalalignment='center',
            transform=ax.get_xaxis_transform()
        )

    plt.title("Survival Curves for High-Expression Patients by Cluster")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {output_path}")

def perform_survival_analysis_(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # Same order as cluster_labels
    output_path: str
):
    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]

    # Load expression matrix (genes x patients)
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.strip().str[:15]

    # Filter genes to only those in cluster_labels
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    # Map gene to cluster
    gene_to_cluster = {
        gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)
    }
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Setup plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    # For storing p-values and CI widths for annotation
    cluster_pvals = {}
    cluster_ci_widths = {}

    text_y_start = 0.9  # y-position for first color line and text
    line_x_start = 0.65 * ax.get_xlim()[1]
    line_length = 0.04 * ax.get_xlim()[1]
    line_spacing = 0.06  # vertical spacing between color lines

    for i, cluster_id in enumerate(sorted(np.unique(cluster_labels))):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if genes.empty:
            continue

        cluster_expr = genes.mean(axis=0)
        common = cluster_expr.index.intersection(survival_df['sample'])
        if len(common) < 10:
            continue

        surv = survival_df[survival_df['sample'].isin(common)].copy()
        cluster_expr = cluster_expr[common]
        surv['expr'] = cluster_expr.values
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            continue

        # Select only Low expression group for plotting
        grp = surv[surv['group'] == 'Low']
        if len(grp) < 2:
            continue

        kmf.fit(grp['OS.time'], grp['OS'], label=f"Cluster {cluster_id}")
        color = CLUSTER_COLORS.get(cluster_id, "gray")
        kmf.plot(ci_show=True, ax=ax, color=color, linewidth=2)

        # Calculate CI width (mean width over time)
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
        cluster_ci_widths[cluster_id] = ci_width

        # Log-rank p-value between low and high groups
        try:
            ix = surv['group'] == 'Low'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception:
            pval = np.nan
        cluster_pvals[cluster_id] = pval

    # Remove legend box entirely
    if ax.get_legend():
        ax.get_legend().remove()

    # Add colored horizontal lines + p-value and CI text annotations
    for i, cluster_id in enumerate(sorted(cluster_pvals.keys())):
        y_pos = text_y_start - i * line_spacing
        color = CLUSTER_COLORS.get(cluster_id, "gray")

        # Horizontal colored line as marker
        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=color,
            linewidth=4,
            transform=ax.get_xaxis_transform()
        )

        # p-value and CI stacked text, close together
        pval_text = f"p = {cluster_pvals[cluster_id]:.4g}"
        ci_text = f"CI = {cluster_ci_widths[cluster_id]:.3f}"

        ax.text(
            line_x_start + line_length + 0.01 * ax.get_xlim()[1],
            y_pos + 0.012,
            pval_text,
            fontsize=9,
            verticalalignment='bottom',
            transform=ax.get_xaxis_transform()
        )
        ax.text(
            line_x_start + line_length + 0.01 * ax.get_xlim()[1],
            y_pos - 0.012,
            ci_text,
            fontsize=9,
            verticalalignment='top',
            transform=ax.get_xaxis_transform()
        )

    plt.title("Survival Curves for Low-Expression Patients by Cluster")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {output_path}")


def perform_survival_analysis_one_curve_per_cluster_with_ci_(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_cluster=10
):
    print("üîç Starting survival analysis - High expression curves only...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    text_y_start = 0.9
    text_y_step = 0.03  # less vertical space for tighter lines
    annotation_idx = 0

    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    line_x_start = 0.60 * xlim[1]
    line_length = 0.04 * xlim[1]

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_cluster or low_mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        # Fit KM only for high expression
        kmf.fit(T_high, E_high, label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=False,
                 color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank test: High vs Low
        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # Draw short horizontal color line as marker instead of patch
        y_pos = text_y_start - annotation_idx * text_y_step

        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=CLUSTER_COLORS.get(cluster_id, '#333333'),
            linewidth=2,
            transform=ax.get_xaxis_transform()
        )

        # p-value and CI stacked with tight vertical spacing
        ax.text(
            line_x_start + line_length + 0.01 * xlim[1],
            y_pos + 0.01 * ylim[1],
            f"p = {pval:.4g}",
            fontsize=10,
            verticalalignment='bottom',
            transform=ax.get_xaxis_transform()
        )
        ax.text(
            line_x_start + line_length + 0.01 * xlim[1],
            y_pos - 0.01 * ylim[1],
            f"CI = {ci_width:.3f}",
            fontsize=10,
            verticalalignment='top',
            transform=ax.get_xaxis_transform()
        )

        annotation_idx += 1

    # Remove legend box if any
    if ax.get_legend():
        ax.get_legend().remove()

    plt.title("Survival Curves (High Expression Only)")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()

    save_path = os.path.join(output_dir, "km_high_expression_per_cluster.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {save_path}")

def perform_survival_analysis_one_curve_per_cluster_with_ci_(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_cluster=10
):
    print("üîç Starting survival analysis - High expression curves only...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    # Coordinates for annotation in axes fraction [0,1]
    text_y_start = 0.92
    text_y_step = 0.06
    line_length = 0.05  # length of color line in axes fraction
    line_x_start = 0.65  # starting x pos of color line
    pval_x = line_x_start + line_length + 0.01
    ci_x = pval_x + 0.12

    annotation_idx = 0

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_cluster or low_mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        # Fit KM only for high expression
        kmf.fit(T_high, E_high, label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=False,
                 color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank test: High vs Low
        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # Y position for current annotation (top to bottom)
        y_pos = text_y_start - annotation_idx * text_y_step

        # Draw short horizontal color line in axes fraction coordinates
        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=CLUSTER_COLORS.get(cluster_id, '#333333'),
            linewidth=4,
            transform=ax.transAxes,
            clip_on=False
        )

        # Draw p-value and CI in two columns with tight vertical spacing
        ax.text(
            pval_x,
            y_pos + 0.01,
            f"p = {pval:.4g}",
            fontsize=10,
            verticalalignment='bottom',
            transform=ax.transAxes
        )
        ax.text(
            ci_x,
            y_pos - 0.01,
            f"CI = {ci_width:.3f}",
            fontsize=10,
            verticalalignment='top',
            transform=ax.transAxes
        )

        annotation_idx += 1

    # Remove legend box if any
    if ax.get_legend():
        ax.get_legend().remove()

    plt.title("Survival Curves (High Expression Only)")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()

    save_path = os.path.join(output_dir, "km_high_expression_per_cluster.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {save_path}")

def perform_survival_analysis_(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # Same order as cluster_labels
    output_path: str
):
    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]

    # Load expression matrix (genes x patients)
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.strip().str[:15]

    # Filter genes to only those in cluster_labels
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    # Map gene to cluster
    gene_to_cluster = {
        gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)
    }
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Setup plot
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    # For storing p-values and CI widths for annotation
    cluster_pvals = {}
    cluster_ci_widths = {}

    # Use axes fraction coordinates for annotations
    text_y_start = 0.9  # top position in axes coords
    line_x_start = 0.65  # start of color line in axes coords (0-1)
    line_length = 0.05   # length of color line in axes coords
    line_spacing = 0.07  # vertical spacing in axes coords

    for i, cluster_id in enumerate(sorted(np.unique(cluster_labels))):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if genes.empty:
            continue

        cluster_expr = genes.mean(axis=0)
        common = cluster_expr.index.intersection(survival_df['sample'])
        if len(common) < 10:
            continue

        surv = survival_df[survival_df['sample'].isin(common)].copy()
        cluster_expr = cluster_expr[common]
        surv['expr'] = cluster_expr.values
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            continue

        # Select only Low expression group for plotting
        grp = surv[surv['group'] == 'Low']
        if len(grp) < 2:
            continue

        kmf.fit(grp['OS.time'], grp['OS'], label=f"Cluster {cluster_id}")
        color = CLUSTER_COLORS.get(cluster_id, "gray")
        kmf.plot(ci_show=True, ax=ax, color=color, linewidth=2)

        # Calculate CI width (mean width over time)
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
        cluster_ci_widths[cluster_id] = ci_width

        # Log-rank p-value between low and high groups
        try:
            ix = surv['group'] == 'Low'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception:
            pval = np.nan
        cluster_pvals[cluster_id] = pval

    # Remove legend box entirely
    if ax.get_legend():
        ax.get_legend().remove()

    # Add colored horizontal lines + p-value and CI text annotations using axes fraction coords
    for i, cluster_id in enumerate(sorted(cluster_pvals.keys())):
        y_pos = text_y_start - i * line_spacing
        color = CLUSTER_COLORS.get(cluster_id, "gray")

        # Horizontal colored line as marker
        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=color,
            linewidth=4,
            transform=ax.transAxes,
            clip_on=False
        )

        # p-value and CI stacked text, close together, two lines
        pval_text = f"p = {cluster_pvals[cluster_id]:.4g}"
        ci_text = f"CI = {cluster_ci_widths[cluster_id]:.3f}"

        ax.text(
            line_x_start + line_length + 0.01,
            y_pos + 0.015,
            pval_text,
            fontsize=9,
            verticalalignment='bottom',
            transform=ax.transAxes
        )
        ax.text(
            line_x_start + line_length + 0.01,
            y_pos - 0.015,
            ci_text,
            fontsize=9,
            verticalalignment='top',
            transform=ax.transAxes
        )

    plt.title("Survival Curves for Low-Expression Patients by Cluster")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {output_path}")

def perform_survival_analysis(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # Same order as cluster_labels
    output_path: str
):
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.strip().str[:15]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    gene_to_cluster = {
        gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)
    }
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    cluster_pvals = {}
    cluster_ci_widths = {}

    text_y_start = 0.9
    line_x_start = 0.65
    line_length = 0.04
    text_x_start = line_x_start + line_length + 0.01
    text_gap = 0.1  # horizontal gap between p-value and CI text
    line_spacing = 0.03

    for i, cluster_id in enumerate(sorted(np.unique(cluster_labels))):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if genes.empty:
            continue

        cluster_expr = genes.mean(axis=0)
        common = cluster_expr.index.intersection(survival_df['sample'])
        if len(common) < 10:
            continue

        surv = survival_df[survival_df['sample'].isin(common)].copy()
        cluster_expr = cluster_expr[common]
        surv['expr'] = cluster_expr.values
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            continue

        grp = surv[surv['group'] == 'Low']
        if len(grp) < 2:
            continue

        kmf.fit(grp['OS.time'], grp['OS'], label=f"Cluster {cluster_id}")
        color = CLUSTER_COLORS.get(cluster_id, "gray")
        kmf.plot(ci_show=False, ax=ax, color=color, linewidth=2)

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
        cluster_ci_widths[cluster_id] = ci_width

        try:
            ix = surv['group'] == 'Low'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception:
            pval = np.nan
        cluster_pvals[cluster_id] = pval

    if ax.get_legend():
        ax.get_legend().remove()

    for i, cluster_id in enumerate(sorted(cluster_pvals.keys())):
        y_pos = text_y_start - i * line_spacing
        color = CLUSTER_COLORS.get(cluster_id, "gray")

        # Draw short horizontal color line at y_pos
        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=color,
            linewidth=1,
            transform=ax.transAxes,
            clip_on=False
        )

        # p-value text, horizontally aligned with CI text on same y_pos
        ax.text(
            text_x_start,
            y_pos,
            f"p = {cluster_pvals[cluster_id]:.4g}",
            fontsize=9,
            verticalalignment='center',
            transform=ax.transAxes
        )

        # CI text, placed right to p-value text with a horizontal gap
        ax.text(
            text_x_start + text_gap,
            y_pos,
            f"CI = {cluster_ci_widths[cluster_id]:.3f}",
            fontsize=9,
            verticalalignment='center',
            transform=ax.transAxes
        )

    plt.title("Survival Curves for Low-Expression Patients by Cluster")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {output_path}")

def perform_survival_analysis_one_curve_per_cluster_with_ci(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_cluster=10
):
    print("üîç Starting survival analysis - High expression curves only...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    # Coordinates for annotation in axes fraction [0,1]
    text_y_start = 0.92
    text_y_step = 0.03
    line_length = 0.05  # length of color line in axes fraction
    line_x_start = 0.65  # starting x pos of color line
    pval_x = line_x_start + line_length + 0.01
    ci_x = pval_x + 0.12

    annotation_idx = 0

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_cluster or low_mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        # Fit KM only for high expression
        kmf.fit(T_high, E_high, label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=False,
                 color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank test: High vs Low
        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # Y position for current annotation (top to bottom)
        y_pos = text_y_start - annotation_idx * text_y_step

        # Draw short horizontal color line in axes fraction coordinates
        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=CLUSTER_COLORS.get(cluster_id, '#333333'),
            linewidth=1,
            transform=ax.transAxes,
            clip_on=False
        )

        # Draw p-value and CI side-by-side, horizontally aligned
        ax.text(
            pval_x,
            y_pos,
            f"p = {pval:.4g}",
            fontsize=10,
            verticalalignment='center',
            transform=ax.transAxes
        )
        ax.text(
            ci_x,
            y_pos,
            f"CI = {ci_width:.3f}",
            fontsize=10,
            verticalalignment='center',
            transform=ax.transAxes
        )

        annotation_idx += 1

    # Remove legend box if any
    if ax.get_legend():
        ax.get_legend().remove()

    plt.title("Survival Curves (High Expression Only)")
    plt.xlabel("Time (Days)")
    plt.ylabel("Survival Probability")
    plt.tight_layout()

    save_path = os.path.join(output_dir, "km_high_expression_per_cluster.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {save_path}")

def perform_survival_analysis(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # Same order as cluster_labels
    output_path: str
):
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.strip().str[:15]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    gene_to_cluster = {
        gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)
    }
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    cluster_pvals = {}
    cluster_ci_widths = {}

    text_y_start = 0.9
    line_x_start = 0.65
    line_length = 0.04
    text_x_start = line_x_start + line_length + 0.01
    text_gap = 0.16  # horizontal gap between p-value and CI text
    line_spacing = 0.04

    for i, cluster_id in enumerate(sorted(np.unique(cluster_labels))):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if genes.empty:
            continue

        cluster_expr = genes.mean(axis=0)
        common = cluster_expr.index.intersection(survival_df['sample'])
        if len(common) < 10:
            continue

        surv = survival_df[survival_df['sample'].isin(common)].copy()
        cluster_expr = cluster_expr[common]
        surv['expr'] = cluster_expr.values
        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})

        if surv['group'].nunique() < 2:
            continue

        grp = surv[surv['group'] == 'Low']
        if len(grp) < 2:
            continue

        kmf.fit(grp['OS.time'], grp['OS'], label=f"Cluster {cluster_id}")
        color = CLUSTER_COLORS.get(cluster_id, "gray")
        kmf.plot(ci_show=False, ax=ax, color=color, linewidth=2)

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
        cluster_ci_widths[cluster_id] = ci_width

        try:
            ix = surv['group'] == 'Low'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception:
            pval = np.nan
        cluster_pvals[cluster_id] = pval

    if ax.get_legend():
        ax.get_legend().remove()

    # Set large font for ticks
    ax.tick_params(axis='both', which='major', labelsize=14)

    for i, cluster_id in enumerate(sorted(cluster_pvals.keys())):
        y_pos = text_y_start - i * line_spacing
        color = CLUSTER_COLORS.get(cluster_id, "gray")

        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=color,
            linewidth=2,
            transform=ax.transAxes,
            clip_on=False
        )

        # Larger font size here
        ax.text(
            text_x_start,
            y_pos,
            f"p = {cluster_pvals[cluster_id]:.4g}",
            fontsize=16,
            verticalalignment='center',
            transform=ax.transAxes
        )

        ax.text(
            text_x_start + text_gap,
            y_pos,
            f"CI = {cluster_ci_widths[cluster_id]:.3f}",
            fontsize=16,
            verticalalignment='center',
            transform=ax.transAxes
        )

    # Larger font for axis labels and title
    ax.set_xlabel("Time (Days)", fontsize=16)
    ax.set_ylabel("Survival Probability", fontsize=16)
    ax.set_title("Survival Curves for Low-Expression Patients by Cluster", fontsize=18)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {output_path}")

def perform_survival_analysis_one_curve_per_cluster_with_ci(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_cluster=10
):
    print("üîç Starting survival analysis - High expression curves only...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    # Set larger font size for ticks
    ax.tick_params(axis='both', which='major', labelsize=14)

    # Coordinates for annotation in axes fraction [0,1]
    text_y_start = 0.92
    text_y_step = 0.04
    line_length = 0.05  # length of color line in axes fraction
    line_x_start = 0.65  # starting x pos of color line
    pval_x = line_x_start + line_length + 0.01
    ci_x = pval_x + 0.16

    annotation_idx = 0

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_cluster or low_mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        # Fit KM only for high expression
        kmf.fit(T_high, E_high, label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=False,
                 color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank test: High vs Low
        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # Y position for current annotation (top to bottom)
        y_pos = text_y_start - annotation_idx * text_y_step

        # Draw short horizontal color line in axes fraction coordinates
        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=CLUSTER_COLORS.get(cluster_id, '#333333'),
            linewidth=2,
            transform=ax.transAxes,
            clip_on=False
        )

        # Draw p-value and CI side-by-side, horizontally aligned with larger font
        ax.text(
            pval_x,
            y_pos,
            f"p = {pval:.4g}",
            fontsize=16,
            verticalalignment='center',
            transform=ax.transAxes
        )
        ax.text(
            ci_x,
            y_pos,
            f"CI = {ci_width:.3f}",
            fontsize=16,
            verticalalignment='center',
            transform=ax.transAxes
        )

        annotation_idx += 1

    # Remove legend box if any
    if ax.get_legend():
        ax.get_legend().remove()

    ax.set_xlabel("Time (Days)", fontsize=16)
    ax.set_ylabel("Survival Probability", fontsize=16)
    ax.set_title("Survival Curves (High Expression Only)", fontsize=18)

    plt.tight_layout()

    save_path = os.path.join(output_dir, "km_high_expression_per_cluster.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {save_path}")

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    clusters_to_plot=[0, 1, 2, 3]
):


    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    for cl in clusters_to_plot:
        col = f"cluster_{cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(8, 6))

        for group in [0, 1]:
            label = f"{col} {'Low' if group == 0 else 'High'}"
            kmf.fit(T[merged[group_col] == group], E[merged[group_col] == group], label=label)
            kmf.plot_survival_function(ci_show=True, ax=ax, linewidth=2)

        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"Log-rank test failed: {e}")
            pval = float('nan')

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        ax.set_title(f"Survival by {col} Expression", fontsize=18)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.tick_params(axis='both', labelsize=12)

        legend = ax.get_legend()
        if legend:
            legend.remove()

        # ‚úÖ Vertically stacked text in same X, shifted Y
        text_x = 0.65
        text_y_start = 0.85
        text_y_step = 0.05

        ax.text(
            text_x, text_y_start - text_y_step,
            f"CI = {ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )
        ax.text(
            text_x, text_y_start,
            f"p  = {pval:.4g}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()

    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def perform_survival_analysis_per_cluster(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,  # same order as cluster_labels
    output_dir: str
):
    print("üîç Starting survival analysis per cluster...")

    os.makedirs(output_dir, exist_ok=True)

    # --- Load survival data ---
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # üîë Drop duplicate samples
    if survival_df['sample'].duplicated().any():
        n_dup = survival_df['sample'].duplicated().sum()
        print(f"‚ö†Ô∏è Found {n_dup} duplicate sample IDs in survival data. Dropping duplicates...")
        survival_df = survival_df.drop_duplicates(subset='sample')

    # --- Load expression matrix (genes x patients) ---
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    # --- Filter for top-k genes ---
    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # --- Map genes to clusters ---
    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    summary_records = []

    for cluster_id in sorted(np.unique(cluster_labels)):
        print(f"üß¨ Cluster {cluster_id}...")

        cluster_genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster')
        if cluster_genes.empty:
            print(f"‚ö†Ô∏è Cluster {cluster_id} has no genes. Skipping.")
            continue

        cluster_expr = cluster_genes.mean(axis=0)  # average expression per patient

        # Find intersection
        common_samples = cluster_expr.index.intersection(survival_df['sample'])
        if len(common_samples) < 10:
            print(f"‚ö†Ô∏è Too few samples in cluster {cluster_id} ({len(common_samples)}). Skipping.")
            continue

        # Filter survival data and drop duplicates
        surv = survival_df[survival_df['sample'].isin(common_samples)].copy()
        surv = surv.drop_duplicates(subset='sample')

        # Make sure cluster_expr only has common samples
        cluster_expr = cluster_expr[common_samples].rename('expr')

        # Index align
        surv = surv.set_index('sample')
        surv = surv.loc[surv.index.intersection(cluster_expr.index)]
        cluster_expr = cluster_expr.loc[surv.index]

        surv['expr'] = cluster_expr

        surv = surv.reset_index()



        if surv['expr'].std() < 1e-5:
            print(f"‚ö†Ô∏è Cluster {cluster_id} has near-constant expression. Skipping.")
            continue

        surv['group'] = (surv['expr'] >= surv['expr'].median()).map({True: 'High', False: 'Low'})
        if surv['group'].nunique() < 2:
            print(f"‚ö†Ô∏è Cluster {cluster_id} has only one group. Skipping.")
            continue

        kmf = KaplanMeierFitter()
        plt.figure(figsize=(6, 5))
        ax = plt.gca()

        for name, grp in surv.groupby('group'):
            if len(grp) < 2:
                continue
            kmf.fit(grp['OS.time'], grp['OS'], label=name)
            kmf.plot(ax=ax, ci_show=True, linewidth=2)

        # Confidence interval width
        ci_widths = []
        for name, grp in surv.groupby('group'):
            if len(grp) < 2:
                continue
            kmf.fit(grp['OS.time'], grp['OS'])
            ci_df = kmf.confidence_interval_
            ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
            ci_widths.append(ci_width)
        avg_ci_width = np.mean(ci_widths) if ci_widths else np.nan

        try:
            ix = surv['group'] == 'High'
            result = logrank_test(
                surv.loc[ix, 'OS.time'], surv.loc[~ix, 'OS.time'],
                event_observed_A=surv.loc[ix, 'OS'],
                event_observed_B=surv.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # Vertically aligned annotation
        text_x = 0.7 * surv['OS.time'].max()
        text_y = 0.9
        ax.text(
            text_x, text_y,
            f"p = {pval:.4g}\nCI = {avg_ci_width:.3f}",
            fontsize=12,
            bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'),
            transform=ax.get_xaxis_transform(),
            verticalalignment='top'
        )

        if ax.get_legend():
            ax.get_legend().remove()

        plt.title(f"Cluster {cluster_id}", fontsize=16)
        plt.xlabel("Time (Days)", fontsize=12)
        plt.ylabel("Survival Probability", fontsize=12)
        plt.tight_layout()

        plot_path = os.path.join(output_dir, f"KM_cluster_{cluster_id}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved KM plot for cluster {cluster_id} to {plot_path}")

        summary_records.append({
            'cluster_id': cluster_id,
            'n_samples': len(surv),
            'p_value': pval,
            'avg_ci_width': avg_ci_width
        })

    summary_df = pd.DataFrame(summary_records)
    summary_csv_path = os.path.join(output_dir, "survival_cluster_summary.csv")
    summary_df.to_csv(summary_csv_path, index=False)
    print(f"\nüìÑ Summary saved to: {summary_csv_path}")

def perform_survival_analysis_per_cluster_high_low(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_group=10,
    cluster_colors=None  # Optional: dict { 'high': color, 'low': color }
):
    print("üîç Starting survival analysis ‚Äî High vs Low, separate plots...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])
    survival_df = survival_df.drop_duplicates(subset='sample')

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_group or low_mask.sum() < min_samples_per_group:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        fig, ax = plt.subplots(figsize=(8, 6))
        kmf = KaplanMeierFitter()

        # Plot HIGH group
        kmf.fit(T_high, E_high, label="High Expression")
        kmf.plot(ax=ax, ci_show=True,
                 color=cluster_colors.get('high', '#1f77b4') if cluster_colors else None,
                 linewidth=2)

        # Plot LOW group
        kmf.fit(T_low, E_low, label="Low Expression")
        kmf.plot(ax=ax, ci_show=True,
                 color=cluster_colors.get('low', '#ff7f0e') if cluster_colors else None,
                 linewidth=2)

        # Average CI width
        kmf.fit(T_high, E_high)
        ci_df_high = kmf.confidence_interval_
        ci_width_high = (ci_df_high.iloc[:, 1] - ci_df_high.iloc[:, 0]).mean()

        kmf.fit(T_low, E_low)
        ci_df_low = kmf.confidence_interval_
        ci_width_low = (ci_df_low.iloc[:, 1] - ci_df_low.iloc[:, 0]).mean()

        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # === ANNOTATION ===
        # Vertical stack: p-value on top, CI below it, same x, same left align
        text_x = 0.7
        text_y_p = 0.9
        text_y_ci = text_y_p - 0.05

        ax.text(
            text_x, text_y_p,
            f"p = {pval:.4g}",
            transform=ax.transAxes, fontsize=16,
            verticalalignment='center'
        )
        ax.text(
            text_x, text_y_ci,
            f"CI High = {ci_width_high:.3f}\nCI Low = {ci_width_low:.3f}",
            transform=ax.transAxes, fontsize=16,
            verticalalignment='top'
        )

        if ax.get_legend():
            ax.get_legend().set_title(f"Cluster {cluster_id}")

        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.set_title(f"Cluster {cluster_id}: High vs Low Expression", fontsize=16)
        ax.tick_params(axis='both', which='major', labelsize=12)

        plt.tight_layout()

        save_path = os.path.join(output_dir, f"km_cluster_{cluster_id}_high_low.png")
        plt.savefig(save_path, dpi=300)
        plt.close()

        print(f"‚úÖ Saved KM plot for cluster {cluster_id} to {save_path}")

    print("‚úÖ‚úÖ Done: separate plots for all clusters.")


def run_survival_analysis_by_cluster_(
    row_labels_bio,
    node_names_topk,
    survival_file,
    expression_file,
    output_dir,
    cluster_threshold="median",
    min_samples_per_group=5,
    cluster_colors=None  # Optional: dict { 'high': color, 'low': color }
):
    print("üöÄ Running survival analysis for clusters...")

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # --- Load survival data ---
    survival_df = pd.read_csv(survival_file, sep='\t')
    if 'sample' not in survival_df.columns and '_PATIENT' in survival_df.columns:
        survival_df['sample'] = survival_df['_PATIENT']
    survival_df['sample'] = survival_df['sample'].str.strip().str.upper()
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # --- Load expression data ---
    expr_df = pd.read_csv(expression_file, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:15]

    # --- Map genes to clusters ---
    cluster_to_genes = defaultdict(list)
    for gene, cluster in zip(node_names_topk, row_labels_bio):
        cluster_to_genes[cluster].append(gene.upper())

    # --- Calculate mean expression per cluster per patient ---
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # --- Merge survival + expression ---
    merged = survival_df.merge(patient_cluster_scores, left_on='sample', right_index=True)
    if merged.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    for cl in sorted(cluster_to_genes.keys()):
        col = f'cluster_{cl}'
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged['OS.time']
        E = merged['OS']

        # --- Robust splitting ---
        try:
            merged[f'group_{cl}'] = pd.qcut(
                merged[col],
                q=2,
                labels=['Low', 'High']
            )
        except ValueError:
            print(f"‚ö†Ô∏è Identical expression for cluster {cl} ‚Äî fallback to median.")
            threshold = merged[col].median()
            merged[f'group_{cl}'] = np.where(merged[col] > threshold, 'High', 'Low')

        if merged[f'group_{cl}'].nunique() < 2:
            print(f"‚ö†Ô∏è Could not split cluster {cl} into 2 groups. Skipping.")
            continue

        n_high = (merged[f'group_{cl}'] == 'High').sum()
        n_low = (merged[f'group_{cl}'] == 'Low').sum()
        if n_high < min_samples or n_low < min_samples:
            print(f"‚ö†Ô∏è Skipping cluster {cl} (too few samples: High={n_high}, Low={n_low})")
            continue

        fig, ax = plt.subplots(figsize=(8, 6))
        kmf = KaplanMeierFitter()
        ci_widths = []

        for group_name, grouped in merged.groupby(f'group_{cl}'):
            kmf.fit(grouped['OS.time'], grouped['OS'], label=group_name)
            kmf.plot_survival_function(ax=ax, ci_show=True, linewidth=2)

            # Collect CI for each group
            ci_df = kmf.confidence_interval_
            ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
            ci_widths.append(ci_width)

        avg_ci_width = np.mean(ci_widths)

        try:
            ix = merged[f'group_{cl}'] == 'High'
            result = logrank_test(
                merged.loc[ix, 'OS.time'],
                merged.loc[~ix, 'OS.time'],
                event_observed_A=merged.loc[ix, 'OS'],
                event_observed_B=merged.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cl}: {e}")
            pval = float('nan')

        ax.set_title(f"Survival by {col} Expression", fontsize=18)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.tick_params(axis='both', labelsize=12)

        if ax.get_legend():
            ax.get_legend().set_title(None)

        text_x = 0.65
        text_y_p = 0.85
        text_y_ci = text_y_p - 0.05

        ax.text(
            text_x, text_y_p,
            f"p = {pval:.4g}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )
        ax.text(
            text_x, text_y_ci,
            f"Avg CI = {avg_ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"survival_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved: {plot_path}")

    # --- Cox ---
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[['OS.time', 'OS'] + cox_cols].copy()
    cox_df.columns = ['time', 'event'] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col='time', event_col='event')
    cox_path = os.path.join(output_dir, "cox_summary.csv")
    cph.summary.to_csv(cox_path)
    print(f"üìÑ Saved Cox summary: {cox_path}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis_by_cluster_(
    row_labels_bio,
    node_names_topk,
    survival_file,
    expression_file,
    output_dir,
    cluster_threshold="median",
    min_samples=5
):
    print("üöÄ Running survival analysis for clusters...")
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # --- Load survival data ---
    survival_df = pd.read_csv(survival_file, sep='\t')
    if 'sample' not in survival_df.columns and '_PATIENT' in survival_df.columns:
        survival_df['sample'] = survival_df['_PATIENT']
    survival_df['sample'] = survival_df['sample'].str.strip().str.upper()
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # --- Load expression data ---
    expr_df = pd.read_csv(expression_file, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:15]

    # --- Map genes to clusters ---
    cluster_to_genes = defaultdict(list)
    for gene, cluster in zip(node_names_topk, row_labels_bio):
        cluster_to_genes[cluster].append(gene.upper())

    # --- Calculate mean expression per cluster per patient ---
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # --- Merge survival + expression ---
    merged = survival_df.merge(patient_cluster_scores, left_on='sample', right_index=True)
    if merged.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    for cl in sorted(cluster_to_genes.keys()):
        col = f'cluster_{cl}'
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged['OS.time']
        E = merged['OS']

        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        n_high = (merged[group_col] == 1).sum()
        n_low = (merged[group_col] == 0).sum()
        if n_high < min_samples or n_low < min_samples:
            print(f"‚ö†Ô∏è Skipping cluster {cl} (too few samples: High={n_high}, Low={n_low})")
            continue

        fig, ax = plt.subplots(figsize=(8, 6))
        kmf = KaplanMeierFitter()

        for group in [0, 1]:
            mask = merged[group_col] == group
            if mask.sum() < min_samples:
                print(f"‚ö†Ô∏è Group {group} too small for cluster {cl} (n={mask.sum()}). Skipping curve.")
                continue
            kmf.fit(T[mask], E[mask], label=f"{col} {'Low' if group == 0 else 'High'}")
            kmf.plot_survival_function(ax=ax, ci_show=True, linewidth=2)

        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cl}: {e}")
            pval = float('nan')

        # Use last CI interval for display
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        ax.set_title(f"Survival by {col} Expression", fontsize=18)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.tick_params(axis='both', labelsize=12)

        # Remove legend title if any
        if ax.get_legend():
            ax.get_legend().set_title(None)

        # --- Add vertically aligned annotation ---
        text_x = 0.65
        text_y_p = 0.85
        text_y_ci = text_y_p - 0.05

        ax.text(
            text_x, text_y_p,
            f"p = {pval:.4g}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )
        ax.text(
            text_x, text_y_ci,
            f"CI = {ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"survival_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved: {plot_path}")

    # --- Cox model summary ---
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[['OS.time', 'OS'] + cox_cols].copy()
    cox_df.columns = ['time', 'event'] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col='time', event_col='event')
    cox_path = os.path.join(output_dir, "cox_summary.csv")
    cph.summary.to_csv(cox_path)
    print(f"üìÑ Saved Cox summary: {cox_path}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis_from_relevance(
    expr_file,
    survival_file,
    node_names_topk,
    relevance_matrix_bio,
    output_dir=None,
    plot_title="Survival Analysis: Relevance-weighted Expression"
):
    """
    Perform survival analysis using relevance-weighted gene expression.
    Shows KM curves, CI width, and log-rank p-value inside the plot.

    Args:
        expr_file (str): Path to TCGA-KIRC.expression.tsv.
        survival_file (str): Path to TCGA-KIRC.survival.tsv.
        node_names_topk (List[str]): Top-k genes.
        relevance_matrix_bio (np.ndarray): [num_genes, 64] relevance scores.
        output_dir (str or None): Save plots if set, else show inline.
        plot_title (str): Plot title.
    """
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
    # === Load expression ===
    expr = pd.read_csv(expr_file, sep='\t', index_col=0)
    genes_found = expr.index.intersection(node_names_topk)
    if len(genes_found) == 0:
        raise ValueError("No top-k genes found in expression file.")
    expr = expr.loc[genes_found].T  # [samples x genes]

    # === Match relevance ===
    gene_to_idx = {gene: idx for idx, gene in enumerate(node_names_topk)}
    matched_idx = [gene_to_idx[g] for g in expr.columns]
    rel = relevance_matrix_bio[matched_idx]
    relevance_weights = rel.mean(axis=1)
    relevance_weights = (relevance_weights - relevance_weights.min()) / (relevance_weights.max() - relevance_weights.min())

    # === Risk score ===
    risk_scores = expr.to_numpy() @ relevance_weights  # [samples]

    # === Survival data ===
    surv = pd.read_csv(survival_file, sep='\t').set_index("sample")
    common_samples = expr.index.intersection(surv.index)
    if len(common_samples) < 10:
        raise ValueError("Too few samples with both expression and survival.")

    risk_scores = pd.Series(risk_scores, index=expr.index).loc[common_samples]
    surv = surv.loc[common_samples]

    # === Median split ===
    median = risk_scores.median()
    risk_group = (risk_scores > median).astype(int)

    # === KM plot ===
    kmf = KaplanMeierFitter()
    fig, ax = plt.subplots(figsize=(8, 6))
    ci_widths = []

    for label in [0, 1]:
        T = surv["OS.time"][risk_group == label]
        E = surv["OS"][risk_group == label]
        kmf.fit(T, E, label=f"Group {label}")
        kmf.plot_survival_function(ax=ax, ci_show=True, linewidth=2)
        
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
        ci_widths.append(ci_width)

    avg_ci = np.mean(ci_widths)

    # === Log-rank ===
    ix = risk_group == 1
    result = logrank_test(
        surv["OS.time"][ix],
        surv["OS.time"][~ix],
        event_observed_A=surv["OS"][ix],
        event_observed_B=surv["OS"][~ix]
    )
    pval = result.p_value

    # === Annotate ===
    text_x = 0.65
    text_y_start = 0.85
    text_y_step = 0.05

    ax.text(
        text_x, text_y_start,
        f"p = {pval:.4g}",
        transform=ax.transAxes,
        fontsize=16,
        verticalalignment='center'
    )
    ax.text(
        text_x, text_y_start - text_y_step,
        f"CI = {avg_ci:.3f}",
        transform=ax.transAxes,
        fontsize=16,
        verticalalignment='center'
    )

    ax.set_title(plot_title, fontsize=16)
    ax.set_xlabel("Time (Days)", fontsize=16)
    ax.set_ylabel("Survival Probability", fontsize=16)
    ax.tick_params(axis='both', which='major', labelsize=12)

    plt.tight_layout()

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        plt.savefig(os.path.join(output_dir, "km_plot_by_relevance_risk.png"), dpi=300)
        plt.close()
        print(f"‚úÖ KM plot saved to: {os.path.join(output_dir, 'km_plot_by_relevance_risk.png')}")
    else:
        plt.close()

    # === Cox PH ===
    cox_df = pd.DataFrame({
        "risk": risk_scores,
        "time": surv["OS.time"],
        "event": surv["OS"]
    })

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    if output_dir:
        with open(os.path.join(output_dir, "cox_summary.txt"), "w") as f:
            cph.print_summary(stream=f)
        print(f"‚úÖ Cox summary saved to: {os.path.join(output_dir, 'cox_summary.txt')}")
    else:
        cph.print_summary()

def run_survival_analysis_multigroup_from_relevance(
    expr_file,
    survival_file,
    node_names_topk,
    relevance_matrix_bio,
    cluster_labels,  # shape: [len(node_names_topk)]
    output_dir=None,
    plot_title="Survival Analysis: Relevance-weighted Cluster Assignment"
):
    """
    Perform survival analysis using per-cluster relevance-weighted expression,
    assigning each patient to the cluster with highest score.

    Args:
        expr_file (str): Path to TCGA-KIRC.expression.tsv.
        survival_file (str): Path to TCGA-KIRC.survival.tsv.
        node_names_topk (List[str]): Top-k genes.
        relevance_matrix_bio (np.ndarray): [num_genes, 64] relevance matrix.
        cluster_labels (np.ndarray): Cluster assignment per gene.
        output_dir (str): Where to save plots and summary.
        plot_title (str): Title for the KM plot.
    """
    
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
    expr = pd.read_csv(expr_file, sep='\t', index_col=0)
    expr.index = expr.index.str.upper()
    genes_found = expr.index.intersection(node_names_topk)
    if len(genes_found) == 0:
        raise ValueError("‚ùå No top-k genes found in expression matrix.")
    expr = expr.loc[genes_found].T  # [samples x genes]

    gene_to_idx = {gene: idx for idx, gene in enumerate(node_names_topk)}
    matched_idx = [gene_to_idx[g] for g in expr.columns]
    rel = relevance_matrix_bio[matched_idx]
    rel_weights = (rel - rel.min()) / (rel.max() - rel.min())  # [genes x dims]
    cluster_ids = cluster_labels[matched_idx]

    # Aggregate relevance weights by cluster
    cluster_to_gene_idx = {cl: [] for cl in np.unique(cluster_ids)}
    for i, cl in enumerate(cluster_ids):
        cluster_to_gene_idx[cl].append(i)

    patient_cluster_scores = pd.DataFrame(index=expr.index)
    for cl, idxs in cluster_to_gene_idx.items():
        genes = expr.columns[idxs]
        weights = rel_weights[idxs].mean(axis=1)  # avg weight per gene in cluster
        weighted_expr = expr[genes] @ weights
        patient_cluster_scores[f"cluster_{cl}"] = weighted_expr

    # Assign each patient to cluster with highest score
    patient_groups = patient_cluster_scores.idxmax(axis=1).str.extract(r"cluster_(\d+)")[0].astype(int)

    # Load survival
    surv = pd.read_csv(survival_file, sep="\t").set_index("sample")
    common_samples = expr.index.intersection(surv.index)
    if len(common_samples) < 10:
        raise ValueError("Too few samples matched between expression and survival.")

    surv = surv.loc[common_samples]
    patient_groups = patient_groups.loc[common_samples]
    patient_cluster_scores = patient_cluster_scores.loc[common_samples]

    # === KM plot ===
    fig, ax = plt.subplots(figsize=(8, 6))
    kmf = KaplanMeierFitter()
    ci_widths = []

    for cl in sorted(patient_groups.unique()):
        mask = patient_groups == cl
        T, E = surv["OS.time"][mask], surv["OS"][mask]
        if len(T) < 5:
            print(f"‚ö†Ô∏è Cluster {cl} has too few patients ({len(T)}). Skipping.")
            continue
        kmf.fit(T, E, label=f"Cluster {cl}")
        kmf.plot_survival_function(ax=ax, ci_show=True, linewidth=2)

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()
        ci_widths.append(ci_width)

    # Log-rank test across >2 groups
    T_all, E_all = surv["OS.time"], surv["OS"]
    result = multivariate_logrank_test(T_all, patient_groups, E_all)
    pval = result.p_value
    avg_ci = np.mean(ci_widths) if ci_widths else float("nan")

    # Annotation
    text_x = 0.65
    text_y_start = 0.85
    text_y_step = 0.05

    ax.text(
        text_x, text_y_start,
        f"p = {pval:.4g}",
        transform=ax.transAxes,
        fontsize=16,
        verticalalignment='center'
    )
    ax.text(
        text_x, text_y_start - text_y_step,
        f"Avg CI = {avg_ci:.3f}",
        transform=ax.transAxes,
        fontsize=16,
        verticalalignment='center'
    )

    ax.set_title(plot_title, fontsize=16)
    ax.set_xlabel("Time (Days)", fontsize=16)
    ax.set_ylabel("Survival Probability", fontsize=16)
    ax.tick_params(axis='both', which='major', labelsize=12)
    plt.tight_layout()

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        fig.savefig(os.path.join(output_dir, "km_multigroup_by_cluster.png"), dpi=300)
        plt.close()
        print(f"‚úÖ KM plot saved to {output_dir}/km_multigroup_by_cluster.png")
    else:
        plt.close()

    # Optional: save per-patient cluster assignments
    if output_dir:
        patient_groups.to_csv(os.path.join(output_dir, "patient_cluster_assignment.tsv"), sep='\t')

def run_survival_analysis_by_cluster_no_debug(
    row_labels_bio,
    node_names_topk,
    survival_file,
    expression_file,
    output_dir,
    cluster_threshold="median",
    min_samples=10
):
    print("üöÄ Running survival analysis for clusters...")

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # --- Load survival data ---
    survival_df = pd.read_csv(survival_file, sep='\t')
    if 'sample' not in survival_df.columns and '_PATIENT' in survival_df.columns:
        survival_df['sample'] = survival_df['_PATIENT']
    survival_df['sample'] = survival_df['sample'].str.strip().str.upper()
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # --- Load expression data ---
    expr_df = pd.read_csv(expression_file, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:15]

    # --- Map genes to clusters ---
    cluster_to_genes = defaultdict(list)
    for gene, cluster in zip(node_names_topk, row_labels_bio):
        cluster_to_genes[cluster].append(gene.upper())

    # --- Calculate mean expression per cluster per patient ---
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # --- Merge survival + expression ---
    merged = survival_df.merge(patient_cluster_scores, left_on='sample', right_index=True)
    if merged.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    for cl in sorted(cluster_to_genes.keys()):
        col = f'cluster_{cl}'
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged['OS.time']
        E = merged['OS']

        # --- Split patients robustly ---
        try:
            merged[f'group_{cl}'] = pd.qcut(
                merged[col],
                q=2,
                labels=['Low', 'High']
            )
        except ValueError:
            print(f"‚ö†Ô∏è Identical expression for cluster {cl} ‚Äî fallback to median.")
            threshold = merged[col].median()
            merged[f'group_{cl}'] = np.where(merged[col] > threshold, 'High', 'Low')

        if merged[f'group_{cl}'].nunique() < 2:
            print(f"‚ö†Ô∏è Could not split cluster {cl} into 2 groups. Skipping.")
            continue

        n_high = (merged[f'group_{cl}'] == 'High').sum()
        n_low = (merged[f'group_{cl}'] == 'Low').sum()
        if n_high < min_samples or n_low < min_samples:
            print(f"‚ö†Ô∏è Skipping cluster {cl} (too few samples: High={n_high}, Low={n_low})")
            continue

        fig, ax = plt.subplots(figsize=(8, 6))
        kmf = KaplanMeierFitter()

        for group_name, grouped in merged.groupby(f'group_{cl}'):
            kmf.fit(grouped['OS.time'], grouped['OS'], label=group_name)
            kmf.plot_survival_function(ax=ax, ci_show=True, linewidth=2)

        try:
            ix = merged[f'group_{cl}'] == 'High'
            result = logrank_test(
                merged.loc[ix, 'OS.time'],
                merged.loc[~ix, 'OS.time'],
                event_observed_A=merged.loc[ix, 'OS'],
                event_observed_B=merged.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cl}: {e}")
            pval = float('nan')

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        ax.set_title(f"Survival by {col} Expression", fontsize=18)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.tick_params(axis='both', labelsize=12)

        if ax.get_legend():
            ax.get_legend().set_title(None)

        text_x = 0.65
        text_y_p = 0.85
        text_y_ci = text_y_p - 0.05

        ax.text(
            text_x, text_y_p,
            f"p = {pval:.4g}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )
        ax.text(
            text_x, text_y_ci,
            f"CI = {ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"survival_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved: {plot_path}")

    # --- Cox ---
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[['OS.time', 'OS'] + cox_cols].copy()
    cox_df.columns = ['time', 'event'] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col='time', event_col='event')
    cox_path = os.path.join(output_dir, "cox_summary.csv")
    cph.summary.to_csv(cox_path)
    print(f"üìÑ Saved Cox summary: {cox_path}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis_by_cluster_empty_output(
    row_labels_bio,
    node_names_topk,
    survival_file,
    expression_file,
    output_dir,
    cluster_threshold="median",
    min_samples=10
):
    print("üöÄ Running survival analysis for clusters...")

    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # --- Load survival data ---
    survival_df = pd.read_csv(survival_file, sep='\t')
    print(f"üìÑ Loaded survival data: {survival_df.shape}")

    if 'sample' not in survival_df.columns and '_PATIENT' in survival_df.columns:
        survival_df['sample'] = survival_df['_PATIENT']
    survival_df['sample'] = survival_df['sample'].str.strip().str.upper()
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    print("üîç First survival samples:")
    print(survival_df['sample'].head())

    # --- Load expression data ---
    expr_df = pd.read_csv(expression_file, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]  # Try [:12] for TCGA

    print(f"üìÑ Loaded expression data: {expr_df.shape}")
    print("üîç First expression samples (columns):")
    print(expr_df.columns[:5])

    # --- Map genes to clusters ---
    cluster_to_genes = defaultdict(list)
    for gene, cluster in zip(node_names_topk, row_labels_bio):
        cluster_to_genes[cluster].append(gene.upper())

    # --- Calculate mean expression per cluster per patient ---
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)

    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        print(f"‚û°Ô∏è Cluster {cl}: {len(genes)} genes mapped, {len(valid_genes)} valid in expression matrix")
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    print("üóÇÔ∏è Patient cluster scores:")
    print(patient_cluster_scores.head())

    # --- Merge survival + expression ---
    merged = survival_df.merge(patient_cluster_scores, left_on='sample', right_index=True)
    print(f"üîó Merged data shape: {merged.shape}")
    print(merged.head())

    if merged.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    for cl in sorted(cluster_to_genes.keys()):
        col = f'cluster_{cl}'
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found in merged. Skipping.")
            continue

        # Drop missing scores
        merged = merged.dropna(subset=[col])

        print(f"üìä Cluster {cl}: Expression score stats:")
        print(merged[col].describe())

        T = merged['OS.time']
        E = merged['OS']

        # --- Split patients robustly ---
        try:
            merged[f'group_{cl}'] = pd.qcut(
                merged[col],
                q=2,
                labels=['Low', 'High']
            )
            print(f"‚úÖ Split cluster {cl} with qcut.")
        except ValueError:
            print(f"‚ö†Ô∏è Identical expression for cluster {cl} ‚Äî fallback to median.")
            threshold = merged[col].median()
            merged[f'group_{cl}'] = np.where(merged[col] > threshold, 'High', 'Low')

        print(f"üìä Group counts for cluster {cl}:")
        print(merged[f'group_{cl}'].value_counts())

        if merged[f'group_{cl}'].nunique() < 2:
            print(f"‚ö†Ô∏è Could not split cluster {cl} into 2 groups. Skipping.")
            continue

        n_high = (merged[f'group_{cl}'] == 'High').sum()
        n_low = (merged[f'group_{cl}'] == 'Low').sum()
        print(f"üìè Cluster {cl} counts: High={n_high}, Low={n_low}")

        if n_high < min_samples or n_low < min_samples:
            print(f"‚ö†Ô∏è Skipping cluster {cl} (too few samples: High={n_high}, Low={n_low})")
            continue

        # --- KM plot ---
        fig, ax = plt.subplots(figsize=(8, 6))
        kmf = KaplanMeierFitter()

        for group_name, grouped in merged.groupby(f'group_{cl}'):
            kmf.fit(grouped['OS.time'], grouped['OS'], label=group_name)
            kmf.plot_survival_function(ax=ax, ci_show=True, linewidth=2)

        try:
            ix = merged[f'group_{cl}'] == 'High'
            result = logrank_test(
                merged.loc[ix, 'OS.time'],
                merged.loc[~ix, 'OS.time'],
                event_observed_A=merged.loc[ix, 'OS'],
                event_observed_B=merged.loc[~ix, 'OS']
            )
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cl}: {e}")
            pval = float('nan')

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        ax.set_title(f"Survival by {col} Expression", fontsize=18)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.tick_params(axis='both', labelsize=12)

        if ax.get_legend():
            ax.get_legend().set_title(None)

        text_x = 0.65
        text_y_p = 0.85
        text_y_ci = text_y_p - 0.05

        ax.text(
            text_x, text_y_p,
            f"p = {pval:.4g}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )
        ax.text(
            text_x, text_y_ci,
            f"CI = {ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"survival_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved: {plot_path}")

    # --- Cox ---
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    if not cox_cols:
        print("‚ö†Ô∏è No valid cluster columns for Cox. Skipping Cox fit.")
        return

    cox_df = merged[['OS.time', 'OS'] + cox_cols].copy()
    cox_df.columns = ['time', 'event'] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col='time', event_col='event')
    cox_path = os.path.join(output_dir, "cox_summary.csv")
    cph.summary.to_csv(cox_path)
    print(f"üìÑ Saved Cox summary: {cox_path}")

    return cph.summary, merged, patient_cluster_scores

def plot_collapsed_clusterfirst_multilevel_sankey_bio(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    output_dir,
    CLUSTER_COLORS
):


    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]
    selected_known_genes = ["EGFR", "BRCA2"]

    combined_genes = []
    seen = set()
    # for g in top_scored_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        graph_node_idx = name_to_index[gene]
        gene_cluster = cluster_assignments[graph_node_idx]
        cluster_label = f"Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

    # üìÅ Save gene-neighbor relationships (CSV export)
    all_neighbor_rows = []
    for gene, neighbors in gene_to_neighbors.items():
        graph_node_idx = name_to_index[gene]
        gene_cluster = int(cluster_assignments[graph_node_idx])
        for neighbor_idx, score in neighbors.items():
            if neighbor_idx >= len(cluster_assignments):
                continue
            neighbor_name = node_id_to_name[neighbor_idx]
            neighbor_cluster = int(cluster_assignments[neighbor_idx])
            # üßº Skip neighbors with cluster -1
            if neighbor_cluster == -1:
                continue
            all_neighbor_rows.append([
                gene,
                neighbor_name,
                str(neighbor_idx),
                float(score),
                neighbor_cluster
            ])

    # üíæ Write CSV
    os.makedirs(output_dir, exist_ok=True)
    neighbors_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_gene_neighbors_epo{args.num_epochs}.csv"
    )
    with open(neighbors_csv_path, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Neighbor_Cluster"])
        writer.writerows(all_neighbor_rows)
    print(f"üìÑ Saved gene-neighbor relevance data to: {neighbors_csv_path}")

    source = []
    target = []
    value = []
    link_colors = []

    existing_edges = set()  # Track (source, target) to avoid A‚ÜíB and B‚ÜíA cycles

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                # saliency = relevance_scores[rel_idx].sum().item()
                # gene_cluster = row_labels[rel_idx]
                # color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                # all_colors.append(color)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                graph_node_idx = name_to_index[gene]
                gene_cluster = cluster_assignments[graph_node_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)

            gene_idx = label_to_idx[gene]

            # Cluster ‚Üí Gene
            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]

                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nCluster {neighbor_cluster}"


                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                # Gene ‚Üí Neighbor (skip if reverse exists)
                # if (neighbor_node_idx, gene_idx) not in existing_edges:
                #     source.append(gene_idx)
                #     target.append(neighbor_node_idx)
                #     value.append(neighbor_score)
                #     link_colors.append("rgba(160,160,160,0.5)")
                #     existing_edges.add((gene_idx, neighbor_node_idx))

                # Neighbor ‚Üí Cluster (skip if reverse exists)
                if (neighbor_cluster_idx, neighbor_node_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

                # Only allow Gene ‚Üí Neighbor if Neighbor ‚Üí Cluster exists
                if (neighbor_node_idx, neighbor_cluster_idx) not in existing_edges:
                    continue

                if (gene_idx, neighbor_node_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                if (neighbor_node_idx, neighbor_cluster_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))
                    
    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"‚úÖ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"üñºÔ∏è PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)", fontsize=20)
    plt.xlabel("Entropy", fontsize=18)
    plt.ylabel("Cluster", fontsize=18)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()


    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"C1": c1, "C2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    
    # Use only clusters that appeared in the entropy plot
    # entropy_clusters = list(entropy.keys())  # or entropy_df["Cluster"].tolist() for sorted order
    # pivot_df = jaccard_df.pivot(index="C1", columns="C2", values="Jaccard")
    # pivot_df = pivot_df.reindex(index=entropy_clusters, columns=entropy_clusters).fillna(0)

    # all_clusters = sorted(set([k for k, _ in jaccard.keys()] + [k for _, k in jaccard.keys()]))
    # pivot_df = jaccard_df.pivot(index="C1", columns="C2", values="Jaccard").reindex(index=all_clusters, columns=all_clusters).fillna(0)

    pivot_df = jaccard_df.pivot(index="C1", columns="C2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, 
        cbar_kws={'label': 'Jaccard Index'}
    )
    plt.title("Jaccard Similarity Between Gene Clusters", fontsize=20)
    plt.xlabel("C2", fontsize=18)
    plt.ylabel("C1", fontsize=18)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()


    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)", fontsize=20)
    plt.xlabel("Centrality", fontsize=18)
    plt.ylabel("Gene", fontsize=18)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()


    return sankey_stats


def plot_confirmed_neighbors_bio_topk_integer(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes,
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores,
    node_names_topk  # ‚Üê Use top 1000 genes
):

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # ‚ú® Filter predicted genes that exist in relevance_scores
    valid_genes = [g for g in predicted_cancer_genes if g in topk_name_to_index]
    
    # ‚ú® Sort by total saliency (descending)
    valid_genes_sorted = sorted(
        valid_genes,
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    # ‚ú® Select top 1000
    selected_genes = valid_genes_sorted[:node_names_topk]

    # ‚úÖ Get neighbor dictionary
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, selected_genes)

    all_neighbor_rows = []

    for gene in selected_genes:
        node_idx = topk_name_to_index[gene]
        gene_score = relevance_scores[node_idx].sum().item()
        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()

        print(f"{gene} ‚Üí Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        neighbor_scores = get_top_neighbors(
            gene,
            neighbors,
            name_to_index,
            topk_name_to_index,
            relevance_scores,
            node_id_to_name,
            output_dir=None,
            k=10
        )

        if not neighbor_scores:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        for neighbor_idx, score in sorted(neighbor_scores.items(), key=lambda x: -x[1]):
            neighbor_name = node_id_to_name.get(neighbor_idx, "UNK")
            all_neighbor_rows.append([
                gene,
                neighbor_name,
                neighbor_idx,
                score,
                gene_cluster
            ])

        # Optional: comment out to skip plotting for 1000 genes
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )
        plot_neighbor_relevance(
            neighbor_scores=neighbor_scores,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

    # ‚úÖ Save one CSV for all gene-neighbor relationships
    combined_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_topk_confirmed_neighbors_epo{args.num_epochs}.csv"
    )

    with open(combined_csv_path, "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"])
        writer.writerows(all_neighbor_rows)

    print(f"üìÑ Combined confirmed neighbor CSV saved to: {combined_csv_path}")

def get_neighbors_gene_names_safe_version(graph, node_names, name_to_index, genes):
    neighbors_dict = {}

    for gene in genes:
        if gene not in name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not in name_to_index. Skipping.")
            continue

        idx = name_to_index[gene]

        if idx not in graph.nodes:
            print(f"‚ö†Ô∏è Index {idx} for gene {gene} not in graph. Skipping.")
            continue

        neighbors = list(graph.successors(idx))
        neighbor_names = [node_names[n] for n in neighbors if n < len(node_names)]
        neighbors_dict[gene] = neighbor_names

    return neighbors_dict

def get_neighbors_gene_names_(graph, node_names, name_to_index, genes):
    neighbors_dict = {}
    for gene in genes:
        if gene in name_to_index:
            idx = name_to_index[gene]
            neighbors = graph.successors(idx).tolist()
            neighbor_names = [node_names[n] for n in neighbors]
            neighbors_dict[gene] = neighbor_names
    return neighbors_dict

def plot_confirmed_neighbors_bio(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes,
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores,
    node_names_topk  # ‚Üê a list of top-k gene names
):

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # ‚úÖ Filter: only genes that are in predicted list, in graph, and in topk
    valid_genes = [
        g for g in predicted_cancer_genes
        if g in topk_name_to_index and g in node_names_topk
    ]

    # ‚úÖ Sort by total relevance score (descending)
    valid_genes_sorted = sorted(
        valid_genes,
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    # ‚úÖ Final selected genes: keep them all
    selected_genes = valid_genes_sorted

    print(f"üîç Selected {len(selected_genes)} genes for neighbor analysis")

    # ‚úÖ Get neighbor dictionary
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, selected_genes)

    all_neighbor_rows = []

    for gene in selected_genes:
        node_idx = topk_name_to_index[gene]
        gene_score = relevance_scores[node_idx].sum().item()
        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()

        print(f"{gene} ‚Üí Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        neighbor_scores = get_top_neighbors(
            gene,
            neighbors,
            name_to_index,
            topk_name_to_index,
            relevance_scores,
            node_id_to_name,
            output_dir=None,
            k=10
        )

        if not neighbor_scores:
            print(f"‚ö†Ô∏è No valid neighbors found for {gene}.")
            continue

        for neighbor_idx, score in sorted(neighbor_scores.items(), key=lambda x: -x[1]):
            neighbor_name = node_id_to_name.get(neighbor_idx, "UNK")
            all_neighbor_rows.append([
                gene,
                neighbor_name,
                neighbor_idx,
                score,
                gene_cluster
            ])

        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )
        plot_neighbor_relevance(
            neighbor_scores=neighbor_scores,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

    os.makedirs(output_dir, exist_ok=True)

    combined_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_topk_confirmed_neighbors_epo{args.num_epochs}.csv"
    )

    with open(combined_csv_path, "w", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"])
        writer.writerows(all_neighbor_rows)

    print(f"üìÑ Combined confirmed neighbor CSV saved to: {combined_csv_path}")


def plot_neighbor_relevance_by_mode(
    gene,
    relevance_scores,   # ‚Üê Full relevance matrix (array)
    mode,
    neighbor_scores,    # ‚Üê dict of local neighbor scores
    neighbors_dict,
    name_to_index,
    node_id_to_name,
    graph,
    row_labels,
    total_clusters,
    args,
    save_dir="results/gene_prediction/neighbor_feature_contributions/"
):
    if gene not in name_to_index:
        print(f"‚ö†Ô∏è Gene {gene} not found in the graph.")
        return

    node_idx = name_to_index[gene]
    # ‚úÖ Use relevance_scores for gene score, NOT neighbor_scores
    gene_score = relevance_scores[node_idx].sum().item()

    if mode == "bio":
        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
    elif mode == "topo":
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()
    else:
        gene_cluster = -1

    print(f"[{mode.upper()}] {gene} ‚Üí Node {node_idx} | Predicted score: {gene_score:.4f} | Cluster: {gene_cluster}")

    neighbors = neighbors_dict.get(gene, [])
    neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

    relevance_vals = [relevance_scores[i].sum().item() for i in neighbor_indices]
    scores_dict = dict(zip(neighbor_indices, relevance_vals))

    cluster_str = f"cluster_{gene_cluster}" if gene_cluster >= 0 else "cluster_unassigned"
    cluster_dir = os.path.join(save_dir, mode.lower(), cluster_str)
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(
        cluster_dir,
        f"{args.model_type}_{args.net_type}_{gene}_{mode}_neighbor_relevance_epo{args.num_epochs}.png"
    )

    plot_neighbor_relevance(
        neighbor_scores=scores_dict,
        gene_name=f"{gene} (Cluster {gene_cluster})",
        node_id_to_name=node_id_to_name,
        output_path=output_path,
        row_labels=row_labels,
        total_clusters=total_clusters,
        add_legend=False
    )

def plot_neighbor_relevance_by_mode_(
    gene,
    relevance_scores,
    mode,
    neighbor_scores,
    neighbors_dict,
    name_to_index,
    node_id_to_name,
    graph,
    row_labels,
    total_clusters,
    args,
    save_dir="results/gene_prediction/neighbor_feature_contributions/"
):
    os.makedirs(save_dir, exist_ok=True)

    if gene not in name_to_index:
        print(f"‚ö†Ô∏è Gene {gene} not found in the graph.")
        return

    node_idx = name_to_index[gene]

    # ‚úÖ Safe lookup
    gene_score = neighbor_scores[node_idx] if isinstance(neighbor_scores, np.ndarray) else neighbor_scores.get(node_idx, 0.0)

    # ‚úÖ Get cluster by mode
    if mode == "bio":
        gene_cluster = int(graph.ndata["cluster_bio"][node_idx].item())
    elif mode == "topo":
        gene_cluster = int(graph.ndata["cluster_topo"][node_idx].item())
    else:
        gene_cluster = -1

    print(f"[{mode}] {gene} ‚Üí Node {node_idx} | Predicted score: {gene_score:.4f} | Cluster: {gene_cluster}")

    neighbors = neighbors_dict.get(gene, [])
    neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

    relevance_vals = [relevance_scores[i].sum().item() for i in neighbor_indices]
    scores_dict = dict(zip(neighbor_indices, relevance_vals))

    output_path = os.path.join(
        save_dir,
        f"{args.model_type}_{args.net_type}_{gene}_{mode}_neighbor_relevance_epo{args.num_epochs}.png"
    )

    plot_neighbor_relevance(
        neighbor_scores=scores_dict,
        gene_name=gene,   # ‚úÖ only gene name!
        gene_cluster=gene_cluster,  # ‚úÖ pass real cluster ID
        node_id_to_name=node_id_to_name,
        output_path=output_path,
        row_labels=row_labels,
        total_clusters=total_clusters,
        add_legend=False
    )

def plot_neighbor_relevance_(
    neighbor_scores,
    gene_name,
    gene_cluster,   # ‚úÖ separate arg!
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter self-links
    filtered = {
        k: v for k, v in neighbor_scores.items()
        if v > 0.0 and node_id_to_name.get(k, "") != gene_name
    }

    # Top 10 + pad
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])
    while len(top_neighbors) < 10:
        top_neighbors[f"dummy_{len(top_neighbors)}"] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            try:
                nid_int = int(nid)
                if nid_int < len(row_labels):
                    cid = int(row_labels[nid_int])
                    cluster_ids.append(cid)
                    colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
                else:
                    colors.append("gray")
            except Exception:
                colors.append("gray")
        else:
            colors.append("white")

    # Inject cluster folder
    output_path = Path(output_path)
    cluster_subdir = output_path.parent / f"cluster_{gene_cluster if gene_cluster >= 0 else 'unassigned'}"
    output_path = cluster_subdir / output_path.name
    cluster_subdir.mkdir(parents=True, exist_ok=True)

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name} (Cluster {gene_cluster})", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"‚úÖ Saved neighbor relevance plot to {output_path}")

def get_neighbors_gene_names_topk(graph, node_names, name_to_index, genes):
    """
    For each gene, get its neighbors, but keep only those that are also in `genes`.
    Filtering happens at index level for efficiency.
    """
    # For fast reverse lookup: gene names ‚Üí indices
    genes_idx_set = set(name_to_index[g] for g in genes if g in name_to_index)

    neighbors_dict = {}

    for gene in genes:
        if gene not in name_to_index:
            continue

        idx = name_to_index[gene]

        # Get raw neighbors
        neighbors = graph.successors(idx).tolist()

        # ‚úÖ Filter neighbor indices to those in genes_idx_set
        filtered_neighbors = [n for n in neighbors if n in genes_idx_set]

        # Convert back to names
        neighbor_names = [node_names[n] for n in filtered_neighbors]

        neighbors_dict[gene] = neighbor_names

    return neighbors_dict

def pathway_enrichment(
    bio_embeddings_np,
    best_k,
    node_names,
    tag,
    cancer_list=None,
    sources=None,
    top_n=20
):
    """
    Runs clustering, enrichment, and dot-plot for multiple cancers and sources.

    Args:
        bio_embeddings_np: np.ndarray of embeddings.
        best_k: Number of clusters for biclustering.
        node_names: List of node names.
        tag: Label for the run, e.g. "bio".
        cancer_list: List of cancer types. Defaults to 16 common ones.
        sources: List of enrichment sources. Defaults to ['REAC', 'KEGG', 'HP', 'GO:BP'].
        top_n: How many top terms to plot.
    """
    if cancer_list is None:
        cancer_list = [
            'BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP',
            'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC'
        ]

    if sources is None:
        sources = ['REAC', 'KEGG', 'HP', 'GO:BP']

    for cancer_type in cancer_list:
        print(f"\nüî¨ Cancer: {cancer_type}")

        # 1Ô∏è‚É£ Extract features
        cancer_feature = extract_all_omics_for_cancer(bio_embeddings_np, cancer_type)

        # 2Ô∏è‚É£ Cluster
        row_labels = apply_full_spectral_biclustering_cancer(
            cancer_feature, n_clusters=best_k
        )
        cluster_dict = make_cluster_dict(row_labels, node_names)

        # 3Ô∏è‚É£ Enrichment
        enrichment_files = run_gprofiler_enrichment(
            cluster_dict, cancer_type, tag
        )

        # 4Ô∏è‚É£ Plot for each source
        for source in sources:
            print(f"üìä Plotting {source} enrichment for {cancer_type}...")

            terms = collect_enrichment_with_ratios(
                cancer_type=cancer_type,
                tag=tag,
                source=source,
                top_n=top_n
            )

            draw_dot_plot_with_ratio(
                terms=terms,
                cancer_type=cancer_type,
                source=source,
                top_n=top_n
            )

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    clusters_to_plot=[0, 1, 2, 3]
):


    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    for cl in clusters_to_plot:
        col = f"cluster_{cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(8, 6))

        for group in [0, 1]:
            label = f"{col} {'Low' if group == 0 else 'High'}"
            kmf.fit(T[merged[group_col] == group], E[merged[group_col] == group], label=label)
            kmf.plot_survival_function(ci_show=True, ax=ax, linewidth=2)

        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"Log-rank test failed: {e}")
            pval = float('nan')

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        ax.set_title(f"Survival by {col} Expression", fontsize=18)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.tick_params(axis='both', labelsize=12)

        legend = ax.get_legend()
        if legend:
            legend.remove()


        # Fit Cox model for this cluster only
        cox_df = merged[[ "OS.time", "OS", col ]].copy()
        cox_df.columns = ["time", "event", "cluster_score"]

        cph_single = CoxPHFitter()
        cph_single.fit(cox_df, duration_col="time", event_col="event")
        hr = cph_single.hazard_ratios_["cluster_score"]

        # Mean CI already computed above: ci_width

        # ‚úÖ Annotation block: CI ‚Üí HR ‚Üí p-value
        text_x = 0.03
        text_y_start = 0.12
        text_y_step = 0.045

        # CI on top
        ax.text(
            text_x, text_y_start,
            f"CI = {ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )

        # HR next
        ax.text(
            text_x, text_y_start - text_y_step,
            f"HR = {hr:.3f}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )

        # p-value at bottom
        ax.text(
            text_x, text_y_start - 2 * text_y_step,
            f"p = {pval:.4g}",
            transform=ax.transAxes,
            fontsize=16,
            verticalalignment='center'
        )



        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()

    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

# _high_low
def perform_survival_analysis_per_cluster(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_group=5,
    cluster_colors=None  # Optional: dict { 'high': color, 'low': color }
):
    print("üîç Starting survival analysis ‚Äî High vs Low, separate plots...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])
    survival_df = survival_df.drop_duplicates(subset='sample')

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_group or low_mask.sum() < min_samples_per_group:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        fig, ax = plt.subplots(figsize=(8, 6))
        kmf = KaplanMeierFitter()

        # Plot HIGH group
        kmf.fit(T_high, E_high, label="High Expression")
        kmf.plot(ax=ax, ci_show=True,
                 color=cluster_colors.get('high', '#1f77b4') if cluster_colors else None,
                 linewidth=2)

        # Plot LOW group
        kmf.fit(T_low, E_low, label="Low Expression")
        kmf.plot(ax=ax, ci_show=True,
                 color=cluster_colors.get('low', '#ff7f0e') if cluster_colors else None,
                 linewidth=2)

        # Average CI width
        kmf.fit(T_high, E_high)
        ci_df_high = kmf.confidence_interval_
        ci_width_high = (ci_df_high.iloc[:, 1] - ci_df_high.iloc[:, 0]).mean()

        kmf.fit(T_low, E_low)
        ci_df_low = kmf.confidence_interval_
        ci_width_low = (ci_df_low.iloc[:, 1] - ci_df_low.iloc[:, 0]).mean()

        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # === ANNOTATION ===
        # Vertical stack: p-value on top, CI below it, same x, same left align
        text_x = 0.03
        text_y_start = 0.12
        text_y_step = 0.045

        ax.text(
            text_x, text_y_p,
            f"p = {pval:.4g}",
            transform=ax.transAxes, fontsize=16,
            verticalalignment='center'
        )
        ax.text(
            text_x, text_y_ci,
            f"CI High = {ci_width_high:.3f}\nCI Low = {ci_width_low:.3f}",
            transform=ax.transAxes, fontsize=16,
            verticalalignment='top'
        )

        if ax.get_legend():
            ax.get_legend().set_title(f"Cluster {cluster_id}")

        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Survival Probability", fontsize=16)
        ax.set_title(f"Cluster {cluster_id}: High vs Low Expression", fontsize=16)
        ax.tick_params(axis='both', which='major', labelsize=12)

        plt.tight_layout()

        save_path = os.path.join(output_dir, f"km_cluster_{cluster_id}_high_low.png")
        plt.savefig(save_path, dpi=300)
        plt.close()

        print(f"‚úÖ Saved KM plot for cluster {cluster_id} to {save_path}")

    print("‚úÖ‚úÖ Done: separate plots for all clusters.")


def perform_survival_analysis_one_curve_per_cluster_with_ci_high_low(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_cluster=10
):
    print("üîç Starting survival analysis - High expression curves only...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    gene_to_cluster = {gene.upper(): cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlapping patients between survival and expression data.")
        return

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    # Set larger font size for ticks
    ax.tick_params(axis='both', which='major', labelsize=14)

    # Coordinates for annotation in axes fraction [0,1]
    text_y_start = 0.92
    text_y_step = 0.04
    line_length = 0.05  # length of color line in axes fraction
    line_x_start = 0.65  # starting x pos of color line
    pval_x = line_x_start + line_length + 0.01
    ci_x = pval_x + 0.16

    annotation_idx = 0

    for cluster_id in sorted(np.unique(cluster_labels)):
        col = f'cluster_{cluster_id}'
        if col not in merged_df.columns:
            continue

        cluster_expr = merged_df[col]
        threshold = cluster_expr.median()
        high_mask = cluster_expr >= threshold
        low_mask = cluster_expr < threshold

        if high_mask.sum() < min_samples_per_cluster or low_mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue

        T_high, E_high = merged_df.loc[high_mask, 'OS.time'], merged_df.loc[high_mask, 'OS']
        T_low, E_low = merged_df.loc[low_mask, 'OS.time'], merged_df.loc[low_mask, 'OS']

        # Fit KM only for high expression
        kmf.fit(T_high, E_high, label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=False,
                 color=CLUSTER_COLORS.get(cluster_id, None), linewidth=2)

        # CI width
        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Log-rank test: High vs Low
        try:
            result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank test failed for cluster {cluster_id}: {e}")
            pval = np.nan

        # Y position for current annotation (top to bottom)
        y_pos = text_y_start - annotation_idx * text_y_step

        # Draw short horizontal color line in axes fraction coordinates
        ax.hlines(
            y=y_pos,
            xmin=line_x_start,
            xmax=line_x_start + line_length,
            colors=CLUSTER_COLORS.get(cluster_id, '#333333'),
            linewidth=2,
            transform=ax.transAxes,
            clip_on=False
        )

        # Draw p-value and CI side-by-side, horizontally aligned with larger font
        ax.text(
            pval_x,
            y_pos,
            f"p = {pval:.4g}",
            fontsize=16,
            verticalalignment='center',
            transform=ax.transAxes
        )
        ax.text(
            ci_x,
            y_pos,
            f"CI = {ci_width:.3f}",
            fontsize=16,
            verticalalignment='top',
            transform=ax.transAxes
        )

        annotation_idx += 1

    # Remove legend box if any
    if ax.get_legend():
        ax.get_legend().remove()

    ax.set_xlabel("Time (Days)", fontsize=16)
    ax.set_ylabel("Survival Probability", fontsize=16)
    ax.set_title("Survival Curves (High Expression Only)", fontsize=18)

    plt.tight_layout()

    save_path = os.path.join(output_dir, "km_high_expression_per_cluster.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Saved survival plot to {save_path}")


'''
compares high vs. low expression within a single cluster (using median or mean split). But shows Kaplan-Meier curves for multiple discrete clusters (Clus1 to Clus6) ‚Äî not high/low expressions.

Instead of computing expression scores per cluster and splitting into high/low:

Assign each patient to a single cluster (based on highest average expression from that cluster‚Äôs genes),

Then, compare survival across these cluster-assigned groups using log-rank test and Kaplan-Meier curves.
'''

def run_survival_analysis_by_cluster_cluster_assigned_no_CI(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC"
):
    """
    Runs cluster-assigned survival analysis:
    - Assigns each patient to the cluster with the highest cluster-level expression score.
    - Plots Kaplan-Meier curves for all clusters in one plot.
    - Annotates overall multivariate log-rank test p-value.
    """

    os.makedirs(output_dir, exist_ok=True)

    # Load survival + expression
    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)

    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Map cluster -> genes
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # Average score per cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Hard assign patient to cluster with max mean score
    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster_(\d+)')[0].astype(int)
    )
    patient_cluster_scores["cluster_id"] = cluster_assignments.values

    # Merge
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No overlap between survival and expression samples.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    # KM plot for all clusters
    plt.figure(figsize=(8, 6))
    unique_clusters = sorted(groups.unique())
    kmf = KaplanMeierFitter()

    for cl in unique_clusters:
        mask = (groups == cl)
        label = f"Cluster {cl}"  # +1 if you want to show 1-based
        kmf.fit(T[mask], E[mask], label=label)
        kmf.plot_survival_function(ci_show=False, linewidth=2)

    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value


    # Prepare data for Cox model
    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster"]
    cox_df["cluster"] = cox_df["cluster"].astype("category")

    # Fit Cox model with cluster as categorical variable
    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    # Choose the most representative HR to display
    hr_values = cph.summary["exp(coef)"].values
    if len(hr_values) > 0:
        representative_hr = hr_values.max()  # could also use np.mean(hr_values)
        hr_text = f"HR = {representative_hr:.2f}"
    else:
        hr_text = "HR = N/A"

        
    plt.grid(False)

    plt.title(f"{cancer_type}", fontsize=20, fontweight='bold')
    plt.xlabel("Time (Days)", fontsize=16)
    plt.ylabel("Survival Probability", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    # Bold cancer type and p-value
    # plt.text(-1200, 1.05, cancer_type, fontsize=26, fontweight='bold', va='top')
    '''if pval < 0.0001:
        p_text = r"$p < 0.0001$"
    else:
        p_text = f"p = {pval:.4g}"
    plt.text(0.05, 0.05, p_text, transform=plt.gca().transAxes,
             fontsize=16)'''
    # Plot p-value and HR
    if pval < 0.0001:
        # p_text = r"$p < 0.0001$"
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    plt.text(0.05, 0.07, p_text, transform=plt.gca().transAxes, fontsize=16)
    plt.text(0.05, 0.02, hr_text, transform=plt.gca().transAxes, fontsize=16)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_{cancer_type}.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ KM plot saved: {plot_path}")
    return result.summary, merged

def run_survival_analysis_cluster_assign_combo(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC"
):
    os.makedirs(output_dir, exist_ok=True)

    # 1Ô∏è‚É£ Load data
    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # 2Ô∏è‚É£ Map cluster ‚Üí genes
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # 3Ô∏è‚É£ Mean score per cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # 4Ô∏è‚É£ Assign patient to cluster with highest mean
    cluster_assignments = patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster_(\d+)')[0].astype(int)
    patient_cluster_scores["cluster_id"] = cluster_assignments.values

    # 5Ô∏è‚É£ Merge with survival
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patients found.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    # 6Ô∏è‚É£ KM plot
    plt.figure(figsize=(8, 6))
    unique_clusters = sorted(groups.unique())
    kmf = KaplanMeierFitter()

    for cl in unique_clusters:
        mask = (groups == cl)
        label = f"Cluster{cl}"
        kmf.fit(T[mask], E[mask], label=label)
        kmf.plot_survival_function(ci_show=False, linewidth=2)

    # Global logrank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    plt.title(f"Survival Analysis", fontsize=22, fontweight='bold', loc='center')
    plt.xlabel("Time (Days)", fontsize=16)
    plt.ylabel("Survival probability", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.text(-1200, 1.05, cancer_type, fontsize=28, fontweight='bold', va='top')

    if pval < 0.0001:
        # p_text = r"$p < 0.0001$"
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4f}"

    plt.text(0.05, 0.05, p_text, transform=plt.gca().transAxes,
             fontsize=20, fontweight='bold')

    plt.tight_layout()
    km_plot_path = os.path.join(output_dir, f"survival_{cancer_type}.png")
    plt.savefig(km_plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ KM plot saved: {km_plot_path}")

    # 7Ô∏è‚É£ Cox PH with cluster as categorical
    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster_id"]
    cox_df["cluster_id"] = cox_df["cluster_id"].astype("category")

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary_cluster_assign.csv"))
    print(f"üìÑ Cox summary saved: {os.path.join(output_dir, 'cox_summary_cluster_assign.csv')}")

    # 8Ô∏è‚É£ Forest plot of cluster HRs
    summary = cph.summary.copy()
    summary['HR'] = np.exp(summary['coef'])
    summary['HR_lower'] = np.exp(summary['coef lower 95%'])
    summary['HR_upper'] = np.exp(summary['coef upper 95%'])

    fig, ax = plt.subplots(figsize=(8, len(summary) * 0.5 + 2))

    y = np.arange(len(summary))
    ax.errorbar(summary['HR'], y, xerr=[summary['HR'] - summary['HR_lower'],
                                        summary['HR_upper'] - summary['HR']],
                fmt='o', color='blue', ecolor='black', capsize=4)

    ax.axvline(1, color='grey', linestyle='--')
    ax.set_yticks(y)
    ax.set_yticklabels(summary.index)
    ax.set_xlabel('Hazard Ratio (HR)')
    ax.set_title(f"Cox PH (Cluster Assigned)")

    for i, (hr, p) in enumerate(zip(summary['HR'], summary['p'])):
        ax.text(hr + 0.05, i, f'p={p:.4g}', va='center', fontsize=10,
                fontweight='bold' if p < 0.05 else 'normal')

    plt.tight_layout()
    forest_path = os.path.join(output_dir, "cox_forestplot_cluster_assign.png")
    plt.savefig(forest_path, dpi=300)
    plt.close()

    print(f"‚úÖ Forest plot saved: {forest_path}")

    return result.summary, cph.summary, merged

def run_survival_analysis_cox(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
):
    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs.")
        return

    # ‚ûú Compute global mean or sum across clusters
    cluster_cols = [col for col in patient_cluster_scores.columns if col in merged.columns]
    merged["global_cluster_score"] = merged[cluster_cols].mean(axis=1)

    # ‚ûú Split into High / Low based on median
    T = merged["OS.time"]
    E = merged["OS"]
    threshold = merged["global_cluster_score"].median() if cluster_threshold == "median" else merged["global_cluster_score"].mean()
    merged["global_group"] = (merged["global_cluster_score"] >= threshold).astype(int)

    kmf = KaplanMeierFitter()
    fig, ax = plt.subplots(figsize=(8, 6))

    for group in [0, 1]:
        label = f"{'Low' if group == 0 else 'High'}"
        kmf.fit(T[merged["global_group"] == group], E[merged["global_group"] == group], label=label)
        kmf.plot_survival_function(ci_show=True, ax=ax, linewidth=2)

    ax.legend(fontsize=16, loc='upper right', frameon=False)

    # ‚ûú One logrank p-value for global split
    result = logrank_test(
        T[merged["global_group"] == 1],
        T[merged["global_group"] == 0],
        event_observed_A=E[merged["global_group"] == 1],
        event_observed_B=E[merged["global_group"] == 0]
    )
    pval = result.p_value

    ci_df = kmf.confidence_interval_
    ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

    ax.set_title(f"Survival by Global Cluster Expression", fontsize=20)
    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.tick_params(axis='both', labelsize=16)

    text_x = 0.03
    text_y_start = 0.12
    text_y_step = 0.045

    ax.text(
        text_x, text_y_start,
        f"CI = {ci_width:.3f}",
        transform=ax.transAxes,
        fontsize=16
    )
    ax.text(
        text_x, text_y_start - 0.05,
        f"p = {pval:.4g}",
        transform=ax.transAxes,
        fontsize=16
    )

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"km_global_clusters.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ Global KM plot saved: {plot_path}")

    # ‚ûú Cox PH on all clusters separately
    cox_df = merged[["OS.time", "OS"] + cluster_cols].copy()
    cox_df.columns = ["time", "event"] + cluster_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def plot_cox_forest(cox_summary, title="Cox PH Forest Plot"):
    df = cox_summary.copy()
    df = df.reset_index().rename(columns={'index': 'covariate'})
    df = df[['covariate', 'exp(coef)', 'coef lower 95%', 'coef upper 95%', 'p']]
    df = df.sort_values('exp(coef)', ascending=False)

    fig, ax = plt.subplots(figsize=(8, len(df) * 0.6))

    y_pos = range(len(df))
    ax.errorbar(
        df['exp(coef)'], y_pos,
        xerr=[
            df['exp(coef)'] - df['coef lower 95%'],
            df['coef upper 95%'] - df['exp(coef)']
        ],
        fmt='o', color='darkblue', ecolor='gray', capsize=3
    )

    ax.axvline(1, color='red', linestyle='--')

    ax.set_yticks(y_pos)
    ax.set_yticklabels(df['covariate'], fontsize=10)
    ax.set_xlabel("Hazard Ratio (HR)", fontsize=12)
    ax.set_title(title, fontsize=16)
    plt.tight_layout()
    plt.close()

def plot_cox_forest_with_pvalues_styled(
    cox_summary,
    output_path="cox_forest_plot_styled.png",
    title="Cox PH Forest Plot"
):
    sns.set_style("whitegrid")
    df = cox_summary.copy().reset_index().rename(columns={'index': 'covariate'})
    df = df[['covariate', 'exp(coef)', 'coef lower 95%', 'coef upper 95%', 'p']]
    df = df.sort_values('exp(coef)', ascending=False).reset_index(drop=True)

    fig, ax = plt.subplots(figsize=(10, len(df) * 0.8 + 2))

    for i, row in df.iterrows():
        hr = row['exp(coef)']
        lower = row['coef lower 95%']
        upper = row['coef upper 95%']
        pval = row['p']

        lower_error = max(hr - lower, 0)
        upper_error = max(upper - hr, 0)

        color = 'red' if hr > 1 else 'blue'

        ax.errorbar(
            hr, i,
            xerr=[[lower_error], [upper_error]],
            fmt='o',
            color='black',
            ecolor=color,
            elinewidth=2,
            capsize=6
        )

        # Add p-value above the error bar
        text_props = dict(
            va='bottom',
            fontsize=16,
            color='red' if pval < 0.05 else 'black',
            weight='bold' if pval < 0.05 else 'normal'
        )
        ax.text(
            hr, i + 0.25,
            f"p = {pval:.3g}",
            ha='center',
            **text_props
        )

    # Red vertical line at HR = 1
    ax.axvline(1, color='red', linestyle='--', linewidth=1.5)

    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['covariate'], fontsize=16)
    ax.set_xlabel("Hazard Ratio (HR)", fontsize=16)
    ax.set_title(title, fontsize=18)

    ax.tick_params(axis='x', labelsize=14)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

    print(f"‚úÖ Saved styled forest plot with large font & p-values: {output_path}")

def plot_cox_forest_with_pvalues_styled_gray(
    cox_summary,
    output_path="cox_forest_plot_styled.png",
    title="Cox PH Forest Plot"
):
    sns.set_style("whitegrid")
    df = cox_summary.copy().reset_index().rename(columns={'index': 'covariate'})
    df = df[['covariate', 'exp(coef)', 'coef lower 95%', 'coef upper 95%', 'p']]
    df = df.sort_values('exp(coef)', ascending=False).reset_index(drop=True)

    fig, ax = plt.subplots(figsize=(8, len(df) * 0.6 + 2))

    for i, row in df.iterrows():
        hr = row['exp(coef)']
        lower = row['coef lower 95%']
        upper = row['coef upper 95%']
        pval = row['p']

        lower_error = max(hr - lower, 0)
        upper_error = max(upper - hr, 0)

        color = 'red' if hr > 1 else 'blue'

        ax.errorbar(
            hr, i,
            xerr=[[lower_error], [upper_error]],
            fmt='o',
            color='black',
            ecolor=color,
            elinewidth=2,
            capsize=4
        )

        # Add styled p-value
        text_props = dict(
            va='center',
            fontsize=10,
            color='red' if pval < 0.05 else 'gray',
            weight='bold' if pval < 0.05 else 'normal'
        )
        ax.text(
            hr + 0.1, i,
            f"p = {pval:.3g}",
            **text_props
        )

    ax.axvline(1, color='red', linestyle='--')

    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['covariate'], fontsize=10)
    ax.set_xlabel("Hazard Ratio (HR)", fontsize=12)
    ax.set_title(title, fontsize=16)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

    print(f"‚úÖ Saved styled forest plot with significance to {output_path}")

def run_survival_analysis_by_cluster_cluster_assigned(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC"
):
    """
    Cluster-assigned survival analysis:
    - Each patient assigned to cluster with highest mean expression score.
    - KM curves for all clusters.
    - Shows multivariate log-rank p-value, HR, CI.
    """

    os.makedirs(output_dir, exist_ok=True)

    # Load data
    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)

    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Map cluster -> genes
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # Patient-wise mean per cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Hard assign: max cluster
    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    patient_cluster_scores["cluster_id"] = cluster_assignments.values

    # Merge with survival
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No overlap between survival and expression samples.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    # KM plot
    plt.figure(figsize=(8, 6))
    kmf = KaplanMeierFitter()
    unique_clusters = sorted(groups.unique())

    for cl in unique_clusters:
        mask = (groups == cl)
        label = f"Cluster {cl}"
        kmf.fit(T[mask], E[mask], label=label)
        kmf.plot_survival_function(ci_show=False, linewidth=2)

    # Multivariate log-rank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox model (cluster as categorical)
    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster"]
    cox_df["cluster"] = cox_df["cluster"].astype("category")

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    # Representative HR: max or mean
    hr_values = cph.summary["exp(coef)"].values
    representative_hr = np.mean(hr_values) if len(hr_values) > 0 else np.nan

    # Approx CI: mean width
    ci_widths = cph.summary["coef upper 95%"] - cph.summary["coef lower 95%"]
    representative_ci = ci_widths.mean() if len(ci_widths) > 0 else np.nan

    # Plot styling
    plt.grid(False)
    plt.title(f"{cancer_type}", fontsize=20, fontweight='bold')
    plt.xlabel("Time (Days)", fontsize=16)
    plt.ylabel("Survival Probability", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    # Text block: CI ‚Üí HR ‚Üí p-value
    text_x = 0.03
    text_y_start = 0.10
    text_y_step = 0.045

    plt.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=plt.gca().transAxes,
        fontsize=16
    )
    plt.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=plt.gca().transAxes,
        fontsize=16
    )
    if pval < 0.0001:
        # p_text = r"$p < 0.0001$"
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"
        
    plt.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=plt.gca().transAxes,
        fontsize=16
    )

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_{cancer_type}.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ KM plot saved: {plot_path}")
    return result.summary, merged

def perform_survival_analysis_cluster_assigned(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    cancer_type="KIRC",
    min_samples_per_cluster=10
):
    print("üîç Running cluster-assigned survival analysis...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # Map genes to clusters
    gene_to_cluster = {gene: cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Average per cluster
    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster {cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    # Assign each patient to max-score cluster
    assigned_clusters = (
        cluster_expr_df.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    cluster_expr_df['assigned_cluster'] = assigned_clusters

    # Merge
    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlap between survival and expression.")
        return

    T = merged_df['OS.time']
    E = merged_df['OS']
    groups = merged_df['assigned_cluster']

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    ax.tick_params(axis='both', which='major', labelsize=14)

    # KM per assigned cluster
    unique_clusters = sorted(groups.unique())
    for cluster_id in unique_clusters:
        mask = (groups == cluster_id)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue
        kmf.fit(T[mask], E[mask], label=f"Cluster {cluster_id}")
        kmf.plot(ax=ax, ci_show=True, linewidth=2)

    # Overall log-rank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox PH (assigned cluster as categorical)
    cox_df = merged_df[['OS.time', 'OS', 'assigned_cluster']].copy()
    cox_df.columns = ['time', 'event', 'cluster']
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col='time', event_col='event')

    hr_values = cph.summary['exp(coef)'].values
    representative_hr = np.mean(hr_values) if len(hr_values) > 0 else np.nan
    ci_widths = cph.summary['coef upper 95%'] - cph.summary['coef lower 95%']
    representative_ci = ci_widths.mean() if len(ci_widths) > 0 else np.nan

    # Plot annotations: CI ‚Üí HR ‚Üí p-value
    text_x = 0.03
    text_y_start = 0.10
    text_y_step = 0.045

    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.set_title(f"{cancer_type} Survival (Cluster-Assigned)", fontsize=20)
    plt.grid(False)

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes, fontsize=16
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes, fontsize=16
    )
    
    # Decide the text first
    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    # Then use it in ax.text
    ax.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=ax.transAxes,
        fontsize=16
    )


    if ax.get_legend():
        ax.get_legend().remove()

    plt.tight_layout()
    save_path = os.path.join(output_dir, "km_cluster_assigned.png")
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"‚úÖ Cluster-assigned survival plot saved: {save_path}")

    return cph.summary, merged_df

def plot_cox_forest_with_pvalues_styled(
    cox_summary,
    output_path="cox_forest_plot_styled.png",
    title="Cox PH Forest Plot"
):
    # Remove default grid
    sns.set_style("white")  # <- no grid

    df = cox_summary.copy().reset_index().rename(columns={'index': 'covariate'})
    df = df[['covariate', 'exp(coef)', 'coef lower 95%', 'coef upper 95%', 'p']]
    df = df.sort_values('exp(coef)', ascending=False).reset_index(drop=True)

    fig, ax = plt.subplots(figsize=(10, len(df) * 0.8 + 2))

    for i, row in df.iterrows():
        hr = row['exp(coef)']
        lower = row['coef lower 95%']
        upper = row['coef upper 95%']
        pval = row['p']

        lower_error = max(hr - lower, 0)
        upper_error = max(upper - hr, 0)

        color = 'red' if hr > 1 else 'blue'

        ax.errorbar(
            hr, i,
            xerr=[[lower_error], [upper_error]],
            fmt='o',
            color='black',
            ecolor=color,
            elinewidth=2.5,
            capsize=6
        )

        # P-value above error bar, larger font, always black
        ax.text(
            hr, i + 0.3,
            f"p = {pval:.3g}",
            ha='center',
            va='bottom',
            fontsize=16,
            color='black',
            weight='bold'
        )

    # HR = 1 reference line
    ax.axvline(1, color='red', linestyle='--', linewidth=1.5)

    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['covariate'], fontsize=16)
    ax.set_xlabel("Hazard Ratio (HR)", fontsize=18)
    ax.set_title(title, fontsize=20)

    ax.tick_params(axis='x', labelsize=16)

    # Remove grid
    ax.grid(False)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

    print(f"‚úÖ Saved styled forest plot with large font & p-values: {output_path}")

def plot_cox_forest_with_pvalues_styled(
    cox_summary,
    output_path="cox_forest_plot_styled.png",
    title="Cox PH Forest Plot"
):
    sns.set_style("white")  # No grid

    df = cox_summary.copy().reset_index().rename(columns={'index': 'covariate'})
    df = df[['covariate', 'exp(coef)', 'coef lower 95%', 'coef upper 95%', 'p']]
    df = df.sort_values('exp(coef)', ascending=False).reset_index(drop=True)

    fig, ax = plt.subplots(figsize=(10, len(df) * 0.8 + 2))

    for i, row in df.iterrows():
        hr = row['exp(coef)']
        lower = row['coef lower 95%']
        upper = row['coef upper 95%']
        pval = row['p']

        lower_error = max(hr - lower, 0)
        upper_error = max(upper - hr, 0)

        color = 'red' if hr > 1 else 'blue'

        ax.errorbar(
            hr, i,
            xerr=[[lower_error], [upper_error]],
            fmt='o',
            color='black',
            ecolor=color,
            elinewidth=2.5,
            capsize=6
        )

        # P-value closer to dot
        ax.text(
            hr, i + 0.12,   # tighter offset
            f"p = {pval:.3g}",
            ha='center',
            va='bottom',
            fontsize=15,
            color='black',
            weight='bold'
        )

    ax.axvline(1, color='red', linestyle='--', linewidth=1.5)

    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['covariate'], fontsize=16)
    ax.set_xlabel("Hazard Ratio (HR)", fontsize=18)
    ax.set_title(title, fontsize=20)

    ax.tick_params(axis='x', labelsize=16)
    ax.grid(False)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

    print(f"‚úÖ Saved styled forest plot with tight p-values: {output_path}")

def perform_survival_analysis_cluster_assigned_(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    cancer_type="KIRC",
    min_samples_per_cluster=5
):
    os.makedirs(output_dir, exist_ok=True)

    # 1Ô∏è‚É£ Load survival
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # 2Ô∏è‚É£ Load expression
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # 3Ô∏è‚É£ Map genes to clusters
    cluster_to_genes = defaultdict(list)
    for gene, cl in zip(top_gene_names, cluster_labels):
        cluster_to_genes[cl].append(gene.upper())

    # 4Ô∏è‚É£ Calculate cluster mean per patient
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # 5Ô∏è‚É£ Assign each patient to cluster with highest mean
    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    patient_cluster_scores['cluster_id'] = cluster_assignments

    # 6Ô∏è‚É£ Merge
    merged_df = survival_df.merge(patient_cluster_scores, left_on='_PATIENT', right_index=True)
    if merged_df.empty:
        print("‚ùå No overlap between survival & expression data.")
        return

    # 7Ô∏è‚É£ KM plot
    T = merged_df['OS.time']
    E = merged_df['OS']
    groups = merged_df['cluster_id']

    plt.figure(figsize=(10, 7))
    kmf = KaplanMeierFitter()

    for cl in sorted(groups.unique()):
        mask = (groups == cl)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cl} skipped, too few samples.")
            continue
        kmf.fit(T[mask], E[mask], label=f"Cluster {cl}")
        kmf.plot(ci_show=True, linewidth=2)

    # Log-rank test
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # 8Ô∏è‚É£ Cox PH
    cox_df = merged_df[['OS.time', 'OS', 'cluster_id']].dropna().copy()
    cox_df.columns = ['time', 'event', 'cluster']
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    # Drop rare levels
    valid_clusters = cox_df['cluster'].value_counts()
    valid_clusters = valid_clusters[valid_clusters >= min_samples_per_cluster].index
    cox_df = cox_df[cox_df['cluster'].isin(valid_clusters)]

    # Fit Cox
    if len(cox_df['cluster'].cat.categories) > 1:
        cph = CoxPHFitter()
        cph.fit(cox_df, duration_col='time', event_col='event')
        hr_text = f"HR = {cph.summary['exp(coef)'].max():.2f}"
        cph.summary.to_csv(os.path.join(output_dir, 'cox_summary.csv'))
    else:
        print("‚ùå Not enough cluster levels for Cox.")
        hr_text = "HR = N/A"

    # 9Ô∏è‚É£ Annotate plot
    plt.title(f"{cancer_type} Survival by Cluster", fontsize=20, fontweight='bold')
    plt.xlabel("Time (Days)", fontsize=16)
    plt.ylabel("Survival Probability", fontsize=16)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.grid(False)

    plt.text(0.05, 0.07, f"p = {pval:.4g}", transform=plt.gca().transAxes, fontsize=16)
    plt.text(0.05, 0.02, hr_text, transform=plt.gca().transAxes, fontsize=16)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_{cancer_type}_cluster_assigned.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ KM plot saved to {plot_path}")

    return result.summary, merged_df

def run_survival_analysis_by_cluster_cluster_assigned(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC",
    min_samples_per_cluster=5
):
    """
    Cluster-assigned survival:
    - Each patient is assigned to the cluster with highest mean expression.
    - One KM curve per cluster (no high/low).
    - Multivariate log-rank, Cox PH, CI, HR shown.
    """

    os.makedirs(output_dir, exist_ok=True)

    # üì• Load
    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)

    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # üß¨ Map clusters
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # üßÆ Mean scores per patient per cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Assign patient to cluster with max mean
    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    patient_cluster_scores["cluster_id"] = cluster_assignments.values

    # Merge with survival
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No overlap between survival and expression data.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    plt.figure(figsize=(10, 7))
    kmf = KaplanMeierFitter()
    unique_clusters = sorted(groups.unique())

    # üìä KM: one curve per assigned cluster
    for cl in unique_clusters:
        mask = (groups == cl)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cl} skipped: too few patients.")
            continue
        kmf.fit(T[mask], E[mask], label=f"Cluster {cl}")
        kmf.plot_survival_function(ci_show=True, linewidth=2)

    # üìå Multivariate log-rank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # üìå Cox: cluster categorical
    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster"]
    cox_df["cluster"] = cox_df["cluster"].astype("category")

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    hr_values = cph.summary["exp(coef)"].values
    representative_hr = np.mean(hr_values) if len(hr_values) else np.nan

    ci_widths = cph.summary["coef upper 95%"] - cph.summary["coef lower 95%"]
    representative_ci = ci_widths.mean() if len(ci_widths) else np.nan

    # üñºÔ∏è Style
    plt.grid(False)
    plt.title(f"{cancer_type} Survival by Cluster", fontsize=20)
    plt.xlabel("Time (Days)", fontsize=16)
    plt.ylabel("Survival Probability", fontsize=16)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)

    text_x = 0.03
    text_y_start = 0.12
    text_y_step = 0.045

    plt.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=plt.gca().transAxes,
        fontsize=16
    )
    plt.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=plt.gca().transAxes,
        fontsize=16
    )
    # Decide p_text first
    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    # Now call plt.text with p_text
    plt.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=plt.gca().transAxes,
        fontsize=16
    )


    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_{cancer_type}_assigned.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ KM plot saved: {plot_path}")

    # Save Cox table
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))

    return result.summary, merged

def perform_survival_analysis_cluster_assigned(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    cancer_type="KIRC",
    min_samples_per_cluster=10
):
    print("üîç Running cluster-assigned survival analysis...")

    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # Map genes to clusters
    gene_to_cluster = {gene: cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Average per cluster
    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster {cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    # Assign each patient to max-score cluster
    assigned_clusters = (
        cluster_expr_df.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    cluster_expr_df['assigned_cluster'] = assigned_clusters

    # Merge
    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlap between survival and expression.")
        return

    T = merged_df['OS.time']
    E = merged_df['OS']
    groups = merged_df['assigned_cluster']

    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    kmf = KaplanMeierFitter()

    ax.tick_params(axis='both', which='major', labelsize=14)

    handles = []
    labels = []

    # KM per assigned cluster
    unique_clusters = sorted(groups.unique())
    for cluster_id in unique_clusters:
        mask = (groups == cluster_id)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue
        kmf.fit(T[mask], E[mask], label=f"Cluster {cluster_id}")
        line = kmf.plot(ax=ax, ci_show=True, show_censors=True, linewidth=2)
        handle = line.get_lines()[-1]
        handles.append(handle)
        labels.append(f"Cluster {cluster_id}")

    # Overall log-rank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox PH (assigned cluster as categorical)
    cox_df = merged_df[['OS.time', 'OS', 'assigned_cluster']].copy()
    cox_df.columns = ['time', 'event', 'cluster']
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col='time', event_col='event')

    hr_values = cph.summary['exp(coef)'].values
    representative_hr = np.mean(hr_values) if len(hr_values) > 0 else np.nan
    ci_widths = cph.summary['coef upper 95%'] - cph.summary['coef lower 95%']
    representative_ci = ci_widths.mean() if len(ci_widths) > 0 else np.nan

    # Plot annotations: CI ‚Üí HR ‚Üí p-value
    text_x = 0.03
    text_y_start = 0.10
    text_y_step = 0.045

    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.set_title(f"{cancer_type} Survival (Cluster-Assigned)", fontsize=20)
    plt.grid(False)

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes, fontsize=16
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes, fontsize=16
    )
    # Decide the text first
    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    # Then use it in ax.text
    ax.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=ax.transAxes,
        fontsize=16
    )


    # ‚úÖ Add proper legend
    ax.legend(handles=handles, labels=labels, fontsize=16, loc='upper right', frameon=False)

    plt.tight_layout()
    save_path = os.path.join(output_dir, "km_cluster_assigned.png")
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"‚úÖ Cluster-assigned survival plot saved: {save_path}")

    return cph.summary, merged_df

def plot_cox_forest_with_pvalues_styled_(
    cox_summary,
    output_path="cox_forest_plot_styled.png",
    title="Cox PH Forest Plot",
    cancer_type="KIRC"
):
    # Use white background without grid
    sns.set_style("white")

    df = cox_summary.copy().reset_index().rename(columns={'index': 'covariate'})
    df = df[['covariate', 'exp(coef)', 'coef lower 95%', 'coef upper 95%', 'p']]
    df = df.sort_values('exp(coef)', ascending=False).reset_index(drop=True)

    fig, ax = plt.subplots(figsize=(12, len(df) * 0.8 + 2))
    # ax.spines['top'].set_visible(False)
    # ax.spines['right'].set_visible(False)
    for spine in ['top', 'right', 'bottom', 'left']:
        ax.spines[spine].set_visible(False)

    for i, row in df.iterrows():
        hr = row['exp(coef)']
        lower = row['coef lower 95%']
        upper = row['coef upper 95%']
        pval = row['p']

        lower_error = max(hr - lower, 0)
        upper_error = max(upper - hr, 0)

        color = 'red' if hr > 1 else 'blue'

        ax.errorbar(
            hr, i,
            xerr=[[lower_error], [upper_error]],
            fmt='o',
            color='black',
            ecolor=color,
            elinewidth=3,
            capsize=7
        )

        # P-value closer to dot
        p_text = f"p = {pval:.1e}" if pval < 0.0001 else f"p = {pval:.3g}"

        ax.text(
            hr, i + 0.15,  # tight offset
            p_text,
            ha='center',
            va='bottom',
            fontsize=18,
            color='black',
            # weight='bold'
        )


    # Reference line at HR = 1
    ax.axvline(1, color='red', linestyle='--', linewidth=2)

    # Y labels
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['covariate'], fontsize=18)

    # Axis labels
    ax.set_xlabel("Hazard Ratio (HR)", fontsize=20)
    ax.set_title(title, fontsize=22)

    ax.tick_params(axis='x', labelsize=18)
    ax.grid(False)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

    print(f"‚úÖ Saved styled forest plot with large fonts: {output_path}")

def perform_survival_analysis_cluster_assigned(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    cancer_type="KIRC",
    min_samples_per_cluster=10
):
    print("üîç Running cluster-assigned survival analysis...")

    os.makedirs(output_dir, exist_ok=True)

    # üì• Load and clean
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # üóÇÔ∏è Map genes to clusters
    gene_to_cluster = {gene: cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # üßÆ Average per cluster
    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster {cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    # Assign each patient to max-score cluster
    assigned_clusters = (
        cluster_expr_df.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    cluster_expr_df['assigned_cluster'] = assigned_clusters

    # üóÇÔ∏è Merge
    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlap between survival and expression.")
        return

    T = merged_df['OS.time']
    E = merged_df['OS']
    groups = merged_df['assigned_cluster']

    fig, ax = plt.subplots(figsize=(12, 8))
    kmf = KaplanMeierFitter()

    ax.tick_params(axis='both', which='major', labelsize=16)

    handles = []
    labels = []

    # üìà KM per cluster
    unique_clusters = sorted(groups.unique())
    for cluster_id in unique_clusters:
        mask = (groups == cluster_id)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples.")
            continue
        kmf.fit(T[mask], E[mask], label=f"Cluster {cluster_id}")
        line = kmf.plot(ax=ax, ci_show=True, show_censors=True, linewidth=2)
        handle = line.get_lines()[-1]
        handles.append(handle)
        labels.append(f"Cluster {cluster_id}")

    # üìå Overall log-rank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # üìå Cox PH
    cox_df = merged_df[['OS.time', 'OS', 'assigned_cluster']].copy()
    cox_df.columns = ['time', 'event', 'cluster']
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col='time', event_col='event')

    hr_values = cph.summary['exp(coef)'].values
    representative_hr = np.mean(hr_values) if len(hr_values) > 0 else np.nan
    ci_widths = cph.summary['coef upper 95%'] - cph.summary['coef lower 95%']
    representative_ci = ci_widths.mean() if len(ci_widths) > 0 else np.nan

    # üìä Annotations
    text_x = 0.03
    text_y_start = 0.12
    text_y_step = 0.045

    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.set_title(f"{cancer_type} Survival (Cluster-Assigned)", fontsize=20)
    ax.grid(False)

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes, fontsize=18
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes, fontsize=18
    )

    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    ax.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=ax.transAxes, fontsize=18
    )

    # ‚úÖ Legend
    ax.legend(handles=handles, labels=labels, fontsize=16, loc='upper right', frameon=False)

    plt.tight_layout()
    save_path = os.path.join(output_dir, "km_cluster_assigned.png")
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"‚úÖ Cluster-assigned survival plot saved: {save_path}")

    return cph.summary, merged_df

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    clusters_to_plot=[0, 1, 2, 3]
):
    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    for cl in clusters_to_plot:
        col = f"cluster {cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(10, 7))

        handles = []
        labels = []

        for group in [0, 1]:
            label = f"{'Low' if group == 0 else 'High'}"
            kmf.fit(T[merged[group_col] == group], E[merged[group_col] == group], label=label)
            line = kmf.plot_survival_function(ci_show=True, show_censors=True, ax=ax, linewidth=2)
            handle = line.get_lines()[-1]
            handles.append(handle)
            labels.append(label)

        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"Log-rank test failed: {e}")
            pval = float('nan')

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Fit Cox model for this cluster only
        cox_df = merged[["OS.time", "OS", col]].copy()
        cox_df.columns = ["time", "event", "cluster_score"]

        cph_single = CoxPHFitter()
        cph_single.fit(cox_df, duration_col="time", event_col="event")
        hr = cph_single.hazard_ratios_["cluster_score"]

        # üî† Annotations
        # text_x = 0.65
        # text_y_start = 0.85
        # text_y_step = 0.05
        text_x = 0.03
        text_y_start = 0.12
        text_y_step = 0.045
    
        ax.set_title(f"Survival by {col} Expression", fontsize=20)
        ax.set_xlabel("Time (Days)", fontsize=18)
        ax.set_ylabel("Survival Probability", fontsize=18)
        ax.tick_params(axis='both', labelsize=16)

        ax.text(
            text_x, text_y_start,
            f"CI = {ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=18
        )

        ax.text(
            text_x, text_y_start - text_y_step,
            f"HR = {hr:.3f}",
            transform=ax.transAxes,
            fontsize=18
        )

        if pval < 0.0001:
            p_text = f"p = {pval:.1e}"
        else:
            p_text = f"p = {pval:.4g}"

        ax.text(
            text_x, text_y_start - 2 * text_y_step,
            p_text,
            transform=ax.transAxes,
            fontsize=18
        )

        ax.legend(handles=handles, labels=labels, fontsize=16, loc='upper right', frameon=False)

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved: {plot_path}")

    # ‚úÖ Multivariate Cox for all clusters
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis_by_cluster_cluster_assigned(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC",
    min_samples_per_cluster=5
):
    """
    Cluster-assigned survival:
    - Each patient assigned to cluster with highest mean expression.
    - One KM curve per cluster (no high/low).
    - Multivariate log-rank, Cox PH, CI, HR shown.
    """
    os.makedirs(output_dir, exist_ok=True)

    # Load data
    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)

    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Map clusters to genes
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # Compute mean expression per patient per cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Assign each patient to cluster with max mean score
    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    patient_cluster_scores["cluster_id"] = cluster_assignments.values

    # Merge with survival data
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No overlap between survival and expression data.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    plt.figure(figsize=(10, 7))
    kmf = KaplanMeierFitter()
    unique_clusters = sorted(groups.unique())

    # Plot KM curves, one per cluster
    for cl in unique_clusters:
        mask = (groups == cl)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cl} skipped: too few patients.")
            continue
        kmf.fit(T[mask], E[mask], label=f"Cluster {cl}")
        kmf.plot_survival_function(ci_show=True, show_censors=True, linewidth=2)

    # Multivariate log-rank test
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox proportional hazards model
    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster"]
    cox_df["cluster"] = cox_df["cluster"].astype("category")

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    hr_values = cph.summary["exp(coef)"].values
    representative_hr = np.mean(hr_values) if len(hr_values) else np.nan

    ci_widths = cph.summary["coef upper 95%"] - cph.summary["coef lower 95%"]
    representative_ci = ci_widths.mean() if len(ci_widths) else np.nan

    # Styling the plot with larger fonts and bold labels
    plt.grid(False)
    plt.title(f"{cancer_type} Survival by Cluster", fontsize=22)
    plt.xlabel("Time (Days)", fontsize=18)
    plt.ylabel("Survival Probability", fontsize=18)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)

    # text_x = 0.03
    # text_y_start = 0.12
    # text_y_step = 0.04
    text_x = 0.03
    text_y_start = 0.12
    text_y_step = 0.045
    
    plt.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=plt.gca().transAxes,
        fontsize=18,
        # weight='bold'
    )
    plt.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=plt.gca().transAxes,
        fontsize=18,
        # weight='bold'
    )
    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    plt.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=plt.gca().transAxes,
        fontsize=18,
        # weight='bold'
    )

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_{cancer_type}_assigned.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ KM plot saved: {plot_path}")

    # Save Cox summary table
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))

    return result.summary, merged

def run_survival_analysis_cox(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
):
    os.makedirs(output_dir, exist_ok=True)

    # Step 1: Load survival and expression data
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Step 2: Cluster gene sets ‚Üí cluster scores per patient
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Step 3: Merge with survival and assign global score
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs.")
        return

    cluster_cols = [col for col in patient_cluster_scores.columns if col in merged.columns]
    merged["global_cluster_score"] = merged[cluster_cols].mean(axis=1)

    T = merged["OS.time"]
    E = merged["OS"]
    threshold = merged["global_cluster_score"].median() if cluster_threshold == "median" else merged["global_cluster_score"].mean()
    merged["global_group"] = (merged["global_cluster_score"] >= threshold).astype(int)

    # Step 4: Kaplan-Meier plot with at-risk table
    kmf_high = KaplanMeierFitter()
    kmf_low = KaplanMeierFitter()

    fig, ax = plt.subplots(figsize=(7, 6))

    mask_high = merged["global_group"] == 1
    mask_low = merged["global_group"] == 0

    T_high, E_high = T[mask_high], E[mask_high]
    T_low, E_low = T[mask_low], E[mask_low]

    kmf_high.fit(T_high, E_high, label="High")
    kmf_low.fit(T_low, E_low, label="Low")

    kmf_high.plot_survival_function(ax=ax, color="#d62728", linewidth=2)  # red
    kmf_low.plot_survival_function(ax=ax, color="#17becf", linewidth=2)   # blue

    add_at_risk_counts(kmf_high, kmf_low, ax=ax)

    # Step 5: Logrank p-value and sample sizes
    result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
    pval = result.p_value

    sample_sizes = {"High": mask_high.sum(), "Low": mask_low.sum()}

    ax.set_title("Survival by Global Cluster Expression", fontsize=16)
    ax.set_xlabel("Time (Days)", fontsize=14)
    ax.set_ylabel("Overall Survival Probability", fontsize=14)
    ax.tick_params(axis='both', labelsize=12)
    ax.grid(False)
    ax.legend(fontsize=12, loc='best', frameon=False)

    # Annotate p-value and N
    ax.text(0.05, 0.15, f"N = {sample_sizes['High'] + sample_sizes['Low']}", transform=ax.transAxes, fontsize=13)
    ax.text(0.05, 0.08, f"p = {pval:.4g}", transform=ax.transAxes, fontsize=13)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, "km_global_clusters.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ KM plot saved: {plot_path}")

    # Step 6: Cox proportional hazards model
    cox_df = merged[["OS.time", "OS"] + cluster_cols].copy()
    cox_df.columns = ["time", "event"] + cluster_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    clusters_to_plot=[0, 1, 2, 3]
):
    os.makedirs(output_dir, exist_ok=True)

    # Step 1: Load survival and expression data
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Step 2: Assign genes to clusters
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Step 3: Merge and compute groupings
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    for cl in clusters_to_plot:
        col = f"cluster {cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        # Step 4: Kaplan-Meier fitting
        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(8, 6))

        line_styles = ['-','-']
        colors = ['#17becf', '#d62728']
        markers = ['|', '|']
        labels = []
        sample_sizes = {}

        for idx, group in enumerate([0, 1]):
            label = f"{'Low' if group == 0 else 'High'}"
            mask = merged[group_col] == group
            kmf.fit(T[mask], E[mask], label=label)
            kmf.plot_survival_function(
                ax=ax,
                ci_show=True,
                show_censors=True,
                color=colors[idx],
                linewidth=2,
                linestyle='-', 
                marker=markers[idx],    
                censor_styles={'marker': '|', 'ms': 8, 'mew': 1}  # ‚ûú Short vertical lines = censoring marks
            )
            labels.append(label)
            sample_sizes[label] = mask.sum()

        # Step 5: Statistical test and annotations
        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"Log-rank test failed: {e}")
            pval = float('nan')

        ci_df = kmf.confidence_interval_
        ci_width = (ci_df.iloc[:, 1] - ci_df.iloc[:, 0]).mean()

        # Cox model (single variable)
        cox_df = merged[["OS.time", "OS", col]].copy()
        cox_df.columns = ["time", "event", "cluster_score"]

        cph_single = CoxPHFitter()
        cph_single.fit(cox_df, duration_col="time", event_col="event")
        hr = cph_single.hazard_ratios_["cluster_score"]

        # Plot decoration
        # text_x = 0.65
        # text_y_start = 0.85
        # text_y_step = 0.05
        text_x = 0.03
        text_y_start = 0.12
        text_y_step = 0.045
        
        ax.set_title(f"Survival by {col} Expression", fontsize=20)
        ax.set_xlabel("Time (Days)", fontsize=18)
        ax.set_ylabel("Survival Probability", fontsize=18)
        ax.tick_params(axis='both', labelsize=16)

        ax.text(
            text_x, text_y_start,
            f"CI = {ci_width:.3f}",
            transform=ax.transAxes,
            fontsize=16
        )

        ax.text(
            text_x, text_y_start - text_y_step,
            f"HR = {hr:.3f}",
            transform=ax.transAxes,
            fontsize=16
        )

        if pval < 0.0001:
            p_text = f"p = {pval:.1e}"
        else:
            p_text = f"p = {pval:.4g}"

        ax.text(
            text_x, text_y_start - 2 * text_y_step,
            p_text,
            transform=ax.transAxes,
            fontsize=16
        )

        ax.legend(fontsize=16, loc='upper right', frameon=False)
        ax.grid(False)

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved KM plot for cluster {cl}: {plot_path}")

    # Step 6: Multivariate Cox regression
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    clusters_to_plot=[0, 1, 2, 3]
):
    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    for cl in clusters_to_plot:
        col = f"cluster {cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(7, 6))

        # Style settings
        group_colors = {0: "blue", 1: "red"}
        sample_sizes = {}

        for group in [0, 1]:
            label = f"{'Low' if group == 0 else 'High'} {col} (n={sum(merged[group_col] == group)})"
            kmf.fit(T[merged[group_col] == group], E[merged[group_col] == group], label=label)
            kmf.plot_survival_function(
                ax=ax,
                ci_show=True,
                show_censors=True,   
                color=group_colors[group],
                linewidth=curve_width,
                censor_styles={'marker': '|', 'ms': 8, 'mew': 1}
            )

            # ‚Äì+‚Äì legend handle: horizontal line with vertical bar marker at center
            line_handle = Line2D(
                [], [],
                color=group_colors[group],
                linestyle='-',
                lw=curve_width,
                marker='|',
                markersize=8,            # vertical bar height
                markeredgewidth=1,#curve_width,  # thickness of vertical bar
                markerfacecolor='none',
                markerfacecoloralt='none',
                label=label
            )
            legend_handles.append(line_handle)

            sample_sizes[group] = sum(merged[group_col] == group)

        # Log-rank test
        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"Log-rank test failed: {e}")
            pval = float('nan')

        # Cox regression for HR
        cox_df = merged[["OS.time", "OS", col]].copy()
        cox_df.columns = ["time", "event", "score"]
        cph_single = CoxPHFitter()
        cph_single.fit(cox_df, duration_col="time", event_col="event")
        hr = cph_single.hazard_ratios_["score"]

        # Annotations
        ax.set_title("Overall Survival", fontsize=20)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Percent survival", fontsize=16)
        ax.tick_params(axis='both', labelsize=14)
        ax.set_ylim(0, 1.05)

        # Convert survival to percent
        yticks = ax.get_yticks()
        ax.set_yticklabels([f"{y:.0%}" for y in yticks])

        ax.text(0.05, 0.15, f"HR={hr:.2f}", transform=ax.transAxes, fontsize=14)
        ax.text(0.05, 0.08, f"P={pval:.3g}", transform=ax.transAxes, fontsize=14)

        # ax.legend(loc="lower left", fontsize=12, frameon=False)
        ax.legend(loc="upper right", fontsize=12, frameon=False)

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved plot: {plot_path}")

    # Multivariate Cox
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df_all = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df_all.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df_all, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis_dash(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    clusters_to_plot=[0, 1, 2, 3]
):
    os.makedirs(output_dir, exist_ok=True)

    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    for cl in clusters_to_plot:
        col = f"cluster {cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group_{cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(7, 6))

        group_colors = {0: "blue", 1: "red"}
        sample_sizes = {}

        for group in [0, 1]:
            mask = merged[group_col] == group
            label = f"{'Low' if group == 0 else 'High'} {col} (n={mask.sum()})"
            kmf.fit(T[mask], E[mask], label=label)
            kmf.plot_survival_function(
                ax=ax,
                ci_show=True,
                show_censors=True,
                color=group_colors[group],
                linewidth=2,
                linestyle='-',    
                censor_styles={'marker': '|', 'ms': 8, 'mew': 1}  # ‚ûú Short vertical lines = censoring marks
            )
            sample_sizes[group] = mask.sum()

        # Log-rank test
        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"Log-rank test failed: {e}")
            pval = float('nan')

        # Cox regression for HR
        cox_df = merged[["OS.time", "OS", col]].copy()
        cox_df.columns = ["time", "event", "score"]
        cph_single = CoxPHFitter()
        cph_single.fit(cox_df, duration_col="time", event_col="event")
        hr = cph_single.hazard_ratios_["score"]

        # Annotations
        ax.set_title(f"Survival by {col} Expression", fontsize=20)
        ax.set_xlabel("Time (Days)", fontsize=16)
        ax.set_ylabel("Percent Survival", fontsize=16)
        ax.tick_params(axis='both', labelsize=14)
        ax.set_ylim(0, 1.05)

        yticks = ax.get_yticks()
        ax.set_yticklabels([f"{y:.0%}" for y in yticks])

        ax.text(0.05, 0.15, f"HR = {hr:.2f}", transform=ax.transAxes, fontsize=14)
        ax.text(0.05, 0.08, f"P = {pval:.3g}", transform=ax.transAxes, fontsize=14)

        # ‚úÖ Legend top right with larger font
        ax.legend(loc="upper right", fontsize=14, frameon=False)

        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved plot: {plot_path}")

    # Multivariate Cox
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df_all = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df_all.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    cph.fit(cox_df_all, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def perform_survival_analysis_cluster_assigned(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    cancer_type="KIRC",
    min_samples_per_cluster=10
):
    print("üîç Running cluster-assigned survival analysis...")

    os.makedirs(output_dir, exist_ok=True)

    # üì• Load and clean
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # üóÇÔ∏è Map genes to clusters
    gene_to_cluster = {gene: cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # üßÆ Average per cluster
    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    # Assign each patient to max-score cluster
    assigned_clusters = (
        cluster_expr_df.idxmax(axis=1).str.extract(r'cluster_(\d+)')[0].astype(int)
    )
    cluster_expr_df['assigned_cluster'] = assigned_clusters

    # üóÇÔ∏è Merge
    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlap between survival and expression.")
        return

    T = merged_df['OS.time']
    E = merged_df['OS']
    groups = merged_df['assigned_cluster']

    fig, ax = plt.subplots(figsize=(12, 8))
    kmf = KaplanMeierFitter()
    ax.tick_params(axis='both', which='major', labelsize=16)

    handles = []
    labels = []

    # üìà KM per cluster
    unique_clusters = sorted(groups.unique())
    for cluster_id in unique_clusters:
        mask = (groups == cluster_id)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped due to insufficient samples ({mask.sum()}).")
            continue
        kmf.fit(T[mask], E[mask], label=f"Cluster {cluster_id}")
        line = kmf.plot(ax=ax, ci_show=True, show_censors=True, linewidth=2)
        handle = line.get_lines()[-1]
        handles.append(handle)
        labels.append(f"Cluster {cluster_id} (n={mask.sum()})")

    # üìå Overall log-rank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # üìå Cox PH ‚Äî robust checks
    cox_df = merged_df[['OS.time', 'OS', 'assigned_cluster']].copy()
    cox_df.columns = ['time', 'event', 'cluster']

    # Drop any missing
    cox_df = cox_df.dropna()

    # Convert cluster to category
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    # Drop clusters with no events
    valid_clusters = []
    for cl in cox_df['cluster'].cat.categories:
        events = cox_df.loc[cox_df['cluster'] == cl, 'event'].sum()
        if events == 0:
            print(f"‚ö†Ô∏è Cluster {cl} has no events. Dropping from Cox PH.")
        else:
            valid_clusters.append(cl)

    cox_df = cox_df[cox_df['cluster'].isin(valid_clusters)].copy()
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    # Fit Cox PH safely
    if len(valid_clusters) <= 1:
        print("‚ùå Not enough valid clusters for Cox PH model. Skipping Cox.")
        representative_hr = np.nan
        representative_ci = np.nan
    else:
        cph = CoxPHFitter()
        try:
            cph.fit(cox_df, duration_col='time', event_col='event')
            hr_values = cph.summary['exp(coef)'].values
            representative_hr = np.mean(hr_values) if len(hr_values) > 0 else np.nan
            ci_widths = cph.summary['coef upper 95%'] - cph.summary['coef lower 95%']
            representative_ci = ci_widths.mean() if len(ci_widths) > 0 else np.nan
            # Save
            cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
        except Exception as e:
            print(f"‚ùå Cox PH fitting failed: {e}")
            representative_hr = np.nan
            representative_ci = np.nan

    # üìä Annotations
    text_x = 0.03
    text_y_start = 0.12
    text_y_step = 0.045

    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.set_title(f"{cancer_type} Survival (Cluster-Assigned)", fontsize=20)
    ax.grid(False)

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes, fontsize=18
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes, fontsize=18
    )
    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    ax.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=ax.transAxes, fontsize=18
    )

    # ‚úÖ Legend
    ax.legend(handles=handles, labels=labels, fontsize=16, loc='upper right', frameon=False)

    plt.tight_layout()
    save_path = os.path.join(output_dir, "km_cluster_assigned.png")
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"‚úÖ Cluster-assigned survival plot saved: {save_path}")

    return (cph.summary if len(valid_clusters) > 1 else None), merged_df

def run_survival_analysis_cox_not_cencer(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
):
    os.makedirs(output_dir, exist_ok=True)

    # Step 1: Load survival and expression data
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Step 2: Cluster gene sets ‚Üí cluster scores per patient
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster_{cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Step 3: Merge with survival and assign global score
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs.")
        return

    cluster_cols = [col for col in patient_cluster_scores.columns if col in merged.columns]
    merged["global_cluster_score"] = merged[cluster_cols].mean(axis=1)

    T = merged["OS.time"]
    E = merged["OS"]
    threshold = merged["global_cluster_score"].median() if cluster_threshold == "median" else merged["global_cluster_score"].mean()
    merged["global_group"] = (merged["global_cluster_score"] >= threshold).astype(int)

    # Step 4: Kaplan-Meier plot with censor marks
    kmf_high = KaplanMeierFitter()
    kmf_low = KaplanMeierFitter()

    fig, ax = plt.subplots(figsize=(8, 6))

    mask_high = merged["global_group"] == 1
    mask_low = merged["global_group"] == 0

    T_high, E_high = T[mask_high], E[mask_high]
    T_low, E_low = T[mask_low], E[mask_low]

    kmf_high.fit(T_high, E_high, label="High")
    kmf_low.fit(T_low, E_low, label="Low")

    # ‚ûú Plot with censor tick marks (|) and larger marker size
    kmf_high.plot_survival_function(
        ax=ax,
        ci_show=True,
        show_censors=True,
        color="#d62728",
        linewidth=2,
        censor_styles={'marker': '|', 'ms': 8, 'mew': 1.0}  # larger marker
    )
    kmf_low.plot_survival_function(
        ax=ax,
        ci_show=True,
        show_censors=True,        
        color="#17becf",
        linewidth=2,
        censor_styles={'marker': '|', 'ms': 8, 'mew': 1.0}
    )

    # Add at-risk table
    '''add_at_risk_counts(kmf_high, kmf_low, ax=ax)'''

    # Step 5: Logrank p-value and sample sizes
    result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
    pval = result.p_value

    sample_sizes = {"High": mask_high.sum(), "Low": mask_low.sum()}

    ax.set_title("Survival by Global Cluster Expression", fontsize=18)
    ax.set_xlabel("Time (Days)", fontsize=16)
    ax.set_ylabel("Overall Survival Probability", fontsize=16)
    ax.tick_params(axis='both', labelsize=14)
    ax.grid(False)
    ax.legend(fontsize=16, loc='upper right', frameon=False)

    # Annotate p-value and N
    ax.text(0.05, 0.15, f"N = {sample_sizes['High'] + sample_sizes['Low']}", transform=ax.transAxes, fontsize=14)
    ax.text(0.05, 0.08, f"p = {pval:.4g}", transform=ax.transAxes, fontsize=14)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, "km_global_clusters.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ KM plot saved: {plot_path}")

    # Step 6: Cox proportional hazards model
    cox_df = merged[["OS.time", "OS"] + cluster_cols].copy()
    cox_df.columns = ["time", "event"] + cluster_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def run_survival_analysis_by_cluster_cluster_assigned_not_cencer_mark_legene(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC",
    min_samples_per_cluster=5
):
    """
    Cluster-assigned survival:
    - Each patient assigned to cluster with highest mean expression.
    - One KM curve per cluster (no high/low).
    - Shows censor tick marks, multivariate log-rank p, CI, HR.
    """
    os.makedirs(output_dir, exist_ok=True)

    # Load data
    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)

    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Map clusters to genes
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # Compute mean expression per patient per cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Assign each patient to cluster with max mean score
    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster_(\d+)')[0].astype(int)
    )
    patient_cluster_scores["cluster_id"] = cluster_assignments.values

    # Merge with survival data
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No overlap between survival and expression data.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    fig, ax = plt.subplots(figsize=(10, 7))
    kmf = KaplanMeierFitter()
    unique_clusters = sorted(groups.unique())

    # Use distinct colors if needed
    color_list = plt.cm.tab10.colors

    handles = []
    labels = []

    for idx, cl in enumerate(unique_clusters):
        mask = (groups == cl)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cl} skipped: too few patients.")
            continue

        kmf.fit(T[mask], E[mask], label=f"Cluster {cl}")

        line = kmf.plot_survival_function(
            ax=ax,
            ci_show=True,
            show_censors=True,   
            color=group_colors[group],
            linewidth=curve_width,
            censor_styles={'marker': '|', 'ms': 8, 'mew': 1}
        )

        # ‚Äì+‚Äì legend handle: horizontal line with vertical bar marker at center
        line_handle = Line2D(
            [], [],
            color=group_colors[group],
            linestyle='-',
            lw=curve_width,
            marker='|',
            markersize=8,            # vertical bar height
            markeredgewidth=1,#curve_width,  # thickness of vertical bar
            markerfacecolor='none',
            markerfacecoloralt='none',
            label=label
        )
        legend_handles.append(line_handle)

        labels.append(f"Cluster {cl} (n={mask.sum()})")

    # Multivariate log-rank test
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox proportional hazards model
    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster"]
    cox_df["cluster"] = cox_df["cluster"].astype("category")

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    hr_values = cph.summary["exp(coef)"].values
    representative_hr = np.mean(hr_values) if len(hr_values) else np.nan

    ci_widths = cph.summary["coef upper 95%"] - cph.summary["coef lower 95%"]
    representative_ci = ci_widths.mean() if len(ci_widths) else np.nan

    ax.set_title(f"{cancer_type} Survival by Cluster", fontsize=22)
    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.tick_params(axis='both', labelsize=16)
    ax.grid(False)

    # Text annotations
    text_x = 0.03
    text_y_start = 0.12
    text_y_step = 0.045

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes,
        fontsize=18
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes,
        fontsize=18
    )

    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    ax.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=ax.transAxes,
        fontsize=18
    )

    # Legend top right
    ax.legend(handles=handles, labels=labels, loc='upper right', fontsize=14, frameon=False)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_{cancer_type}_assigned.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ KM plot saved: {plot_path}")

    # Save Cox summary table
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Cox summary saved: {os.path.join(output_dir, 'cox_summary.csv')}")

    return result.summary, merged

def run_survival_analysis_by_cluster_cluster_assigned(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC",
    min_samples_per_cluster=5
):
    """
    Cluster-assigned survival:
    - Each patient assigned to cluster with highest mean expression.
    - One KM curve per cluster (no high/low).
    - Shows censor tick marks on curve AND in legend.
    - Multivariate log-rank p, CI, HR.
    """
    os.makedirs(output_dir, exist_ok=True)

    # Load data
    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)

    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Map clusters to genes
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    # Compute mean expression per patient per cluster
    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Assign each patient to cluster with max mean score
    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    patient_cluster_scores["cluster_id"] = cluster_assignments.values

    # Merge with survival data
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No overlap between survival and expression data.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    fig, ax = plt.subplots(figsize=(10, 7))
    kmf = KaplanMeierFitter()
    unique_clusters = sorted(groups.unique())

    color_list = plt.cm.tab10.colors

    handles = []
    labels = []

    for idx, cl in enumerate(unique_clusters):
        mask = (groups == cl)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cl} skipped: too few patients.")
            continue

        kmf.fit(T[mask], E[mask], label=f"Cluster {cl}")

        # Plot with censor ticks
        kmf.plot(
            ax=ax,
            ci_show=True,
            show_censors=True,
            censor_styles={'marker': '|', 'ms': 8, 'mew': 1.0},
            # color=color_list[idx % len(color_list)],
            color = CLUSTER_COLORS.get(cl, "#000000"),
            linewidth=2
        )

        # Line handle for legend
        curve_handle = mlines.Line2D([], [], color = CLUSTER_COLORS.get(cl, "#000000"), linewidth=2)
        handles.append(curve_handle)
        labels.append(f"Cluster {cl} (n={mask.sum()})")

    # ‚ûú Add separate censor tick mark handle for legend
    censor_handle = mlines.Line2D([], [], color='k', marker='|', linestyle='None', markersize=8, label='Censored')
    handles.append(censor_handle)
    labels.append('Censored')

    # Multivariate log-rank test
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox proportional hazards model
    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster"]
    cox_df["cluster"] = cox_df["cluster"].astype("category")

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    hr_values = cph.summary["exp(coef)"].values
    representative_hr = np.mean(hr_values) if len(hr_values) else np.nan

    ci_widths = cph.summary["coef upper 95%"] - cph.summary["coef lower 95%"]
    representative_ci = ci_widths.mean() if len(ci_widths) else np.nan

    ax.set_title(f"{cancer_type}", fontsize=22)
    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.tick_params(axis='both', labelsize=16)
    ax.grid(False)

    # Text annotations
    text_x = 0.03
    text_y_start = 0.10
    text_y_step = 0.045

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes,
        fontsize=18
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes,
        fontsize=18
    )

    if pval < 0.0001:
        p_text = f"p = {pval:.1e}"
    else:
        p_text = f"p = {pval:.4g}"

    ax.text(
        text_x, text_y_start - text_y_step,
        p_text,
        transform=ax.transAxes,
        fontsize=18
    )

    ax.legend(handles=handles, labels=labels, loc='upper right', fontsize=14, frameon=False)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_{cancer_type}_assigned.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ KM plot saved: {plot_path}")

    # Save Cox summary table
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Cox summary saved: {os.path.join(output_dir, 'cox_summary.csv')}")

    return result.summary, merged

def run_survival_analysis_cox(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
):
    """
    Global cluster survival analysis:
    - Averages cluster scores per patient.
    - Splits patients into High vs Low by threshold.
    - KM curve with censor ticks & legend entry.
    - Logrank p + Cox model.
    """
    os.makedirs(output_dir, exist_ok=True)

    # Load survival and expression data
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    # Cluster gene sets ‚Üí cluster scores per patient
    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    # Merge with survival and compute global score
    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs.")
        return

    cluster_cols = [col for col in patient_cluster_scores.columns if col in merged.columns]
    merged["global_cluster_score"] = merged[cluster_cols].mean(axis=1)

    T = merged["OS.time"]
    E = merged["OS"]
    threshold = merged["global_cluster_score"].median() if cluster_threshold == "median" else merged["global_cluster_score"].mean()
    merged["global_group"] = (merged["global_cluster_score"] >= threshold).astype(int)

    # Kaplan-Meier plot with censor ticks
    kmf_high = KaplanMeierFitter()
    kmf_low = KaplanMeierFitter()

    fig, ax = plt.subplots(figsize=(8, 6))

    mask_high = merged["global_group"] == 1
    mask_low = merged["global_group"] == 0

    T_high, E_high = T[mask_high], E[mask_high]
    T_low, E_low = T[mask_low], E[mask_low]

    kmf_high.fit(T_high, E_high, label="High")
    kmf_low.fit(T_low, E_low, label="Low")

    kmf_high.plot_survival_function(
        ax=ax,
        ci_show=True,
        show_censors=True,
        color="#d62728",
        linewidth=2,
        censor_styles={'marker': '|', 'ms': 8, 'mew': 1.0}
    )
    kmf_low.plot_survival_function(
        ax=ax,
        ci_show=True,
        show_censors=True,
        color="#17becf",
        linewidth=2,
        censor_styles={'marker': '|', 'ms': 8, 'mew': 1.0}
    )

    # Add censor tick to legend
    line_high = mlines.Line2D([], [], color="#d62728", linewidth=2, label="High")
    line_low = mlines.Line2D([], [], color="#17becf", linewidth=2, label="Low")
    censor_handle = mlines.Line2D([], [], color='k', marker='|', linestyle='None', markersize=8, label='Censored')

    # Logrank p-value
    result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
    pval = result.p_value

    sample_sizes = {"High": mask_high.sum(), "Low": mask_low.sum()}

    ax.set_title("Survival by Global Cluster Expression", fontsize=18)
    ax.set_xlabel("Time (Days)", fontsize=16)
    ax.set_ylabel("Overall Survival Probability", fontsize=16)
    ax.tick_params(axis='both', labelsize=14)
    ax.grid(False)

    ax.legend(handles=[line_high, line_low, censor_handle], fontsize=14, loc='upper right', frameon=False)

    # Annotate p-value and N
    # ax.text(0.05, 0.095, f"N = {sample_sizes['High'] + sample_sizes['Low']}", transform=ax.transAxes, fontsize=14)
    ax.text(0.05, 0.05, f"p-value = {pval:.4g}", transform=ax.transAxes, fontsize=14)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, "km_global_clusters.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ KM plot saved: {plot_path}")

    # Cox PH
    cox_df = merged[["OS.time", "OS"] + cluster_cols].copy()
    cox_df.columns = ["time", "event"] + cluster_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, 'cox_summary.csv')}")

    return cph.summary, merged, patient_cluster_scores

def perform_survival_analysis_cluster_assigned_(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    cancer_type="KIRC",
    min_samples_per_cluster=10
):
    print("üîç Running cluster-assigned survival analysis...")

    os.makedirs(output_dir, exist_ok=True)

    # Load & clean
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # Map genes to clusters
    gene_to_cluster = {gene: cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Average per cluster
    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    # Assign each patient to cluster with max score
    assigned_clusters = (
        cluster_expr_df.idxmax(axis=1).str.extract(r'cluster_(\d+)')[0].astype(int)
    )
    cluster_expr_df['assigned_cluster'] = assigned_clusters

    # Merge with survival
    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlap between survival and expression.")
        return

    T = merged_df['OS.time']
    E = merged_df['OS']
    groups = merged_df['assigned_cluster']

    fig, ax = plt.subplots(figsize=(12, 8))
    kmf = KaplanMeierFitter()
    ax.tick_params(axis='both', which='major', labelsize=16)

    handles = []
    labels = []

    colors = plt.cm.tab10.colors

    # KM per cluster
    unique_clusters = sorted(groups.unique())
    for idx, cluster_id in enumerate(unique_clusters):
        mask = (groups == cluster_id)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped: too few samples ({mask.sum()}).")
            continue

        kmf.fit(T[mask], E[mask], label=f"Cluster {cluster_id}")
        kmf.plot(
            ax=ax,
            ci_show=True,
            show_censors=True,
            color=colors[idx % len(colors)],
            linewidth=2,
            censor_styles={'marker': '|', 'ms': 8, 'mew': 1.0}
        )

        line = Line2D([], [], color=colors[idx % len(colors)], linewidth=2, label=f"Cluster {cluster_id} (n={mask.sum()})")
        handles.append(line)

    # Add separate censor mark handle
    censor_handle = Line2D([], [], color='k', marker='|', linestyle='None', markersize=8, label='Censored')
    handles.append(censor_handle)

    # Logrank
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox PH robust
    cox_df = merged_df[['OS.time', 'OS', 'assigned_cluster']].copy()
    cox_df.columns = ['time', 'event', 'cluster']
    cox_df = cox_df.dropna()
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    valid_clusters = []
    for cl in cox_df['cluster'].cat.categories:
        if cox_df.loc[cox_df['cluster'] == cl, 'event'].sum() > 0:
            valid_clusters.append(cl)
        else:
            print(f"‚ö†Ô∏è Cluster {cl} has no events. Dropping from Cox PH.")

    cox_df = cox_df[cox_df['cluster'].isin(valid_clusters)].copy()
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    if len(valid_clusters) <= 1:
        print("‚ùå Not enough valid clusters for Cox PH.")
        representative_hr = np.nan
        representative_ci = np.nan
        cph_summary = None
    else:
        cph = CoxPHFitter()
        try:
            cph.fit(cox_df, duration_col='time', event_col='event')
            hr_values = cph.summary['exp(coef)'].values
            representative_hr = np.mean(hr_values)
            ci_widths = cph.summary['coef upper 95%'] - cph.summary['coef lower 95%']
            representative_ci = ci_widths.mean()
            cph.summary.to_csv(os.path.join(output_dir, "cox_summary.csv"))
            cph_summary = cph.summary
        except Exception as e:
            print(f"‚ùå Cox PH fitting failed: {e}")
            representative_hr = np.nan
            representative_ci = np.nan
            cph_summary = None

    # Annotations
    ax.set_xlabel("Time (Days)", fontsize=18)
    ax.set_ylabel("Survival Probability", fontsize=18)
    ax.set_title(f"{cancer_type} Survival (Cluster-Assigned)", fontsize=20)
    ax.grid(False)

    text_x = 0.03
    text_y_start = 0.08
    text_y_step = 0.045

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes, fontsize=20
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes, fontsize=20
    )

    ax.text(
        text_x, text_y_start - text_y_step,
        f"p-value = {pval:.4g}" if pval >= 0.0001 else f"p-value = {pval:.1e}",
        transform=ax.transAxes, fontsize=20
    )

    ax.legend(handles=handles, fontsize=20, loc='upper right', frameon=False)

    plt.tight_layout()
    save_path = os.path.join(output_dir, "km_cluster_assigned.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Cluster-assigned survival plot saved: {save_path}")

    return cph_summary, merged_df

def run_survival_analysis(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    clusters_to_plot=[0, 1, 2, 3],
    cancer_type="KIRC",
):
    os.makedirs(output_dir, exist_ok=True)

    plt.rcParams.update({
        "font.size": 22,           # base font size
        "axes.titlesize": 26,      # title
        "axes.labelsize": 24,      # axis labels
        "xtick.labelsize": 20,
        "ytick.labelsize": 20,
        "legend.fontsize": 20,
        "figure.titlesize": 28
    })
    
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes found for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs between expression and survival data.")
        return

    for cl in clusters_to_plot:
        col = f"cluster {cl}"
        if col not in merged.columns:
            print(f"‚ö†Ô∏è Cluster column {col} not found. Skipping.")
            continue

        T = merged["OS.time"]
        E = merged["OS"]
        threshold = merged[col].median() if cluster_threshold == "median" else merged[col].mean()
        group_col = f"group {cl}"
        merged[group_col] = (merged[col] >= threshold).astype(int)

        kmf = KaplanMeierFitter()
        fig, ax = plt.subplots(figsize=(7, 6))

        group_colors = {0: "#1f77b4", 1: "#d62728"}  # blue, red
        curve_width = 2

        legend_handles = []

        for group in [0, 1]:
            mask = merged[group_col] == group
            label = f"{'Low' if group == 0 else 'High'} {col} (n={mask.sum()})"

            kmf.fit(T[mask], E[mask], label=label)
            # Plot the KM curve without showing censor marks:
            kmf.plot_survival_function(
                ax=ax,
                ci_show=True,
                show_censors=True,   
                color=group_colors[group],
                linewidth=curve_width,
                censor_styles={'marker': '|', 'ms': 8, 'mew': 1}
            )

            # ‚Äì+‚Äì legend handle: horizontal line with vertical bar marker at center
            line_handle = Line2D(
                [], [],
                color=group_colors[group],
                linestyle='-',
                lw=curve_width,
                marker='|',
                markersize=6,            # vertical bar height
                markeredgewidth=1,#curve_width,  # thickness of vertical bar
                markerfacecolor='none',
                markerfacecoloralt='none',
                label=label
            )
            legend_handles.append(line_handle)

        # Log-rank test
        try:
            result = logrank_test(
                T[merged[group_col] == 1],
                T[merged[group_col] == 0],
                event_observed_A=E[merged[group_col] == 1],
                event_observed_B=E[merged[group_col] == 0]
            )
            pval = result.p_value
        except Exception as e:
            print(f"‚ö†Ô∏è Log-rank failed: {e}")
            pval = float('nan')

        # Cox PH
        cox_df = merged[["OS.time", "OS", col]].copy()
        cox_df.columns = ["time", "event", "score"]
        cph_single = CoxPHFitter()
        try:
            cph_single.fit(cox_df, duration_col="time", event_col="event")
            hr = cph_single.hazard_ratios_["score"]
        except Exception as e:
            print(f"‚ö†Ô∏è Cox fit failed: {e}")
            hr = np.nan

        ax.set_title(f"Overall Survival", fontsize=26)
        ax.set_xlabel("Time (Days)", fontsize=26)
        ax.set_ylabel("Survival Probability", fontsize=26)
        ax.tick_params(axis='both', labelsize=24)
        ax.set_ylim(0, 1.05)

        yticks = ax.get_yticks()
        ax.set_yticklabels([f"{y:.0%}" for y in yticks])

        ax.text(0.05, 0.12, f"HR = {hr:.2f}" if not np.isnan(hr) else "HR=N/A", transform=ax.transAxes, fontsize=20)
        ax.text(0.05, 0.05, f"p-value = {pval:.3g}" if not np.isnan(pval) else "p-value = N/A", transform=ax.transAxes, fontsize=20)

        # ax.legend(handles=legend_handles, loc="upper right", fontsize=12, frameon=False)
        ax.legend(
            handles=legend_handles,
            loc="upper right",
            fontsize=20,
            frameon=False,
            handlelength=1.0  # shorter horizontal line
        )


        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"km_cluster_{cl}_{cancer_type}.png")
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"‚úÖ Saved plot: {plot_path}")

    # Multivariate Cox PH
    cox_cols = [c for c in patient_cluster_scores.columns if c in merged.columns]
    cox_df_all = merged[["OS.time", "OS"] + cox_cols].copy()
    cox_df_all.columns = ["time", "event"] + cox_cols

    cph = CoxPHFitter()
    try:
        cph.fit(cox_df_all, duration_col="time", event_col="event")
        cph.summary.to_csv(os.path.join(output_dir, f"cox_summary_{cancer_type}.csv"))
        print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, f'cox_summary_{cancer_type}.csv')}")
    except Exception as e:
        print(f"‚ö†Ô∏è Multivariate Cox failed: {e}")

    return cph.summary, merged, patient_cluster_scores

def perform_survival_analysis_cluster_assigned(
    survival_path: str,
    expression_matrix_path: str,
    cluster_labels: np.ndarray,
    top_gene_names: list,
    output_dir: str,
    min_samples_per_cluster=5,
    cancer_type="KIRC",
):
    print("üîç Running cluster-assigned survival analysis...")

    os.makedirs(output_dir, exist_ok=True)

    # Load survival data
    survival_df = pd.read_csv(survival_path, sep='\t')
    survival_df['sample'] = survival_df['sample'].str.strip().str[:15]
    survival_df = survival_df.dropna(subset=['OS', 'OS.time'])

    # Load expression
    expr_df = pd.read_csv(expression_matrix_path, sep='\t', index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = [col.strip().upper()[:15] for col in expr_df.columns]

    top_gene_names = [g.upper() for g in top_gene_names]
    expr_df = expr_df.loc[expr_df.index.intersection(top_gene_names)]

    if expr_df.empty:
        print("‚ùå No top-k genes found in expression matrix.")
        return

    # Map genes to clusters
    gene_to_cluster = {gene: cluster_labels[i] for i, gene in enumerate(top_gene_names)}
    expr_df['cluster'] = expr_df.index.map(gene_to_cluster)

    # Average per cluster
    cluster_expr_df = pd.DataFrame(index=expr_df.columns)
    for cluster_id in sorted(np.unique(cluster_labels)):
        genes = expr_df[expr_df['cluster'] == cluster_id].drop(columns='cluster').index
        if len(genes) == 0:
            continue
        cluster_expr_df[f'cluster_{cluster_id}'] = expr_df.loc[genes].mean(axis=0)

    # Assign each patient to cluster with max score
    assigned_clusters = (
        cluster_expr_df.idxmax(axis=1).str.extract(r'cluster_(\d+)')[0].astype(int)
    )
    cluster_expr_df['assigned_cluster'] = assigned_clusters

    # Merge with survival
    merged_df = survival_df.merge(cluster_expr_df, left_on='sample', right_index=True, how='inner')
    if merged_df.empty:
        print("‚ùå No overlap between survival and expression.")
        return

    T = merged_df['OS.time']
    E = merged_df['OS']
    groups = merged_df['assigned_cluster']

    fig, ax = plt.subplots(figsize=(10, 8))
    kmf = KaplanMeierFitter()
    ax.tick_params(axis='both', which='major', labelsize=20)

    handles = []
    # colors = plt.cm.tab10.colors
    curve_width = 2

    # KM per cluster
    unique_clusters = sorted(groups.unique())
    for idx, cluster_id in enumerate(unique_clusters):
        mask = (groups == cluster_id)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cluster_id} skipped: too few samples ({mask.sum()}).")
            continue

        color = CLUSTER_COLORS.get(cluster_id, "#333333")

        kmf.fit(T[mask], E[mask], label=f"Cluster {cluster_id}")
        kmf.plot(
            ax=ax,
            ci_show=True,
            show_censors=True,
            color=color,
            linewidth=curve_width,
            censor_styles={'marker': '|', 'ms': 8, 'mew': 1}
        )

        line_handle = Line2D(
            [], [],
            color=color,
            linestyle='-',
            lw=curve_width,
            marker='|',
            markersize=6,
            markeredgewidth=1,
            markerfacecolor='none',
            label=f"Cluster {cluster_id} (n={mask.sum()})"
        )
        handles.append(line_handle)

    # Logrank test
    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    # Cox PH
    cox_df = merged_df[['OS.time', 'OS', 'assigned_cluster']].copy()
    cox_df.columns = ['time', 'event', 'cluster']
    cox_df = cox_df.dropna()
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    valid_clusters = []
    for cl in cox_df['cluster'].cat.categories:
        if cox_df.loc[cox_df['cluster'] == cl, 'event'].sum() > 0:
            valid_clusters.append(cl)
        else:
            print(f"‚ö†Ô∏è Cluster {cl} has no events. Dropping from Cox PH.")

    cox_df = cox_df[cox_df['cluster'].isin(valid_clusters)].copy()
    cox_df['cluster'] = cox_df['cluster'].astype('category')

    if len(valid_clusters) <= 1:
        print("‚ùå Not enough valid clusters for Cox PH.")
        representative_hr = np.nan
        representative_ci = np.nan
        cph_summary = None
    else:
        cph = CoxPHFitter()
        try:
            cph.fit(cox_df, duration_col='time', event_col='event')
            hr_values = cph.summary['exp(coef)'].values
            representative_hr = np.mean(hr_values)
            ci_widths = cph.summary['coef upper 95%'] - cph.summary['coef lower 95%']
            representative_ci = ci_widths.mean()
            cph.summary.to_csv(os.path.join(output_dir, f"cox_summary_{cancer_type}.csv"))
            cph_summary = cph.summary
        except Exception as e:
            print(f"‚ùå Cox PH fitting failed: {e}")
            representative_hr = np.nan
            representative_ci = np.nan
            cph_summary = None

    # Annotations
    ax.set_xlabel("Time (Days)", fontsize=28)
    ax.set_ylabel("Survival Probability", fontsize=28)
    ax.set_title(f"{cancer_type} Survival (Cluster-Assigned)", fontsize=28)
    ax.tick_params(axis='both', labelsize=28)
    ax.grid(False)

    text_x = 0.03
    text_y_start = 0.15
    text_y_step = 0.085

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes, fontsize=28
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes, fontsize=28
    )
    ax.text(
        text_x, text_y_start - text_y_step,
        f"p-value = {pval:.4g}" if pval >= 0.0001 else f"p-value = {pval:.1e}",
        transform=ax.transAxes, fontsize=28
    )

    ax.legend(handles=handles, fontsize=28, loc='upper right', frameon=False, handlelength=1.0)

    
    plt.tight_layout()
    save_path = os.path.join(output_dir, f"km_cluster_assigned_{cancer_type}.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ Cluster-assigned survival plot saved: {save_path}")

    return cph_summary, merged_df

def run_survival_analysis_cox(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cluster_threshold="median",
    cancer_type="KIRC",
):
    """
    Global cluster survival analysis:
    - Averages cluster scores per patient.
    - Splits patients into High vs Low by threshold.
    - KM curve with censor ticks & ‚Äì+‚Äì legend.
    - Logrank p + Cox model.
    """
    os.makedirs(output_dir, exist_ok=True)

    plt.rcParams.update({
        "font.size": 22,           # base font size
        "axes.titlesize": 26,      # title
        "axes.labelsize": 24,      # axis labels
        "xtick.labelsize": 20,
        "ytick.labelsize": 20,
        "legend.fontsize": 20,
        "figure.titlesize": 28
    })
    
    survival_df = pd.read_csv(survival_path, sep="\t")
    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()

    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No matching patient IDs.")
        return

    cluster_cols = [col for col in patient_cluster_scores.columns if col in merged.columns]
    merged["global_cluster_score"] = merged[cluster_cols].mean(axis=1)

    T = merged["OS.time"]
    E = merged["OS"]
    threshold = merged["global_cluster_score"].median() if cluster_threshold == "median" else merged["global_cluster_score"].mean()
    merged["global_group"] = (merged["global_cluster_score"] >= threshold).astype(int)

    kmf_high = KaplanMeierFitter()
    kmf_low = KaplanMeierFitter()
    fig, ax = plt.subplots(figsize=(8, 6))

    mask_high = merged["global_group"] == 1
    mask_low = merged["global_group"] == 0

    T_high, E_high = T[mask_high], E[mask_high]
    T_low, E_low = T[mask_low], E[mask_low]

    kmf_high.fit(T_high, E_high, label="High")
    kmf_low.fit(T_low, E_low, label="Low")

    kmf_high.plot_survival_function(
        ax=ax,
        ci_show=True,
        show_censors=True,
        color="#d62728",
        linewidth=2,
        censor_styles={'marker': '|', 'ms': 8, 'mew': 1}
    )
    kmf_low.plot_survival_function(
        ax=ax,
        ci_show=True,
        show_censors=True,
        color="#17becf",
        linewidth=2,
        censor_styles={'marker': '|', 'ms': 8, 'mew': 1}
    )

    # Legend handles with ‚Äì+‚Äì style
    line_high = Line2D(
        [], [], color="#d62728", lw=2, linestyle='-',
        marker='|', markersize=6, markeredgewidth=1,
        label=f"High (n={mask_high.sum()})"
    )
    line_low = Line2D(
        [], [], color="#17becf", lw=2, linestyle='-',
        marker='|', markersize=6, markeredgewidth=1,
        label=f"Low (n={mask_low.sum()})"
    )

    result = logrank_test(T_high, T_low, event_observed_A=E_high, event_observed_B=E_low)
    pval = result.p_value

    ax.set_title("Survival by Global Cluster Expression", fontsize=24)
    ax.set_xlabel("Time (Days)", fontsize=24)
    ax.set_ylabel("Overall Survival Probability", fontsize=24)
    ax.tick_params(axis='both', labelsize=22)
    ax.grid(False)

    ax.legend(handles=[line_high, line_low], fontsize=24, loc='upper right', frameon=False, handlelength=1.0)

    # ax.text(0.05, 0.095, f"N = {mask_high.sum() + mask_low.sum()}", transform=ax.transAxes, fontsize=14)
    ax.text(0.05, 0.05, f"p-value = {pval:.4g}", transform=ax.transAxes, fontsize=24)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"km_global_clusters_{cancer_type}.png")

    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ KM plot saved: {plot_path}")

    cox_df = merged[["OS.time", "OS"] + cluster_cols].copy()
    cox_df.columns = ["time", "event"] + cluster_cols

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")
    cph.summary.to_csv(os.path.join(output_dir, f"cox_summary_{cancer_type}.csv"))

    print(f"üìÑ Saved Cox summary: {os.path.join(output_dir, f'cox_summary_{cancer_type}.csv')}")

    return cph.summary, merged, patient_cluster_scores

def survival_analysis_by_cluster_assigned(
    survival_path,
    expr_path,
    node_names_topk,
    row_labels,
    output_dir,
    cancer_type="KIRC",
    min_samples_per_cluster=5
):
    """
    Cluster-assigned survival:
    - Each patient assigned to cluster with highest mean expression.
    - One KM curve per cluster.
    - KM plot with censor ticks.
    - Legend lines: ‚Äì+‚Äì style (line + vertical tick).
    - Multivariate log-rank p, CI, HR.
    """
    os.makedirs(output_dir, exist_ok=True)


    survival_df = pd.read_csv(survival_path, sep="\t")
    expr_df = pd.read_csv(expr_path, sep="\t", index_col=0)

    survival_df['_PATIENT'] = survival_df['_PATIENT'].str.upper()
    expr_df.index = expr_df.index.str.upper()
    expr_df.columns = expr_df.columns.str.upper().str[:12]

    cluster_to_genes = defaultdict(list)
    for gene, label in zip(node_names_topk, row_labels):
        cluster_to_genes[label].append(gene.upper())

    patient_cluster_scores = pd.DataFrame(index=expr_df.columns)
    for cl, genes in cluster_to_genes.items():
        valid_genes = list(set(genes) & set(expr_df.index))
        if not valid_genes:
            print(f"‚ö†Ô∏è No valid genes for cluster {cl}. Skipping.")
            continue
        patient_cluster_scores[f'cluster {cl}'] = expr_df.loc[valid_genes].mean(axis=0)

    cluster_assignments = (
        patient_cluster_scores.idxmax(axis=1).str.extract(r'cluster (\d+)')[0].astype(int)
    )
    patient_cluster_scores["cluster_id"] = cluster_assignments

    merged = survival_df.merge(patient_cluster_scores, left_on="_PATIENT", right_index=True)
    if merged.empty:
        print("‚ùå No overlap between survival and expression.")
        return

    T = merged["OS.time"]
    E = merged["OS"]
    groups = merged["cluster_id"]

    fig, ax = plt.subplots(figsize=(10, 7))
    kmf = KaplanMeierFitter()
    unique_clusters = sorted(groups.unique())

    color_list = plt.cm.tab10.colors

    handles = []

    for idx, cl in enumerate(unique_clusters):
        mask = (groups == cl)
        if mask.sum() < min_samples_per_cluster:
            print(f"‚ö†Ô∏è Cluster {cl} skipped: too few patients.")
            continue

        kmf.fit(T[mask], E[mask], label=f"Cluster {cl}")

        kmf.plot(
            ax=ax,
            ci_show=True,
            show_censors=True,
            censor_styles={'marker': '|', 'ms': 8, 'mew': 1},
            # color=color_list[idx % len(color_list)],
            color = CLUSTER_COLORS.get(cl, "#000000"),
            linewidth=2
        )

        curve_handle = Line2D(
            [], [],
            # color=color_list[idx % len(color_list)],
            color = CLUSTER_COLORS.get(cl, "#000000"),
            linestyle='-',
            lw=2,
            marker='|',
            markersize=6,
            markeredgewidth=1,
            label=f"Cluster {cl} (n={mask.sum()})"
        )
        handles.append(curve_handle)

    result = multivariate_logrank_test(T, groups=groups, event_observed=E)
    pval = result.p_value

    cox_df = merged[["OS.time", "OS", "cluster_id"]].copy()
    cox_df.columns = ["time", "event", "cluster"]
    cox_df["cluster"] = cox_df["cluster"].astype("category")

    cph = CoxPHFitter()
    cph.fit(cox_df, duration_col="time", event_col="event")

    hr_values = cph.summary["exp(coef)"].values
    representative_hr = np.mean(hr_values) if len(hr_values) else np.nan

    ci_widths = cph.summary["coef upper 95%"] - cph.summary["coef lower 95%"]
    representative_ci = ci_widths.mean() if len(ci_widths) else np.nan

    ax.set_title(f"{cancer_type}", fontsize=28)
    ax.set_xlabel("Time (Days)", fontsize=28)
    ax.set_ylabel("Survival Probability", fontsize=28)
    ax.tick_params(axis='both', labelsize=28)
    ax.grid(False)

    text_x = 0.03
    text_y_start = 0.15
    text_y_step = 0.085

    ax.text(
        text_x, text_y_start + text_y_step,
        f"CI = {representative_ci:.3f}" if not np.isnan(representative_ci) else "CI = N/A",
        transform=ax.transAxes, fontsize=28
    )
    ax.text(
        text_x, text_y_start,
        f"HR = {representative_hr:.2f}" if not np.isnan(representative_hr) else "HR = N/A",
        transform=ax.transAxes, fontsize=28
    )
    ax.text(
        text_x, text_y_start - text_y_step,
        f"p-value = {pval:.4g}" if pval >= 0.0001 else f"p-value = {pval:.1e}",
        transform=ax.transAxes, fontsize=28
    )

    ax.legend(handles=handles, loc='upper right', fontsize=28, frameon=False, handlelength=1.0)

    plt.rcParams.update({
        "font.size": 22,           # base font size
        "axes.titlesize": 26,      # title
        "axes.labelsize": 24,      # axis labels
        "xtick.labelsize": 20,
        "ytick.labelsize": 20,
        "legend.fontsize": 20,
        "figure.titlesize": 28
    })
    
    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"survival_assigned_{cancer_type}.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ KM plot saved: {plot_path}")

    cph.summary.to_csv(os.path.join(output_dir, f"cox_summary_{cancer_type}.csv"))
    print(f"üìÑ Cox summary saved: {os.path.join(output_dir, f'cox_summary_{cancer_type}.csv')}")

    return result.summary, merged

def plot_cox_forest_with_pvalues_styled(
    cox_summary,
    output_dir,
    title="Cox PH Forest Plot",
    # title="Cox PH Forest Plot",
    cancer_type="KIRC"
):
    # Use white background without grid
    sns.set_style("white")

    df = cox_summary.copy().reset_index().rename(columns={'index': 'covariate'})
    df = df[['covariate', 'exp(coef)', 'coef lower 95%', 'coef upper 95%', 'p']]
    df = df.sort_values('exp(coef)', ascending=False).reset_index(drop=True)

    fig, ax = plt.subplots(figsize=(12, len(df) * 0.8 + 2))
    # ax.spines['top'].set_visible(False)
    # ax.spines['right'].set_visible(False)
    for spine in ['top', 'right']:#, 'bottom', 'left']:
        ax.spines[spine].set_visible(False)
        
    for i, row in df.iterrows():
        hr = row['exp(coef)']
        lower = row['coef lower 95%']
        upper = row['coef upper 95%']
        pval = row['p']

        lower_error = max(hr - lower, 0)
        upper_error = max(upper - hr, 0)

        color = 'red' if hr > 1 else 'blue'

        ax.errorbar(
            hr, i,
            xerr=[[lower_error], [upper_error]],
            fmt='o',
            color='black',
            ecolor=color,
            elinewidth=3,
            capsize=7
        )

        # P-value closer to dot
        p_text = f"p = {pval:.1e}" if pval < 0.0001 else f"p = {pval:.3g}"

        ax.text(
            hr, i + 0.15,  # tight offset
            p_text,
            ha='center',
            va='bottom',
            fontsize=26,
            color='black',
            # weight='bold'
        )


    # Reference line at HR = 1
    ax.axvline(1, color='red', linestyle='--', linewidth=2)

    # Y labels
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['covariate'], fontsize=26)

    # Axis labels
    ax.set_xlabel("Hazard Ratio (HR)", fontsize=28)
    ax.set_title(title, fontsize=32, pad=20)

    ax.tick_params(axis='x', labelsize=26)
    ax.grid(False)

    plot_path = os.path.join(output_dir, f"cox_forest_plot_styled_{cancer_type}.png")
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"‚úÖ Saved styled forest plot with large fonts: {plot_path}")




def train(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    output_dir = 'results/gene_prediction/'
    os.makedirs(output_dir, exist_ok=True)


    omics_types = ['cna', 'ge', 'meth', 'mf']
    omics_types = ['cna', 'ge', 'meth', 'mf']
    cancer_names = ['BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP',
                    'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC']
    # cancer_names = [
    #     'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
    #     'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    # ]   

    bio_feat_names = [
        f"{cancer}_{omics}"
        for omics in omics_types
        for cancer in cancer_names
    ]    
    
    topo_feat_names = [f"Topo_{i}" for i in range(64)]

    # Define feature groups
    feature_groups = {
        "bio": (0, 1024),
        "topo": (1024, 2048)
    }
    
    omics_splits = {
        'cna': (0, 15),
        'ge': (16, 31),
        'meth': (32, 47),
        'mf': (48, 63),
    }
    
    epoch_times, cpu_usages, gpu_usages = [], [], []

    data_path = os.path.join('../gat/data/multiomics_meth/', f'{args.net_type}_omics_ppi_embeddings_graph_2048.json')
    ##data_path = os.path.join('../gat/data/json_graphs_omics_mf/', f'{cancer_type}_graph.json')
    ##data_path = '../___KG-PE/embedding/data/merged_gene_embeddings.json'
    
    nodes, edges, embeddings, labels = load_graph_data(data_path)

    node_names = list(nodes.keys())

    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    graph = dgl.graph(edges, num_nodes=len(nodes))

    graph.ndata['feat'] = embeddings
    graph.ndata['label'] = labels
    graph.ndata['train_mask'] = labels != -1
    graph.ndata['test_mask'] = torch.ones_like(labels, dtype=torch.bool)
    graph = dgl.add_self_loop(graph)

    assert len(node_names) == graph.num_nodes(), "Node names length mismatch!"

    # ‚úÖ Load ground truth cancer gene names
    ground_truth_cancer_genes = load_ground_truth_cancer_genes('../gat/data/ncg_8886.txt')
    
    in_feats = embeddings.shape[1]
    model = choose_model(args.model_type, in_feats, args.hidden_feats, 1).to(device)
    if hasattr(model, 'set_graph'):
        model.set_graph(graph)

    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    graph = graph.to(device)
    features = embeddings.to(device)
    labels = labels.to(device).float()
    train_mask = graph.ndata['train_mask'].to(device)


    for epoch in tqdm(range(args.num_epochs), desc="Training Progress", unit="epoch"):
        epoch_start = time.time()
        cpu_usage = psutil.cpu_percent(interval=None)
        gpu_usage = torch.cuda.memory_allocated(device) / 2048**2 if torch.cuda.is_available() else 0.0

        model.train()
        logits = model(graph, features).squeeze()
        loss = loss_fn(logits[train_mask], labels[train_mask])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_times.append(time.time() - epoch_start)
        cpu_usages.append(cpu_usage)
        gpu_usages.append(gpu_usage)

        tqdm.write(f"Epoch {epoch + 1}/{args.num_epochs}, Loss: {loss.item():.4f}, CPU: {cpu_usage}%, GPU: {gpu_usage:.2f} MB")

    model.eval()
    with torch.no_grad():
        ##trained_embeddings = model(graph, features, return_embeddings=True)  # shape: [num_nodes, hidden_feats]
        logits = model(graph, features).squeeze()
        ##node_embeddings = logits.unsqueeze(1)  # shape: [num_nodes, 1] for binary classification
        scores = torch.sigmoid(logits).cpu().numpy()

    ##scores = train_with_relevance_tracking(args, model, graph, embeddings, labels, train_mask, output_dir)

    non_labeled_nodes = [i for i, label in enumerate(labels) if label == -1]
    non_labeled_scores = [(node_names[i], scores[i]) for i in non_labeled_nodes]
    ranking = sorted(non_labeled_scores, key=lambda x: x[1], reverse=True)

    process_predictions(
        ranking, args,
        "../gat/data/796_drivers.txt",
        "../gat/data/oncokb_1172.txt",
        "../gat/data/ongene_803.txt",
        "../gat/data/ncg_8886.txt",
        "../gat/data/intogen_23444.txt",
        node_names,
        non_labeled_nodes
    )

    predicted_cancer_genes = [i for i, _ in ranking[:1000]]
    #random.shuffle(predicted_cancer_genes)

    top_gene_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]
    graph.ndata['degree'] = graph.in_degrees().float().unsqueeze(1)

    if top_gene_indices:
        avg_degree = graph.ndata['degree'][top_gene_indices].float().mean().item()
        print(f"Average degree of top predicted nodes: {avg_degree:.2f}")
    else:
        print("No top nodes predicted above the threshold.")


    ###########################################################################################################################################
    #
    # Biclustering for Bio and Topo 
    # 
    ###########################################################################################################################################
    
    print("Generating feature importance plots...")

    top_node_features = embeddings[top_gene_indices].cpu().numpy()  # shape: [num_top_genes, feature_dim]

    # Call the function to find optimal k
    plot_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_silhouette_score_plot_epo{args.num_epochs}.png")
    best_k_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_best_k_epo{args.num_epochs}.txt")

    # Make sure output_dir exists
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs('data/', exist_ok=True)

    # Extract node names of top-k predicted genes
    node_names_topk = [node_names[i] for i in top_gene_indices]

    # Use dynamic naming based on model/net type and epochs
    bio_output_img = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_BIO_clusters_epo{args.num_epochs}.png")
    topo_output_img = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_TOPO_clusters_epo{args.num_epochs}.png")

    bio_output_path_heatmap = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_BIO_heatmap_epo{args.num_epochs}.png")
    topo_output_path_heatmap = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_TOPO_heatmap_epo{args.num_epochs}.png")
    
    bio_graph_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_clustered_graph.pth")
    topo_graph_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_clustered_graph.pth")

    bio_row_labels_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_row_labels.npy")
    topo_row_labels_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_row_labels.npy")

    bio_total_per_cluster_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_total_genes_per_cluster.npy")
    topo_total_per_cluster_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_total_genes_per_cluster.npy")

    bio_pred_counts_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_pred_counts.npy")
    topo_pred_counts_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_pred_counts.npy")

    plot_silhouette_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_silhouette_score_plot_epo{args.num_epochs}.png")
    best_k_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_best_k_epo{args.num_epochs}.txt")
    #best_k = 10#find_optimal_k(top_node_features, k_range=(5, 20), plot_path=plot_silhouette_path, save_best_k_path=best_k_path)
    # Automatically determine best_k via eigengap
    plot_eigengap_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_epo{args.num_epochs}.png")
    #best_k=eigengap_analysis(summary_bio_topk, max_clusters=25, normalize=True, plot_path=plot_eigengap_path)
    
    #best_k=10


    bio_feats = graph.ndata['feat'][:, :1024]  # biological features
    topo_feats = graph.ndata['feat'][:, 1024:] # topology features

    bio_embeddings_np = bio_feats.cpu().numpy()
    summary_bio_features = extract_summary_features_np_bio(bio_embeddings_np)  # shape [num_nodes, 64]

    topo_embeddings_np = topo_feats.cpu().numpy()
    summary_topo_features = extract_summary_features_np_topo(topo_embeddings_np)  # shape [num_nodes, 64]

    relevance_scores = compute_relevance_scores(model, graph, features)

    # Slice relevance scores for biological and topological embeddings
    relevance_scores_bio = relevance_scores[:, :1024]
    relevance_scores_topo = relevance_scores[:, 1024:]
    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    
    # Step 1: Slice top-k saliency scores for bio features (1024)
    relevance_scores_bio = relevance_scores[:, :1024]
    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    relevance_scores_topk_bio = relevance_scores_bio[topk_node_indices_tensor]
    relevance_matrix_bio = extract_summary_features_np_bio(relevance_scores_topk_bio.detach().cpu().numpy())

    print(relevance_matrix_bio.shape)
    
    relevance_scores_topk_topo = relevance_scores_topo[topk_node_indices_tensor]
    relevance_matrix_topo = extract_summary_features_np_topo(relevance_scores_topk_topo.detach().cpu().numpy())
     

    best_k_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_best_k_epo{args.num_epochs}.txt")
    #best_k = 10#find_optimal_k(top_node_features, k_range=(5, 20), plot_path=plot_silhouette_path, save_best_k_path=best_k_path)
    # Automatically determine best_k via eigengap
    plot_eigengap_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_epo{args.num_epochs}.png")
    assert relevance_matrix_bio.shape[1] == 64, f"Expected 64 summary features, got {relevance_matrix_bio.shape[1]}"
    assert relevance_matrix_topo.shape[1] == 64, f"Expected 64 summary features, got {relevance_matrix_topo.shape[1]}"
    #best_k, eigenvals = eigengap_analysis(relevance_matrix, max_clusters=11, normalize=True, plot_path=plot_eigengap_path)

    best_k=10
    print(f"Best number of clusters (k > 1): {best_k}")

    # Step 2: Convert to numpy
    #relevance_matrix = summary_bio_topk#.detach().cpu().numpy()
    #relevance_matrix = normalize(summary_bio_topk, axis=1)

    # Step 3: Get gene names for top-k nodes
    # node_names_topk = [node_names[i] for i in top_gene_indices]
    
    output_path_genes_clusters = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_clusters_epo{args.num_epochs}.png')
    output_path_predicted_genes = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_genes.csv')

    spectral_biclustering_graph_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_clustered_graph.pth")
    row_labels_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_row_labels.npy")
    total_genes_per_cluster_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_total_genes_per_cluster.npy")
    pred_counts_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_pred_counts.npy")

    # graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
    #     graph=graph,
    #     summary_bio_features=relevance_matrix_bio,  # shape (1000, 64)
    #     node_names=node_names,
    #     predicted_cancer_genes=predicted_cancer_genes,
    #     n_clusters=best_k,
    #     save_path=bio_graph_path,
    #     save_row_labels_path=bio_row_labels_path,
    #     save_total_genes_per_cluster_path=bio_total_per_cluster_path,
    #     save_predicted_counts_path=bio_pred_counts_path,
    #     output_path_genes_clusters=bio_output_img,
    #     output_path_heatmap=bio_output_path_heatmap,
    #     topk_node_indices=top_gene_indices  # <-- add this!
    # )

    graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
        graph=graph,
        summary_bio_features=relevance_matrix_bio,
        node_names_topk=node_names_topk,
        omics_splits=omics_splits,
        predicted_cancer_genes=predicted_cancer_genes,
        save_path=bio_graph_path,
        save_row_labels_path=bio_row_labels_path,
        save_total_genes_per_cluster_path=bio_total_per_cluster_path,
        save_predicted_counts_path=bio_pred_counts_path,
        output_path_genes_clusters=bio_output_img,
        output_path_heatmap=bio_output_path_heatmap,
        topk_node_indices=top_gene_indices,
        output_dir=output_dir,  # <-- new argument
        args=args               # <-- new argument
    )

    # perform_survival_analysis_one_curve_per_cluster_with_ci(
    perform_survival_analysis_cluster_assigned(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expression_matrix_path="data/TCGA-KIRC.expression.tsv",
        cluster_labels=row_labels_bio,
        top_gene_names=node_names_topk,
        output_dir="results/gene_prediction/survival_by_cluster_bio",
        # output_path="results/gene_prediction/survival_by_cluster/km_cluster_curves.png",
        cancer_type="KIRC"
    )
                                                            
    cox_results, merged_df, cluster_expr = run_survival_analysis(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expr_path="data/TCGA-KIRC.expression.tsv",
        node_names_topk=node_names_topk,
        row_labels=row_labels_bio,
        output_dir="results/gene_prediction/km_cox_bio",
        cluster_threshold="median",
        clusters_to_plot=[0,1,2,3,4,5,6,7,8,9],
        cancer_type="KIRC"
    )

    cph_summary_bio, merged, patient_cluster_scores = run_survival_analysis_cox(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expr_path="data/TCGA-KIRC.expression.tsv",
        node_names_topk=node_names_topk,
        row_labels=row_labels_bio,
        output_dir="results/gene_prediction/km_cox_cph_bio",
        cluster_threshold="median",
        cancer_type="KIRC"
        
    )
    
    plot_cox_forest_with_pvalues_styled(
        cph_summary_bio,
        output_dir="results/gene_prediction/km_cox_cph_bio",
        cancer_type="KIRC"
    )

    survival_analysis_by_cluster_assigned(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expr_path="data/TCGA-KIRC.expression.tsv",
        node_names_topk=node_names_topk,
        row_labels=row_labels_bio,
        output_dir="results/gene_prediction/km_cox_cluster-assigned_bio",
        cancer_type="KIRC"
    )
    
    
    
    # run_survival_analysis_by_cluster(
    #     row_labels_bio=row_labels_bio,
    #     node_names_topk=node_names_topk,
    #     survival_file="data/TCGA-KIRC.survival.tsv",
    #     expression_file="data/TCGA-KIRC.expression.tsv",
    #     output_dir='results/gene_prediction/survival_analysis_by_cluster',
    #     cancer_type="KIRC"
    # )

    run_survival_analysis_from_relevance(
        expr_file="data/TCGA-KIRC.expression.tsv",
        survival_file="data/TCGA-KIRC.survival.tsv",
        node_names_topk=node_names_topk,  
        relevance_matrix_bio=relevance_matrix_bio, 
        output_dir='results/gene_predictin/survival_analysis_from_relevance'
    )

    run_survival_analysis_multigroup_from_relevance(
        expr_file='data/TCGA-KIRC.expression.tsv',
        survival_file='data/TCGA-KIRC.survival.tsv',
        node_names_topk=node_names_topk, 
        relevance_matrix_bio=relevance_matrix_bio,
        cluster_labels=row_labels_bio,
        output_dir='results/gene_predictin/survival_analysis_multigroup_from_relevance'
    )
    
    #pring("")
    
    
    # perform_survival_analysis_per_cluster_high_low(
    #     survival_path="data/TCGA-KIRC.survival.tsv",
    #     expression_matrix_path="data/TCGA-KIRC.expression.tsv",
    #     cluster_labels=row_labels_bio,
    #     top_gene_names=node_names_topk,
    #     output_dir="results/gene_prediction/survival_per_cluster_high_low",
    #     min_samples_per_group=5,
    #     cluster_colors={'high': '#1f77b4', 'low': '#ff7f0e'}
    # )
    
    '''run_survival_analysis_by_cluster_no_high_low(
        row_labels_bio=row_labels_bio,
        node_names_topk=node_names_topk,
        survival_path="data/TCGA-KIRC.survival.tsv",
        expression_path="data/TCGA-KIRC.expression.tsv",
        output_dir="results/gene_prediction/survival_per_cluster_no_high_low",
        min_samples_per_group=5,
        cluster_colors={'high': '#1f77b4', 'low': '#ff7f0e'}
    )'''

    # perform_survival_analysis_per_cluster(
    #     survival_path="data/TCGA-KIRC.survival.tsv",
    #     expression_matrix_path="data/TCGA-KIRC.expression.tsv",
    #     cluster_labels=row_labels_bio,
    #     top_gene_names=node_names_topk,
    #     output_dir="results/gene_prediction/survival_by_cluster"
    # )



    # Call survival analysis
    # survival_plot_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_survival_by_cluster.png")
    
    # perform_survival_analysis(
    #     survival_path="data/TCGA-KIRC.survival.tsv",
    #     expression_matrix_path="data/TCGA-KIRC.expression.tsv",
    #     cluster_labels=row_labels_bio,  # shape: [#genes] (assigned during clustering)
    #     top_gene_names=node_names_topk,
    #     output_path=survival_plot_path
    # )
    


        
    # Save expression matrix from relevance
    # expression_matrix_path = os.path.join('data/', 'expression_matrix.tsv')

    # # Save survival plot
    # survival_plot_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_survival_by_cluster.png")
    # survival_data_path = 'data/TCGA-KIRC.survival.tsv'

    # # Call survival analysis
    # perform_survival_analysis(
    #     survival_path=survival_data_path,
    #     expression_matrix_path=expression_matrix_path,
    #     cluster_labels=row_labels_bio,  # shape: [#genes] (assigned during clustering)
    #     output_path=survival_plot_path
    # )

    # First create a full tensor of -1 (default for unclustered)
    full_row_labels_bio = torch.full((graph_bio.num_nodes(),), -1, dtype=torch.long)

    # Then assign the cluster labels only to the relevant subset
    full_row_labels_bio[top_gene_indices] = torch.tensor(row_labels_bio, dtype=torch.long)

    # Now safely assign it to the graph
    graph_bio.ndata['cluster_bio'] = full_row_labels_bio
    
    G_bio = graph_bio.to_networkx(node_attrs=['cluster_bio'])
 
    # plot_relevance_tsne_umap(
    #     relevance_scores,
    #     G_bio,
    #     cluster_key='cluster_bio',
    #     method='umap',  # or 'tsne'
    #     title_suffix="(Biological)"
    # )
    
    # === TOPO BICLUSTERING ===
    graph_topo, row_labels_topo, col_labels_topo, topo_total_counts, topo_pred_counts = apply_full_spectral_biclustering_topo(
        graph=graph,
        #topo_embeddings=topo_feats,
        summary_topo_features=relevance_matrix_topo,
        node_names_topk=node_names_topk,
        #feature_names=None,
        predicted_cancer_genes=node_names_topk,
        n_clusters=best_k,
        save_path=topo_graph_path,
        save_row_labels_path=topo_row_labels_path,
        save_total_genes_per_cluster_path=topo_total_per_cluster_path,
        save_predicted_counts_path=topo_pred_counts_path,
        output_path_genes_clusters=topo_output_img,
        output_path_heatmap=topo_output_path_heatmap,
        topk_node_indices=top_gene_indices  # <-- add this!
    )


    # perform_survival_analysis_one_curve_per_cluster_with_ci(
    perform_survival_analysis_cluster_assigned(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expression_matrix_path="data/TCGA-KIRC.expression.tsv",
        cluster_labels=row_labels_topo,
        top_gene_names=node_names_topk,
        output_dir="results/gene_prediction/survival_by_cluster_topo",
        # output_path="results/gene_prediction/survival_by_cluster/km_cluster_curves.png",
        cancer_type="KIRC"
    )
                                                            
    cox_results, merged_df, cluster_expr = run_survival_analysis(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expr_path="data/TCGA-KIRC.expression.tsv",
        node_names_topk=node_names_topk,
        row_labels=row_labels_topo,
        output_dir="results/gene_prediction/km_cox_topo",
        cluster_threshold="median",
        clusters_to_plot=[0,1,2,3,4,5,6,7,8,9],
        cancer_type="KIRC"
    )

    cph_summary_topo, merged, patient_cluster_scores = run_survival_analysis_cox(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expr_path="data/TCGA-KIRC.expression.tsv",
        node_names_topk=node_names_topk,
        row_labels=row_labels_topo,
        output_dir="results/gene_prediction/km_cox_cph_topo",
        cluster_threshold="median",
        cancer_type="KIRC"
    )
    
    plot_cox_forest_with_pvalues_styled(
        cph_summary_topo,
        output_dir="results/gene_prediction/km_cox_cph_topo",
        cancer_type="KIRC"
    )

    survival_analysis_by_cluster_assigned(
        survival_path="data/TCGA-KIRC.survival.tsv",
        expr_path="data/TCGA-KIRC.expression.tsv",
        node_names_topk=node_names_topk,
        row_labels=row_labels_topo,
        output_dir="results/gene_prediction/km_cox_cluster-assigned_topo",
        cancer_type="KIRC"
    )
    
    
    pring("")
    
    full_row_labels_topo = torch.full((graph_topo.num_nodes(),), -1, dtype=torch.long)

    # Then assign the cluster labels only to the relevant subset
    full_row_labels_topo[top_gene_indices] = torch.tensor(row_labels_topo, dtype=torch.long)

    # Now safely assign it to the graph
    graph_topo.ndata['cluster_topo'] = full_row_labels_topo

    # Plot known cancer gene percentages
    gt_indices = set(name_to_index[name] for name in ground_truth_cancer_genes if name in name_to_index)
    kcg_counts_bio = {
        i: sum((np.array(row_labels_bio) == i) & np.isin(range(len(row_labels_bio)), list(gt_indices)))
        for i in range(best_k)
    }
    

    # === BIO INTERACTION PLOTS ===
    degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()
    row_labels_np_bio = np.array(row_labels_bio)

    kcg_nodes = [i for i, name in enumerate(node_names_topk) if name in ground_truth_cancer_genes]
    kcg_data_bio = pd.DataFrame({
        "Cluster": row_labels_np_bio[kcg_nodes],
        "Interactions": degrees_np[kcg_nodes]
    })

    pcg_data_bio = pd.DataFrame({
        "Cluster": row_labels_np_bio,#[top_gene_indices],
        "Interactions": degrees_np[top_gene_indices]
    })

    suffix = 'bio'
    output_path_interactions_kcgs_bio = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcgs_interaction_{suffix}_epo{args.num_epochs}.png')
    output_path_interactions_pcgs_bio = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_pcgs_interaction_{suffix}_epo{args.num_epochs}.png')

    plot_interactions_with_kcgs(kcg_data_bio, output_path_interactions_kcgs_bio)
    plot_interactions_with_pcgs(pcg_data_bio, output_path_interactions_pcgs_bio)

    # Plot PCG cluster percentages
    plot_pcg_cancer_genes(
        clusters=range(best_k),
        predicted_cancer_genes_count=bio_pred_counts,
        total_genes_per_cluster=bio_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_bio,
        output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_pcg_percent_bio_epo{args.num_epochs}.png')
    )

    # Plot known cancer gene percentages
    kcg_counts_bio = {
        i: sum((np.array(row_labels_bio) == i) & np.isin(range(len(row_labels_bio)), list(gt_indices)))
        for i in range(best_k)
    }
    
    plot_kcg_cancer_genes(
        clusters=range(best_k),
        kcg_count=kcg_counts_bio,
        total_genes_per_cluster=bio_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_bio,
        output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcg_percent_bio_epo{args.num_epochs}.png')
    )

    # ‚ûï Novel PCG bar plot
    
    # Output path
    novel_pcg_plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_novel_pcg_cluster_bar_{suffix}_epo{args.num_epochs}.png"
    )
        
    novel_predicted_genes = [n for n in predicted_cancer_genes if n not in ground_truth_cancer_genes]

    plot_novel_predicted_cancer_genes(
        clusters=list(bio_total_counts.keys()),
        novel_predicted_cancer_genes=novel_predicted_genes,
        total_genes_per_cluster=bio_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_bio,
        output_path=novel_pcg_plot_path
    )
            
            
    # === TOPO INTERACTION PLOTS ===
    row_labels_np_topo = np.array(row_labels_topo)

    kcg_data_topo = pd.DataFrame({
        "Cluster": row_labels_np_topo[kcg_nodes],
        "Interactions": degrees_np[kcg_nodes]
    })

    pcg_data_topo = pd.DataFrame({
        "Cluster": row_labels_np_topo,#[top_gene_indices],
        "Interactions": degrees_np[top_gene_indices]
    })

    suffix = 'topo'
    output_path_interactions_kcgs_topo = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcgs_interaction_{suffix}_epo{args.num_epochs}.png')
    output_path_interactions_pcgs_topo = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_pcgs_interaction_{suffix}_epo{args.num_epochs}.png')

    plot_interactions_with_kcgs(kcg_data_topo, output_path_interactions_kcgs_topo)
    plot_interactions_with_pcgs(pcg_data_topo, output_path_interactions_pcgs_topo)

    novel_pcg_plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_novel_pcg_cluster_bar_{suffix}_epo{args.num_epochs}.png"
    )
        
    novel_predicted_genes = [n for n in predicted_cancer_genes if n not in ground_truth_cancer_genes]

    plot_novel_predicted_cancer_genes(
        clusters=list(topo_total_counts.keys()),
        novel_predicted_cancer_genes=novel_predicted_genes,
        total_genes_per_cluster=topo_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_topo,
        output_path=novel_pcg_plot_path
    )
    
    neighbor_indices = set()
    for idx in top_gene_indices:
        neighbors = graph.successors(idx).tolist()
        neighbor_indices.update(neighbors)

    confirmed_file = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_confirmed_genes_epo{args.num_epochs}.csv")
    # with open(confirmed_file, "r") as f:
    #     confirmed_genes = [line.strip() for line in f if line.strip()]

    save_and_plot_confirmed_genes_bio(
        args=args,
        node_names_topk=node_names_topk,
        node_scores_topk=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
        summary_feature_relevance=relevance_matrix_bio,
        output_dir=output_dir,
        confirmed_genes_save_path=confirmed_file,
        row_labels_topk=row_labels_bio,
        tag="bio",
        confirmed_gene_path='../gat/data/ncg_8886.txt'
        ##confirmed_gene_path="data/796_drivers.txt"
    )

    
    with open(confirmed_file, "r") as f:
        confirmed_genes = [line.strip() for line in f if line.strip()]
                
    ###########################################################################################################################################
    #
    # neighbor relevance
    # 
    ###########################################################################################################################################
    

    name_to_index = {name: idx for idx, name in enumerate(node_names)}

    node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}
    name_to_index_topk = {name: idx for idx, name in enumerate(node_names_topk)}
    
    # neighbors_dict = get_neighbors_gene_names(graph, node_names_topk, name_to_index_topk, node_names_topk)
    
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, node_names_topk)
    # print('neighbors_dict---------------------------\n',neighbors_dict)
    # print('neighbors_dict---------------------------length\n',len(neighbors_dict))
    
    # pring('node_id_to_name---------------------------length\n',len(node_id_to_name))

    '''for gene in node_names_topk:
        plot_neighbor_relevance_by_mode(
            gene=gene,
            relevance_scores=relevance_scores_bio,
            mode='BIO',
            neighbor_scores=relevance_scores_bio,
            neighbors_dict=neighbors_dict,
            name_to_index=name_to_index_topk,
            node_id_to_name=node_id_to_name,
            graph=graph,
            row_labels=row_labels_bio,#graph.ndata["cluster_bio"], 
            total_clusters=best_k,            
            args=args
        )

        plot_neighbor_relevance_by_mode(
            gene=gene,
            relevance_scores=relevance_scores_topo,
            mode='TOPO',
            neighbor_scores=scores,
            neighbors_dict=neighbors_dict,
            name_to_index=name_to_index,
            node_id_to_name=node_id_to_name,
            graph=graph,
            row_labels=graph.ndata["cluster_topo"], 
            total_clusters=best_k,   
            args=args
        )
    '''
    ###############################################################################################################   
    #
    #
    #
    ###############################################################################################################  
     
    for cluster_type, row_labels, relevance_scores_subset, tag in [
        ("bio", full_row_labels_bio, relevance_matrix_bio, "bio"),
        ("topo", full_row_labels_topo, relevance_matrix_topo, "topo")
    ]:
        print(f"üìä Processing cluster type: {tag}")
             
        # Assign cluster labels to graph
        graph.ndata[f'cluster_{tag}'] = torch.tensor(row_labels)

        # Build cluster-to-gene mappings
        predicted_cancer_genes_indices = set(name_to_index[name] for name in predicted_cancer_genes if name in name_to_index)
        cluster_to_genes = {i: [] for i in range(best_k)}
        for idx in predicted_cancer_genes_indices:
            cluster_id = int(row_labels[idx])  # Ensure it's a Python int
            cluster_to_genes[cluster_id].append(node_names[idx])

        # Prepare cluster-degrees dataframe for known and predicted genes
        row_labels_np = graph.ndata[f'cluster_{tag}'].cpu().numpy()
        degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

        # Known cancer genes
        kcg_nodes = [i for i, name in enumerate(node_names) if name in ground_truth_cancer_genes]
        kcg_data = pd.DataFrame({
            "Cluster": row_labels_np[kcg_nodes],
            "Interactions": degrees_np[kcg_nodes]
        })

        # Predicted cancer genes
        pcg_nodes = top_gene_indices
        pcg_data = pd.DataFrame({
            "Cluster": row_labels_np[pcg_nodes],
            "Interactions": degrees_np[pcg_nodes]
        })

        # ‚ûï Plot novel predicted PCGs per cluster
        # novel_predicted_genes = [
        #     name for name in predicted_cancer_genes
        #     if name not in ground_truth_cancer_genes
        # ]

        # Compute total genes per cluster
        row_labels_np = row_labels.cpu().numpy() if isinstance(row_labels, torch.Tensor) else row_labels
        total_genes_per_cluster = {c: np.sum(row_labels_np == c) for c in np.unique(row_labels_np)}

        # ‚ûï Plot known cancer genes (KCGs) per cluster
        kcg_count_per_cluster = {c: 0 for c in np.unique(row_labels)}
        for i, name in enumerate(node_names):
            if name in ground_truth_cancer_genes:
                cluster = int(row_labels[i])
                kcg_count_per_cluster[cluster] += 1

        # Output path
        kcg_plot_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_kcg_cluster_bar_{tag}_epo{args.num_epochs}.png"
        )


        # Save path for confirmed gene list
        confirmed_genes_save_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_confirmed_genes_{tag}_epo{args.num_epochs}.txt"
        )

        # Prepare for heatmap
        '''topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
        relevance_scores_topk = relevance_scores_subset[topk_node_indices_tensor]'''
        row_labels_topk = graph.ndata[f'cluster_{tag}'][topk_node_indices_tensor]

        heatmap_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}.png"
        )

        heatmap_path_kcg = os.path.join(
                    output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_kcg.png"
                )

        heatmap_path_npcg = os.path.join(
                    output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_npcg.png"
                )
        
        heatmap_path_unsort = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_unsort.png"
        )

        heatmap_path_clusters_unsort = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_clusters_unsort.png"
        )
        saliency_ridge_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_saliency_ridge_{tag}_epo{args.num_epochs}.png"
        )

        ridge_all_clusters_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_ridge_all_clusters_{tag}_epo{args.num_epochs}.png"
        )

        ridge_from_bio_path_across = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_ridge_from_{tag}_across_epo{args.num_epochs}"
        ) 
        
        sanity_heatmap_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_input_bio_heatmap_epo{args.num_epochs}.png")
                   


        print(f"üìä Processing cluster type: {tag}")

        if tag == "bio":
            plot_bio_biclustering_heatmap(
                args=args,
                relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                omics_splits=omics_splits,
                #cancer_names=cancer_names,
                output_path=heatmap_path,
                col_labels=col_labels_bio
            )
                        
            plot_collapsed_clusterfirst_multilevel_sankey_bio(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                node_names_topk=node_names_topk,
                # novel_predicted_genes=novel_predicted_genes,
                # scores=scores,
                row_labels=row_labels_bio,
                total_clusters=best_k,
                output_dir=output_dir,
                relevance_scores=relevance_matrix_bio,
                CLUSTER_COLORS=CLUSTER_COLORS 
            )
              
            # ‚úÖ Plot bio neighbor relevance for confirmed genes
            plot_confirmed_neighbors_bio(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                predicted_cancer_genes=predicted_cancer_genes,
                # scores=scores,
                row_labels=row_labels_bio,#graph.ndata["cluster_bio"]
                total_clusters=best_k,
                output_dir=output_dir,
                relevance_scores=relevance_scores_bio,
                node_names_topk=node_names_topk
            )      


                                                
            plot_biclustering_input_feature_heatmap(
                feature_matrix=relevance_matrix_bio,
                node_names_topk=[node_names[i] for i in top_gene_indices],
                output_path=sanity_heatmap_path,
                title="Sanity Check: Input Bio Features (Top-k)"
            )

            
            plot_bio_biclustering_heatmap_kcg(
                args=args,
                relevance_scores=relevance_matrix_bio,
                row_labels=row_labels_bio,
                omics_splits=omics_splits,
                output_path=heatmap_path_kcg,
                gene_names=node_names_topk,
                kcg_list=ground_truth_cancer_genes
            )

            plot_bio_biclustering_heatmap_npcg(
                args=args,
                relevance_scores=relevance_matrix_bio,
                row_labels=row_labels_bio,
                omics_splits=omics_splits,
                output_path=heatmap_path_npcg,
                gene_names=node_names_topk,
                kcg_list=ground_truth_cancer_genes,                  
                plot_only_novel=True
            )


            # plot_bio_biclustering_heatmap_unsort(
            #     args=args,
            #     relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
            #     #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
            #     #gene_names=node_names_topk,
            #     omics_splits=omics_splits,
            #     output_path=heatmap_path_unsort,
            #     col_labels=col_labels_bio
            # )

            # plot_bio_biclustering_clustermap(
            #     args=args,
            #     relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
            #     #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
            #     #gene_names=node_names_topk,
            #     omics_splits=omics_splits,
            #     output_path=heatmap_path_unsort,
            #     col_labels=col_labels_bio
            # )

            plot_bio_biclustering_heatmap_unsort(
                args=args,
                relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                omics_splits=omics_splits,
                output_path=heatmap_path_unsort,
                col_labels=col_labels_bio
            )

            plot_bio_biclustering_heatmap_clusters_unsort(
                args=args,
                relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                omics_splits=omics_splits,
                output_path=heatmap_path_clusters_unsort,
                col_labels=col_labels_bio
            )
            

            
            ######################################################################################################################

        
            cluster_dict = cluster_to_genes  # from earlier code
            

                

            save_and_plot_novel_genes_bio(
                args=args,
                node_names_topk=node_names_topk,
                node_scores_topk=relevance_matrix_bio,
                summary_feature_relevance=relevance_matrix_bio,
                output_dir="results/gene_prediction",
                novel_genes_save_path="results/gene_prediction/bio_novel_genes.txt",
                row_labels_topk=row_labels_bio,
                tag="bio",
                confirmed_gene_path='../gat/data/ncg_8886.txt'
            )


            ###############################################################################################################   
            #
            # 
            #
            ###############################################################################################################  

            pathway_enrichment(
                bio_embeddings_np=bio_embeddings_np,
                best_k=best_k,
                node_names=node_names,
                tag=tag
            )
                        
            # cancer_list = [
            #     'BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP',
            #     'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC'
            # ]

            # sources = ['REAC', 'KEGG', 'HP', 'GO:BP']

            # for cancer_type in cancer_list:
            #     print(f"\nüî¨ Cancer: {cancer_type}")
                
            #     cancer_feature = extract_all_omics_for_cancer(bio_embeddings_np, cancer_type)
            #     row_labels = apply_full_spectral_biclustering_cancer(cancer_feature, n_clusters=best_k)
            #     cluster_dict = make_cluster_dict(row_labels, node_names)

            #     enrichment_files = run_gprofiler_enrichment(cluster_dict, cancer_type, tag)

            #     for source in sources:
            #         print(f"üìä Plotting {source} enrichment for {cancer_type}...")

            #         terms = collect_enrichment_with_ratios(
            #             cancer_type=cancer_type,
            #             tag=tag,
            #             source=source,
            #             top_n=20
            #         )

            #         draw_dot_plot_with_ratio(
            #             terms=terms,
            #             cancer_type=cancer_type,
            #             source=source,
            #             top_n=20
            #         )


            
            plot_all_cancer_ridges_all_omics(
                bio_embeddings_np=bio_embeddings_np,
                node_names=node_names,
                #cluster_labels=row_labels,
                best_k=best_k,
                output_base_path=ridge_from_bio_path_across,
                top_n=10
            )
                
            # üü¢ Apply bio spectral biclustering
            # graph_bio, row_labels, col_labels, total_genes_per_cluster, predicted_counts = apply_full_spectral_biclustering_bio(
            #     graph=graph,
            #     summary_bio_features=relevance_scores_subset,
            #     node_names_topk=node_names_topk,
            #     omics_splits=omics_splits,
            #     predicted_cancer_genes=predicted_cancer_genes,
            #     save_path=bio_graph_path,
            #     save_row_labels_path=bio_row_labels_path,
            #     save_total_genes_per_cluster_path=bio_total_per_cluster_path,
            #     save_predicted_counts_path=bio_pred_counts_path,
            #     output_path_genes_clusters=bio_output_img,
            #     output_path_heatmap=bio_output_path_heatmap,
            #     topk_node_indices=top_gene_indices,
            #     output_dir=output_dir,
            #     args=args
            # )

            # Assign cluster labels
            # graph_bio.ndata[f'cluster_{tag}'] = torch.full((graph_bio.num_nodes(),), -1, dtype=torch.long)
            # graph_bio.ndata[f'cluster_{tag}'][top_gene_indices] = torch.tensor(row_labels, dtype=torch.long)

            # ‚ûï Plot t-SNE/UMAP
            # G_tmp = graph_bio.to_networkx(node_attrs=[f'cluster_{tag}'])
            # plot_relevance_tsne_umap(
            #     relevance_scores_subset,
            #     G_tmp,
            #     cluster_key=f'cluster_{tag}',
            #     method='umap',
            #     title_suffix="(Bio)"
            # )

            # ‚ûï Plot PCG percentages
            # plot_pcg_cancer_genes(
            #     clusters=range(best_k),
            #     predicted_cancer_genes_count=bio_pred_counts,
            #     total_genes_per_cluster=bio_total_counts,
            #     node_names=node_names_topk,
            #     row_labels=row_labels,
            #     output_path=os.path.join(
            #         output_dir,
            #         f'{args.model_type}_{args.net_type}_pcg_percent_{tag}_epo{args.num_epochs}.png'
            #     )
            # )

            # ‚ûï Interaction plots
            # topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64)
            # row_labels_np = np.array(row_labels)
            # degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

            # kcg_nodes = [i for i, name in enumerate(node_names_topk) if name in ground_truth_cancer_genes]
            # kcg_data = pd.DataFrame({
            #     "Cluster": row_labels_np[kcg_nodes],
            #     "Interactions": degrees_np[kcg_nodes]
            # })

            # pcg_data = pd.DataFrame({
            #     "Cluster": row_labels_np,
            #     "Interactions": degrees_np[top_gene_indices]
            # })

            # plot_interactions_with_kcgs(
            #     kcg_data,
            #     os.path.join(output_dir, f"{args.model_type}_{args.net_type}_kcgs_interaction_{tag}_epo{args.num_epochs}.png")
            # )

            # plot_interactions_with_pcgs(
            #     pcg_data,
            #     os.path.join(output_dir, f"{args.model_type}_{args.net_type}_pcgs_interaction_{tag}_epo{args.num_epochs}.png")
            # )

            # # ‚ûï KCG percentage bar plot
            # gt_indices = set(name_to_index[name] for name in ground_truth_cancer_genes if name in name_to_index)
            # kcg_counts = {
            #     i: sum((np.array(row_labels) == i) & np.isin(range(len(row_labels)), list(gt_indices)))
            #     for i in range(best_k)
            # }

            # plot_kcg_cancer_genes(
            #     clusters=range(best_k),
            #     kcg_count=kcg_counts,
            #     total_genes_per_cluster=bio_total_counts,
            #     node_names=node_names_topk,
            #     row_labels=row_labels,
            #     output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcg_percent_{tag}_epo{args.num_epochs}.png')
            # )

            # # ‚ûï Novel PCG bar plot
            # novel_predicted_genes = [n for n in predicted_cancer_genes if n not in ground_truth_cancer_genes]

            # plot_novel_predicted_cancer_genes(
            #     clusters=list(bio_total_counts.keys()),
            #     novel_predicted_cancer_genes=novel_predicted_genes,
            #     total_genes_per_cluster=bio_total_counts,
            #     node_names=node_names_topk,
            #     row_labels=row_labels,
            #     output_path=os.path.join(
            #         output_dir,
            #         f"{args.model_type}_{args.net_type}_novel_pcg_cluster_bar_{tag}_epo{args.num_epochs}.png"
            #     )
            # )

            # ‚ûï KCG cluster bar plot
            # kcg_count_per_cluster = {c: 0 for c in np.unique(row_labels)}
            # for i, name in enumerate(node_names_topk):
            #     if name in ground_truth_cancer_genes:
            #         cluster = row_labels[i]
            #         kcg_count_per_cluster[cluster] += 1

            # plot_kcg_cancer_genes(
            #     clusters=list(bio_total_counts.keys()),
            #     kcg_count=kcg_count_per_cluster,
            #     total_genes_per_cluster=bio_total_counts,
            #     node_names=node_names_topk,
            #     row_labels=row_labels,
            #     output_path=os.path.join(
            #         output_dir,
            #         f"{args.model_type}_{args.net_type}_kcg_cluster_bar_{tag}_epo{args.num_epochs}.png"
            #     )
            # )

            # üü¢ Call the function
            # plot_novel_predicted_cancer_genes(
            #     clusters=list(total_genes_per_cluster.keys()),
            #     novel_predicted_cancer_genes=novel_predicted_genes,
            #     total_genes_per_cluster=total_genes_per_cluster,
            #     node_names=node_names,
            #     row_labels=row_labels,
            #     output_path=novel_pcg_plot_path
            # )

            # print(f"‚úÖ Finished novel PCG plot for {tag} clusters.")


            # # üü¢ Call the KCG plot function
            # plot_kcg_cancer_genes(
            #     clusters=list(bio_total_counts.keys()),
            #     kcg_count=kcg_count_per_cluster,
            #     total_genes_per_cluster=bio_total_counts,
            #     node_names=node_names_topk,
            #     row_labels=row_labels,
            #     output_path=kcg_plot_path
            # )

            # print(f"‚úÖ Finished KCG plot for {tag} clusters.")
                        
        elif tag == "topo":
            # üîµ Apply topo spectral biclustering
            # graph_topo, row_labels, col_labels, total_genes_per_cluster, predicted_counts = apply_full_spectral_biclustering_topo(
            #     graph=graph,
            #     summary_topo_features=relevance_scores_subset,
            #     node_names_topk=node_names_topk,
            #     predicted_cancer_genes=node_names_topk,
            #     n_clusters=best_k,
            #     save_path=topo_graph_path,
            #     save_row_labels_path=topo_row_labels_path,
            #     save_total_genes_per_cluster_path=topo_total_per_cluster_path,
            #     save_predicted_counts_path=topo_pred_counts_path,
            #     output_path_genes_clusters=topo_output_img,
            #     output_path_heatmap=topo_output_path_heatmap,
            #     topk_node_indices=top_gene_indices
            # )

            # Assign cluster labels
            # graph_topo.ndata[f'cluster_{tag}'] = torch.full((graph_topo.num_nodes(),), -1, dtype=torch.long)
            # graph_topo.ndata[f'cluster_{tag}'][top_gene_indices] = torch.tensor(row_labels, dtype=torch.long)

            # ‚ûï Plot t-SNE/UMAP
            # G_tmp = graph_topo.to_networkx(node_attrs=[f'cluster_{tag}'])
            # plot_relevance_tsne_umap(
            #     relevance_scores_subset,
            #     G_tmp,
            #     cluster_key=f'cluster_{tag}',
            #     method='umap',
            #     title_suffix="(Topo)"
            # )

            # ‚ûï Bar plots
            plot_pcg_cancer_genes(
                clusters=range(best_k),
                predicted_cancer_genes_count=topo_pred_counts,
                total_genes_per_cluster=topo_total_counts,
                node_names=node_names_topk,
                row_labels=row_labels,
                output_path=os.path.join(
                    output_dir,
                    f'{args.model_type}_{args.net_type}_pcg_percent_{tag}_epo{args.num_epochs}.png'
                )
            )

            # ‚ûï Interaction plots
            # topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64)
            # row_labels_np = np.array(row_labels)
            # degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

            # kcg_nodes = [i for i, name in enumerate(node_names_topk) if name in ground_truth_cancer_genes]
            # kcg_data = pd.DataFrame({
            #     "Cluster": row_labels_np[kcg_nodes],
            #     "Interactions": degrees_np[kcg_nodes]
            # })

            # pcg_data = pd.DataFrame({
            #     "Cluster": row_labels_np,
            #     "Interactions": degrees_np[top_gene_indices]
            # })

            # plot_interactions_with_kcgs(
            #     kcg_data,
            #     os.path.join(output_dir, f"{args.model_type}_{args.net_type}_kcgs_interaction_{tag}_epo{args.num_epochs}.png")
            # )

            # plot_interactions_with_pcgs(
            #     pcg_data,
            #     os.path.join(output_dir, f"{args.model_type}_{args.net_type}_pcgs_interaction_{tag}_epo{args.num_epochs}.png")
            # )

            # # ‚ûï KCG percent bar plot
            # gt_indices = set(name_to_index[name] for name in ground_truth_cancer_genes if name in name_to_index)
            # kcg_counts = {
            #     i: sum((np.array(row_labels) == i) & np.isin(range(len(row_labels)), list(gt_indices)))
            #     for i in range(best_k)
            # }

            # plot_kcg_cancer_genes(
            #     clusters=range(best_k),
            #     kcg_count=kcg_counts,
            #     total_genes_per_cluster=topo_total_counts,
            #     node_names=node_names_topk,
            #     row_labels=row_labels,
            #     output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcg_percent_{tag}_epo{args.num_epochs}.png')
            # )

        
            plot_topo_biclustering_heatmap(
                args=args,
                relevance_scores=relevance_matrix_topo,# relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_topo,#topk.cpu().numpy(),
                gene_names=node_names_topk,
                output_path=heatmap_path,
                col_labels=col_labels_topo
            )

            plot_topo_biclustering_heatmap_unsorted(
                args=args,
                relevance_scores=relevance_matrix_topo,
                row_labels=row_labels_topo,
                gene_names=node_names_topk,
                output_path=heatmap_path_unsort,
                col_labels=col_labels_topo
            )
            
            plot_topo_biclustering_heatmap_clusters_unsort(
                args=args,
                relevance_scores=relevance_matrix_topo,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_topo,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                #omics_splits=omics_splits,
                output_path=heatmap_path_clusters_unsort,
                col_labels=col_labels_bio
            )
                        
            save_and_plot_confirmed_genes_topo(
                args=args,
                node_names_topk=node_names_topk,
                node_scores_topk=relevance_matrix_topo,#relevance_scores_topk.detach().cpu().numpy(),
                summary_feature_relevance=relevance_matrix_topo,  # full 1024D, will be reduced inside
                output_dir=output_dir,
                confirmed_genes_save_path=confirmed_file,
                row_labels_topk=row_labels_topo,
                tag="topo",
                confirmed_gene_path="data/ncg_8886.txt"
                ##confirmed_gene_path="data/796_drivers.txt"
            )

            # with open(confirmed_file, "r") as f:
            #     confirmed_genes = [line.strip() for line in f if line.strip()]

            #plot_collapsed_clusterfirst_multilevel_sankey_topo_sorted(
            plot_collapsed_clusterfirst_multilevel_sankey_topo(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                #confirmed_genes=confirmed_genes,
                novel_predicted_genes=novel_predicted_genes,
                scores=scores,
                row_labels=row_labels_topo,
                total_clusters=best_k,
                relevance_scores=relevance_matrix_topo,
                CLUSTER_COLORS=CLUSTER_COLORS                 
            )
                            
            # ‚úÖ Plot topo neighbor relevance for confirmed genes
            plot_confirmed_neighbors_topo(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                predicted_cancer_genes=predicted_cancer_genes,
                # scores=scores,
                row_labels=row_labels_topo,
                total_clusters=best_k,
                relevance_scores=relevance_matrix_topo
            )
            

    ###########################################################################################################################################
    #
    # pathway enrichment
    # 
    ###########################################################################################################################################
    
    # === Compare BIO and TOPO cluster assignments ===
    print("Comparing BIO vs TOPO cluster assignments on top predicted genes...")

    # Ensure tensors are on CPU
    row_labels_bio_topk = graph.ndata['cluster_bio'][topk_node_indices_tensor].cpu().numpy()
    row_labels_topo_topk = graph.ndata['cluster_topo'][topk_node_indices_tensor].cpu().numpy()

    # Compute ARI and NMI
    ari_score = adjusted_rand_score(row_labels_bio_topk, row_labels_topo_topk)
    nmi_score = normalized_mutual_info_score(row_labels_bio_topk, row_labels_topo_topk)

    print(f"Adjusted Rand Index (ARI): {ari_score:.4f}")
    print(f"Normalized Mutual Information (NMI): {nmi_score:.4f}")

    # === Contingency matrix ===
    # Plot contingency matrix using modular function
    plot_contingency_matrix(
        row_labels_bio_topk,
        row_labels_topo_topk,
        ari_score,
        nmi_score,
        output_dir,
        args
    )
    
    #######################################################################################################################################
    
    # ==== Cluster-wise Functional Enrichment ====

    def get_marker_genes_by_cluster(node_names_topk, row_labels_topk):
        cluster_to_genes = defaultdict(list)
        for name, label in zip(node_names_topk, row_labels_topk):
            cluster_to_genes[label.item()].append(name)
        return cluster_to_genes

    def run_enrichment(cluster_to_genes, organism='hsapiens'):
        gp = GProfiler(return_dataframe=True)
        enrichment_results = {}
        for cluster_id, genes in cluster_to_genes.items():
            if len(genes) >= 5:  # Skip tiny clusters
                enriched = gp.profile(
                    organism=organism,
                    query=genes,
                    sources=['KEGG', 'REAC', 'HP', 'GO:BP'],
                    user_threshold=0.05
                )
                enrichment_results[cluster_id] = enriched
        return enrichment_results

    def summarize_enrichment(enrichment_dict, label):
        for cluster_id, df in enrichment_dict.items():
            top_terms = df[['name', 'p_value']].sort_values('p_value').head(5)
            print(f"\n{label} Cluster {cluster_id} ‚Äî Top Enriched Terms:")
            print(top_terms.to_string(index=False))

    # Enrichment for bio clusters
    bio_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_bio)
    bio_enrichment = run_enrichment(bio_clusters)
    summarize_enrichment(bio_enrichment, label='Bio')

    # Enrichment for topo clusters
    topo_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_topo)
    topo_enrichment = run_enrichment(topo_clusters)
    summarize_enrichment(topo_enrichment, label='Topo')
    
    ###########################################################################################################    

    # Convert defaultdict to DataFrame
    bio_clusters_df = pd.DataFrame(
        [(cluster, gene) for cluster, genes in bio_clusters.items() for gene in genes],
        columns=["bio_cluster", "gene"]
    )

    topo_clusters_df = pd.DataFrame(
        [(cluster, gene) for cluster, genes in topo_clusters.items() for gene in genes],
        columns=["topo_cluster", "gene"]
    )

    # Then build cluster_gene_map
    cluster_gene_map = {
        'bio': bio_clusters_df.groupby('bio_cluster')['gene'].apply(list).to_dict(),
        'topo': topo_clusters_df.groupby('topo_cluster')['gene'].apply(list).to_dict()
    }

    # Initialize g:Profiler client
    gp = GProfiler(return_dataframe=True)

    # Setup: assuming enrichment_results = {}
    enrichment_results = {}

    for cluster_type, cluster_map in cluster_gene_map.items():
        enrichment_results[cluster_type] = {}
        for cluster_id, genes in cluster_map.items():
            # Skip empty clusters
            if not genes:
                continue
            
            # Perform real enrichment query
            try:
                result_df = gp.profile(
                    organism='hsapiens',
                    query=genes,
                    #sources=['GO:BP', 'REAC', 'KEGG', 'HP'],
                    sources=['KEGG', 'GO:BP', 'REAC', 'HP'],
                    user_threshold=0.05,
                    significance_threshold_method="fdr"
                )

                # Filter and store only significant results
                sig_results = result_df[result_df['p_value'] < 0.05]

                enrichment_results[cluster_type][cluster_id] = sig_results

            except Exception as e:
                print(f"Enrichment failed for {cluster_type.capitalize()} cluster {cluster_id}: {e}")
                enrichment_results[cluster_type][cluster_id] = pd.DataFrame()

    plot_enriched_term_counts(
        enrichment_results=enrichment_results,
        output_path=output_dir,
        model_type=args.model_type,
        net_type=args.net_type,
        num_epochs=args.num_epochs,
        #bio_color='#1f77b4', 
        #topo_color='#ff7f0e'
        #bio_color = '#0077B6',
        #topo_color = '#F15BB5'
        #bio_color = '#F08080',
        #topo_color = '#006400'
        ##topo_color = '#90EE90'
    )
    
    plot_shared_enriched_pathways_venn(
        enrichment_results=enrichment_results,
        output_path=output_dir,
        model_type=args.model_type,
        net_type=args.net_type,
        num_epochs=args.num_epochs
    )

    ########################################################################################################################################    

    #save_and_plot_enriched_pathways(enrichment_results, args, output_dir)
    heatmap_df, topo_terms_df, bio_terms_df = save_and_plot_enriched_pathways(enrichment_results, args, output_dir)

    ########################################################################################################################################    

    '''top_node_embeddings_bio = bio_feats[top_gene_indices].cpu().numpy()

    # Assuming embedding is of shape [N, 1024] or similar
    feature_dim_bio = top_node_embeddings_bio.shape[1]
    top_node_feature_names_bio = [f'feat_{i}' for i in range(feature_dim_bio)]

    relevance_df_bio = pd.DataFrame(
        #relevance_scores_topk.numpy(),
        relevance_matrix_bio,
        index=node_names_topk,                   # list of gene names
        columns=top_node_feature_names_bio           # list of string feature names
    )

    top_node_embeddings_topo = topo_feats[top_gene_indices].cpu().numpy()

    # Assuming embedding is of shape [N, 1024] or similar
    feature_dim_topo = top_node_embeddings_topo.shape[1]
    top_node_feature_names_topo = [f'feat_{i}' for i in range(feature_dim_topo)]

    relevance_df_topo = pd.DataFrame(
        #relevance_scores_topk.numpy(),
        relevance_matrix_topo,
        index=node_names_topk,                   # list of gene names
        columns=top_node_feature_names_topo           # list of string feature names
    )
    

    def cluster_dict_to_df(cluster_dict, cluster_label_name):
        rows = []
        for cluster_id, genes in cluster_dict.items():
            rows.extend([(gene, cluster_id) for gene in genes])
        return pd.DataFrame(rows, columns=['gene', cluster_label_name])

    bio_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_bio)
    topo_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_topo)

    bio_clusters_df = cluster_dict_to_df(bio_clusters, 'bio_cluster')
    topo_clusters_df = cluster_dict_to_df(topo_clusters, 'topo_cluster')
    
    # Create heatmaps per cluster set

    visualize_feature_relevance_heatmaps(relevance_df_bio, bio_clusters_df, "results/bio_heatmaps")
    visualize_feature_relevance_heatmaps(relevance_df_topo, topo_clusters_df, "results/topo_heatmaps")'''

    print("‚úÖ Clustered feature relevance heatmaps saved in 'results/bio_heatmaps' and 'results/topo_heatmaps'")


    ###########################################################################################################################################
    #
    # feature importance
    # 
    ###########################################################################################################################################
    
    gene_indices = [name_to_index[gene] for gene in confirmed_genes if gene in name_to_index]

    # Compute relevance scores using saliency (Integrated Gradients)
    relevance_scores = compute_relevance_scores_norm(
        model=model,
        graph=graph,
        features=features,
        node_indices=gene_indices,
        normalize=True,
        feature_groups=feature_groups
    )
            
    save_dir = os.path.join(output_dir, "gnnexplainer/")
    os.makedirs(save_dir, exist_ok=True)

    for gene in confirmed_genes:
        if gene not in name_to_index:
            print(f"‚ö†Ô∏è Gene {gene} not found in the graph.")
            continue

        node_idx = name_to_index[gene]

        if node_idx not in relevance_scores:
            print(f"‚ö†Ô∏è No relevance found for {gene} (node {node_idx})")
            continue

        if not isinstance(relevance_scores[node_idx], dict) or \
        "bio" not in relevance_scores[node_idx] or "topo" not in relevance_scores[node_idx]:
            print(f"‚ö†Ô∏è Relevance score for {gene} (node /{node_idx}) is not in expected format.")
            continue

        # Get relevance vectors and reduce
        node_idx = name_to_index[gene]
        gene_score = scores[node_idx]
        print(f"{gene} ‚Üí Node {node_idx} | Predicted score = {gene_score:.4f}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

        if node_idx not in relevance_scores:
            print(f"‚ö†Ô∏è No relevance found for {gene} (node {node_idx})")
            continue

        plot_saliency_for_gene(
            gene=gene,
            relevance_scores=relevance_scores,
            node_idx=node_idx,
            save_dir=save_dir,
            args=args,
            bio_feat_names=bio_feat_names,
            topo_feat_names=topo_feat_names
        )

    ###########################################################################################################################################
    #
    # Bio vs Topo contribution comaprison
    # 
    ###########################################################################################################################################
    
    # This is the correct dictionary for group-wise aggregation
    '''groupwise_saliency = defaultdict(list)

    for node_id, group_saliency in relevance_scores.items():
        for group_name, saliency_tensor in group_saliency.items():
            groupwise_saliency[group_name].append(saliency_tensor.cpu().numpy())

    for group_name in groupwise_saliency:
        groupwise_saliency[group_name] = np.stack(groupwise_saliency[group_name], axis=0)  # shape: [N, F]'''

    bio_scores = relevance_matrix_bio#groupwise_saliency["bio"]
    topo_scores = relevance_matrix_topo#groupwise_saliency["topo"]
    save_dir = os.path.join(output_dir, "gnnexplainer/")
    os.makedirs(save_dir, exist_ok=True)

    if len(bio_scores) > 0 and len(topo_scores) > 0:
        bio_means = np.mean(bio_scores, axis=0)
        topo_means = np.mean(topo_scores, axis=0)

        plot_bio_topo_saliency(
            bio_means,
            topo_means,
            title="Average Feature Relevance for Confirmed Genes (Bio vs Topo)",
            save_path=os.path.join(save_dir, f"{args.model_type}_{args.net_type}_bio_topo_comparison_lineplot_square_{args.num_epochs}.png")
        )

        plot_bio_topo_saliency_cuberoot(
            bio_means,
            topo_means,
            title="Average Feature Relevance for Confirmed Genes (Bio vs Topo)",
            save_path=os.path.join(save_dir, f"{args.model_type}_{args.net_type}_bio_topo_comparison_cuberoot_{args.num_epochs}.png")
        )
        
    ###############################################################################################################   
    #
    #
    #
    ###############################################################################################################  
     
def train_(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    output_dir = 'results/gene_prediction/'
    os.makedirs(output_dir, exist_ok=True)


    omics_types = ['cna', 'ge', 'meth', 'mf']
    cancer_types = ['BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP', 'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC']

    bio_feat_names = [
        f"{cancer}_{omics}"
        for omics in omics_types
        for cancer in cancer_types
    ]    
    
    topo_feat_names = [f"Topo_{i}" for i in range(64)]

    # Define feature groups
    feature_groups = {
        "bio": (0, 1024),
        "topo": (1024, 2048)
    }
    
    omics_splits = {
        'cna': (0, 15),
        'ge': (16, 31),
        'meth': (32, 47),
        'mf': (48, 63),
    }
    
    epoch_times, cpu_usages, gpu_usages = [], [], []

    data_path = os.path.join('../gat/data/multiomics_meth/', f'{args.net_type}_omics_ppi_embeddings_graph_2048.json')
    ##data_path = os.path.join('../gat/data/json_graphs_omics_mf/', f'{cancer_type}_graph.json')
    ##data_path = '../___KG-PE/embedding/data/merged_gene_embeddings.json'
    
    nodes, edges, embeddings, labels = load_graph_data(data_path)

    node_names = list(nodes.keys())

    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    graph = dgl.graph(edges, num_nodes=len(nodes))

    graph.ndata['feat'] = embeddings
    graph.ndata['label'] = labels
    graph.ndata['train_mask'] = labels != -1
    graph.ndata['test_mask'] = torch.ones_like(labels, dtype=torch.bool)
    graph = dgl.add_self_loop(graph)

    assert len(node_names) == graph.num_nodes(), "Node names length mismatch!"

    # ‚úÖ Load ground truth cancer gene names
    ground_truth_cancer_genes = load_ground_truth_cancer_genes('data/ncg_8886.txt')
    
    in_feats = embeddings.shape[1]
    model = choose_model(args.model_type, in_feats, args.hidden_feats, 1).to(device)
    if hasattr(model, 'set_graph'):
        model.set_graph(graph)

    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    graph = graph.to(device)
    features = embeddings.to(device)
    labels = labels.to(device).float()
    train_mask = graph.ndata['train_mask'].to(device)


    for epoch in tqdm(range(args.num_epochs), desc="Training Progress", unit="epoch"):
        epoch_start = time.time()
        cpu_usage = psutil.cpu_percent(interval=None)
        gpu_usage = torch.cuda.memory_allocated(device) / 2048**2 if torch.cuda.is_available() else 0.0

        model.train()
        logits = model(graph, features).squeeze()
        loss = loss_fn(logits[train_mask], labels[train_mask])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_times.append(time.time() - epoch_start)
        cpu_usages.append(cpu_usage)
        gpu_usages.append(gpu_usage)

        tqdm.write(f"Epoch {epoch + 1}/{args.num_epochs}, Loss: {loss.item():.4f}, CPU: {cpu_usage}%, GPU: {gpu_usage:.2f} MB")

    model.eval()
    with torch.no_grad():
        logits = model(graph, features).squeeze()
        scores = torch.sigmoid(logits).cpu().numpy()

    non_labeled_nodes = [i for i, label in enumerate(labels) if label == -1]
    non_labeled_scores = [(node_names[i], scores[i]) for i in non_labeled_nodes]
    ranking = sorted(non_labeled_scores, key=lambda x: x[1], reverse=True)

    process_predictions(
        ranking, args,
        "data/796_drivers.txt",
        "data/oncokb_1172.txt",
        "data/ongene_803.txt",
        "data/ncg_8886.txt",
        "data/intogen_23444.txt",
        node_names,
        non_labeled_nodes
    )

    predicted_cancer_genes = [i for i, _ in ranking[:1000]]
    #random.shuffle(predicted_cancer_genes)

    top_gene_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]
    graph.ndata['degree'] = graph.in_degrees().float().unsqueeze(1)

    if top_gene_indices:
        avg_degree = graph.ndata['degree'][top_gene_indices].float().mean().item()
        print(f"Average degree of top predicted nodes: {avg_degree:.2f}")
    else:
        print("No top nodes predicted above the threshold.")


    ###########################################################################################################################################
    #
    # Biclustering for Bio and Topo 
    # 
    ###########################################################################################################################################
    
    print("Generating feature importance plots...")

    top_node_features = embeddings[top_gene_indices].cpu().numpy()  # shape: [num_top_genes, feature_dim]

    # Call the function to find optimal k
    plot_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_silhouette_score_plot_epo{args.num_epochs}.png")
    best_k_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_best_k_epo{args.num_epochs}.txt")

    # Make sure output_dir exists
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs('data/', exist_ok=True)

    # Extract node names of top-k predicted genes
    node_names_topk = [node_names[i] for i in top_gene_indices]


    bio_feats = graph.ndata['feat'][:, :1024]  # biological features
    topo_feats = graph.ndata['feat'][:, 1024:] # topology features

    bio_embeddings_np = bio_feats.cpu().numpy()
    summary_bio_features = extract_summary_features_np_bio(bio_embeddings_np)  # shape [num_nodes, 64]

    topo_embeddings_np = topo_feats.cpu().numpy()
    summary_topo_features = extract_summary_features_np_topo(topo_embeddings_np)  # shape [num_nodes, 64]


    print("Computing relevance scores ...")
        
    ##gene_indices = [name_to_index[gene] for gene in node_names if gene in name_to_index]
    gene_indices = [name_to_index[name] for name in node_names if name in name_to_index]
    #relevance_scores = compute_relevance_scores(model, graph, features)
    #relevance_scores = compute_relevance_scores(model, graph, features, method="integrated_gradients", steps=50)
    relevance_scores = compute_relevance_scores_integrated_gradients(model, graph, features, steps=50)


    # Slice relevance scores for biological and topological embeddings
    relevance_scores_bio = relevance_scores[:, :1024]
    relevance_scores_topo = relevance_scores[:, 1024:]
    
    # Step 1: Slice top-k saliency scores for bio features (1024)
    relevance_scores_bio = relevance_scores[:, :1024]
    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    relevance_scores_topk_bio = relevance_scores_bio[topk_node_indices_tensor]
    summary_bio_topk = extract_summary_features_np_bio(relevance_scores_topk_bio.detach().cpu().numpy())

    print(summary_bio_topk.shape)
    relevance_matrix_bio = summary_bio_topk

    relevance_scores_topk_topo = relevance_scores_topo[topk_node_indices_tensor]
    summary_topo_topk = extract_summary_features_np_topo(relevance_scores_topk_topo.detach().cpu().numpy())
    relevance_matrix_topo = summary_topo_topk
    
    best_k_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_best_k_epo{args.num_epochs}.txt")
    #best_k = 10#find_optimal_k(top_node_features, k_range=(5, 20), plot_path=plot_silhouette_path, save_best_k_path=best_k_path)
    # Automatically determine best_k via eigengap
    plot_eigengap_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_epo{args.num_epochs}.png")
    assert relevance_matrix_bio.shape[1] == 64, f"Expected 64 summary features, got {relevance_matrix_bio.shape[1]}"
    assert relevance_matrix_topo.shape[1] == 64, f"Expected 64 summary features, got {relevance_matrix_topo.shape[1]}"
    #best_k, eigenvals = eigengap_analysis(relevance_matrix, max_clusters=11, normalize=True, plot_path=plot_eigengap_path)

    best_k=10
    print(f"Best number of clusters (k > 1): {best_k}")

    # Step 2: Convert to numpy
    #relevance_matrix = summary_bio_topk#.detach().cpu().numpy()
    #relevance_matrix = normalize(summary_bio_topk, axis=1)

    # Step 3: Get gene names for top-k nodes
    node_names_topk = [node_names[i] for i in top_gene_indices]
 
    
    graph_bio, cluster_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
        graph=graph,
        summary_bio_features=relevance_matrix_bio,  # shape (1000, 64)
        node_names=node_names,
        predicted_cancer_genes=predicted_cancer_genes,
        n_clusters=best_k,
        save_path=bio_graph_path,
        save_cluster_labels_path=bio_cluster_labels_path,
        save_total_genes_per_cluster_path=bio_total_per_cluster_path,
        save_predicted_counts_path=bio_pred_counts_path,
        output_path_genes_clusters=bio_output_img,
        output_path_heatmap=bio_output_path_heatmap,
        topk_node_indices=top_gene_indices  # <-- add this!
    )

    # Assign cluster labels
    #graph.ndata['cluster'] = torch.tensor(cluster_labels_bio)

    # First create a full tensor of -1 (default for unclustered)
    full_cluster_labels_bio = torch.full((graph_bio.num_nodes(),), -1, dtype=torch.long)

    # Then assign the cluster labels only to the relevant subset
    full_cluster_labels_bio[top_gene_indices] = torch.tensor(cluster_labels_bio, dtype=torch.long)

    # Now safely assign it to the graph
    graph_bio.ndata['cluster_bio'] = full_cluster_labels_bio
    
    G_bio = graph_bio.to_networkx(node_attrs=['cluster_bio'])

    # Only include clustered top genes
    valid_mask = full_cluster_labels_bio[top_gene_indices] != -1
    valid_clusters = full_cluster_labels_bio[top_gene_indices][valid_mask]#.cpu().numpy()
    valid_features = relevance_matrix_bio[valid_mask]

    entropy_score = compute_entropy(valid_clusters)
    gini_score = compute_gini(valid_clusters)
    silhouette = compute_silhouette(valid_features, valid_clusters)

    print(f"Entropy: {entropy_score:.3f}, Gini: {gini_score:.3f}, Silhouette: {silhouette:.3f}")

    # === TOPO BICLUSTERING ===
    graph_topo, cluster_labels_topo, col_labels_topo, topo_total_counts, topo_pred_counts = apply_full_spectral_biclustering_topo(
        graph=graph,
        #topo_embeddings=topo_feats,
        summary_topo_features=relevance_matrix_topo,
        node_names=node_names,
        #feature_names=None,
        predicted_cancer_genes=node_names_topk,
        n_clusters=best_k,
        save_path=topo_graph_path,
        save_cluster_labels_path=topo_cluster_labels_path,
        save_total_genes_per_cluster_path=topo_total_per_cluster_path,
        save_predicted_counts_path=topo_pred_counts_path,
        output_path_genes_clusters=topo_output_img,
        output_path_heatmap=topo_output_path_heatmap,
        topk_node_indices=top_gene_indices  # <-- add this!
    )

    # Assign cluster labels
    #graph.ndata['cluster'] = torch.tensor(cluster_labels_topo)
    
    # First create a full tensor of -1 (default for unclustered)
    full_cluster_labels_topo = torch.full((graph_topo.num_nodes(),), -1, dtype=torch.long)

    # Then assign the cluster labels only to the relevant subset
    full_cluster_labels_topo[top_gene_indices] = torch.tensor(cluster_labels_topo, dtype=torch.long)

    # Now safely assign it to the graph
    graph_topo.ndata['cluster_topo'] = full_cluster_labels_topo

    
    # Get neighbors of top predicted genes
    neighbor_indices = set()
    for idx in top_gene_indices:
        neighbors = graph.successors(idx).tolist()
        neighbor_indices.update(neighbors)



    confirmed_file = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_confirmed_genes_epo{args.num_epochs}.csv")
     
    for cluster_type, cluster_labels, relevance_scores_subset, tag in [
        ("bio", full_cluster_labels_bio, relevance_matrix_bio, "bio"),
        ("topo", full_cluster_labels_topo, relevance_matrix_topo, "topo")
        ]:
        # Assign cluster labels to graph
        graph.ndata[f'cluster_{tag}'] = torch.tensor(cluster_labels)

        # Build cluster-to-gene mappings
        predicted_cancer_genes_indices = set(name_to_index[name] for name in predicted_cancer_genes if name in name_to_index)
        cluster_to_genes = {i: [] for i in range(best_k)}
        for idx in predicted_cancer_genes_indices:
            cluster_id = int(cluster_labels[idx])  # ‚Üê Ensure it's a Python int
            cluster_to_genes[cluster_id].append(node_names[idx])

        # Prepare cluster-degrees dataframe for known and predicted genes
        cluster_labels_np = graph.ndata[f'cluster_{tag}'].cpu().numpy()
        degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

   

        if tag == "bio":
            plot_bio_biclustering_heatmap(
                args=args,
                relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                cluster_labels=cluster_labels_bio,#cluster_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                omics_splits=omics_splits,
                output_path=heatmap_path,
                col_labels=col_labels_bio
            )


            save_and_plot_confirmed_genes_bio(
                args=args,
                node_names_topk=node_names_topk,
                node_scores_topk=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                summary_feature_relevance=relevance_matrix_bio,
                output_dir=output_dir,
                confirmed_genes_save_path=confirmed_file,
                cluster_labels_topk=cluster_labels_bio,
                tag="bio",
                confirmed_gene_path="data/ncg_8886.txt"
            )

            with open(confirmed_file, "r") as f:
                confirmed_genes = [line.strip() for line in f if line.strip()]
                
            #sankey_bio_output_path=os.path.join(output_dir, f"{args.model_type}_{args.net_type}_dynamic_sankey_bio_epo{args.num_epochs}.html")
            ##selected_genes = ["PLK1", "SKP2", "SRC", "TRAF2"]
            plot_collapsed_clusterfirst_multilevel_sankey_bio(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                confirmed_genes=confirmed_genes,
                scores=scores,
                cluster_labels=cluster_labels_bio,
                total_clusters=best_k,
                relevance_scores=relevance_matrix_bio,
                CLUSTER_COLORS=CLUSTER_COLORS 
            )



            # ‚úÖ Plot bio neighbor relevance for confirmed genes
            plot_confirmed_neighbors_bio(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                confirmed_genes=confirmed_genes,
                scores=scores,
                cluster_labels=cluster_labels_bio,
                total_clusters=best_k,
                relevance_scores=relevance_matrix_bio
            )     

        else:
            plot_topo_biclustering_heatmap(
                args=args,
                relevance_scores=relevance_matrix_topo,# relevance_scores_topk.detach().cpu().numpy(),
                cluster_labels=cluster_labels_topo,#topk.cpu().numpy(),
                gene_names=node_names_topk,
                output_path=heatmap_path,
                col_labels=col_labels_topo
            )
    
            save_and_plot_confirmed_genes_topo(
                args=args,
                node_names_topk=node_names_topk,
                node_scores_topk=relevance_matrix_topo,#relevance_scores_topk.detach().cpu().numpy(),
                summary_feature_relevance=relevance_matrix_topo,  # full 1024D, will be reduced inside
                output_dir=output_dir,
                confirmed_genes_save_path=confirmed_file,
                cluster_labels_topk=cluster_labels_topo,
                tag="topo",
                confirmed_gene_path="data/ncg_8886.txt"
            )

            with open(confirmed_file, "r") as f:
                confirmed_genes = [line.strip() for line in f if line.strip()]

            #plot_collapsed_clusterfirst_multilevel_sankey_topo_sorted(
            plot_collapsed_clusterfirst_multilevel_sankey_topo(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                confirmed_genes=confirmed_genes,
                scores=scores,
                cluster_labels=cluster_labels_topo,
                total_clusters=best_k,
                relevance_scores=relevance_matrix_topo,
                CLUSTER_COLORS=CLUSTER_COLORS                 
            )
                            
            # ‚úÖ Plot topo neighbor relevance for confirmed genes
            plot_confirmed_neighbors_topo(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                confirmed_genes=confirmed_genes,
                scores=scores,
                cluster_labels=cluster_labels_topo,
                total_clusters=best_k,
                relevance_scores=relevance_matrix_topo
            )

    # Ensure tensors are on CPU
    cluster_labels_bio_topk = graph.ndata['cluster_bio'][topk_node_indices_tensor].cpu().numpy()
    cluster_labels_topo_topk = graph.ndata['cluster_topo'][topk_node_indices_tensor].cpu().numpy()

    # Compute ARI and NMI
    ari_score = adjusted_rand_score(cluster_labels_bio_topk, cluster_labels_topo_topk)
    nmi_score = normalized_mutual_info_score(cluster_labels_bio_topk, cluster_labels_topo_topk)

    print(f"Adjusted Rand Index (ARI): {ari_score:.4f}")
    print(f"Normalized Mutual Information (NMI): {nmi_score:.4f}")

    # === Contingency matrix ===
    # Plot contingency matrix using modular function
    plot_contingency_matrix(
        cluster_labels_bio_topk,
        cluster_labels_topo_topk,
        ari_score,
        nmi_score,
        output_dir,
        args
    )
    
    #######################################################################################################################################
    
    # ==== Cluster-wise Functional Enrichment ====

    def get_marker_genes_by_cluster(node_names_topk, cluster_labels_topk):
        cluster_to_genes = defaultdict(list)
        for name, label in zip(node_names_topk, cluster_labels_topk):
            cluster_to_genes[label.item()].append(name)
        return cluster_to_genes

    def run_enrichment(cluster_to_genes, organism='hsapiens'):
        gp = GProfiler(return_dataframe=True)
        enrichment_results = {}
        for cluster_id, genes in cluster_to_genes.items():
            if len(genes) >= 5:  # Skip tiny clusters
                enriched = gp.profile(
                    organism=organism,
                    query=genes,
                    sources=['KEGG', 'REAC', 'HP', 'GO:BP'],
                    user_threshold=0.05
                )
                enrichment_results[cluster_id] = enriched
        return enrichment_results

    def summarize_enrichment(enrichment_dict, label):
        for cluster_id, df in enrichment_dict.items():
            top_terms = df[['name', 'p_value']].sort_values('p_value').head(5)
            print(f"\n{label} Cluster {cluster_id} ‚Äî Top Enriched Terms:")
            print(top_terms.to_string(index=False))

    # Enrichment for bio clusters
    bio_clusters = get_marker_genes_by_cluster(node_names_topk, cluster_labels_bio)
    bio_enrichment = run_enrichment(bio_clusters)
    summarize_enrichment(bio_enrichment, label='Bio')

    # Enrichment for topo clusters
    topo_clusters = get_marker_genes_by_cluster(node_names_topk, cluster_labels_topo)
    topo_enrichment = run_enrichment(topo_clusters)
    summarize_enrichment(topo_enrichment, label='Topo')
    
    ###########################################################################################################    

    # Convert defaultdict to DataFrame
    bio_clusters_df = pd.DataFrame(
        [(cluster, gene) for cluster, genes in bio_clusters.items() for gene in genes],
        columns=["bio_cluster", "gene"]
    )

    topo_clusters_df = pd.DataFrame(
        [(cluster, gene) for cluster, genes in topo_clusters.items() for gene in genes],
        columns=["topo_cluster", "gene"]
    )

    # Then build cluster_gene_map
    cluster_gene_map = {
        'bio': bio_clusters_df.groupby('bio_cluster')['gene'].apply(list).to_dict(),
        'topo': topo_clusters_df.groupby('topo_cluster')['gene'].apply(list).to_dict()
    }

    # Initialize g:Profiler client
    gp = GProfiler(return_dataframe=True)

    # Setup: assuming enrichment_results = {}
    enrichment_results = {}

    for cluster_type, cluster_map in cluster_gene_map.items():
        enrichment_results[cluster_type] = {}
        for cluster_id, genes in cluster_map.items():
            # Skip empty clusters
            if not genes:
                continue
            
            # Perform real enrichment query
            try:
                result_df = gp.profile(
                    organism='hsapiens',
                    query=genes,
                    #sources=['GO:BP', 'REAC', 'KEGG', 'HP'],
                    sources=['KEGG', 'GO:BP', 'REAC', 'HP'],
                    user_threshold=0.05,
                    significance_threshold_method="fdr"
                )

                # Filter and store only significant results
                sig_results = result_df[result_df['p_value'] < 0.05]

                enrichment_results[cluster_type][cluster_id] = sig_results

            except Exception as e:
                print(f"Enrichment failed for {cluster_type.capitalize()} cluster {cluster_id}: {e}")
                enrichment_results[cluster_type][cluster_id] = pd.DataFrame()

    plot_enriched_term_counts(
        enrichment_results=enrichment_results,
        output_path=output_dir,
        model_type=args.model_type,
        net_type=args.net_type,
        num_epochs=args.num_epochs,
        bio_color='#1f77b4', 
        topo_color='#ff7f0e'
    )
    
    plot_shared_enriched_pathways_venn(
        enrichment_results=enrichment_results,
        output_path=output_dir,
        model_type=args.model_type,
        net_type=args.net_type,
        num_epochs=args.num_epochs,
        bio_color='#1f77b4', 
        topo_color='#ff7f0e'
    )

    ###########################################################################################################################################
    #
    # neighbor relevance
    # 
    ###########################################################################################################################################
    
    with open(confirmed_file, "r") as f:
        confirmed_genes = [line.strip() for line in f if line.strip()]

    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    for gene in confirmed_genes:
        plot_neighbor_relevance_by_mode(
            gene=gene,
            relevance_scores=relevance_scores_bio,
            mode='BIO',
            neighbor_scores=scores,
            neighbors_dict=neighbors_dict,
            name_to_index=name_to_index,
            node_id_to_name=node_id_to_name,
            graph=graph,
            cluster_labels=graph.ndata["cluster_bio"], 
            total_clusters=best_k,            
            args=args
        )

        plot_neighbor_relevance_by_mode(
            gene=gene,
            relevance_scores=relevance_scores_topo,
            mode='TOPO',
            neighbor_scores=scores,
            neighbors_dict=neighbors_dict,
            name_to_index=name_to_index,
            node_id_to_name=node_id_to_name,
            graph=graph,
            cluster_labels=graph.ndata["cluster_topo"], 
            total_clusters=best_k,   
            args=args
        )
